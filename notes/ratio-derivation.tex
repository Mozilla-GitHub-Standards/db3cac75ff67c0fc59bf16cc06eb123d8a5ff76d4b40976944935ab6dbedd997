%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,draft]{article}
%\documentclass[11pt,draft]{amsart}

% Custom styling.
\usepackage{mozdp}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}
%% Title matter
\title{K-Randomization}
\author{Maxim Zhilyaev \and David Zeber}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle


\subsection{Maximum and Local differential privacy}

Assuming local privacy ratio reaches maximum when $D$ consists of $(N-1)$ zeros and a single $1$ bit, and the set original bit is switched to $0$ to obtain $D_m$ - modified collection of all zeros.
Suppose $S$ is the number of sets bits observed in the synthetic output.   Then we can express privacy ratio for every value of $S$.

\begin{align}
P(s=i | D_m ) = \binom{N}{i}q^ip^{N-i} \\
P(s=i | D ) = \binom{N-1}{i}q^{i+1}p^{N-1-i} +   \binom{N-1}{i-1}q^{i-1}p^{N-i+1}  \\
R_i = \frac{P(s=i | D_m)}{P(s=i | D)} = \frac{ \binom{N}{i}q^ip^{N-i} }{  \binom{N-1}{i}q^{i+1}p^{N-1-i} +   \binom{N-1}{i-1}q^{i-1}p^{N-i+1} }
\end{align}

It's actually more convenient to work with $\frac{1}{R_i}$ as in:

\begin{align}
\frac{1}{R_i} = \frac{P(s=i | D)}{P(s=i | D_m)} = \frac{N-i}{N}\frac{q}{p} + \frac{i}{N} \frac{p}{q}
\end{align}

When all $i=0$ - all synthetic bits are 0, the ratio reaches its maximum:
\[
R_0 = \frac{1}{1/R_i}= \frac{p}{q}
\]

When $i=N$ - the synthetic output consists of set bits entirely, the privacy ratio reaches minimum:

\[
R_N = \frac{1}{1/R_i}= \frac{q}{p}
\]
The ratio reduces as $i$ increases, and becomes 1 when number of synthetic bits is equal to expected number of set synthetic bits after randomization:
\[
\frac{1}{R_{qN}} = \frac{N-qN}{N}\frac{q}{p} + \frac{pN}{N} \frac{q}{p} =  (1-q)\frac{q}{p} + q \frac{p}{q} = p + q = 1
\]

The notion of a  \textbf{local differential privacy}, considers the probabilistic ratio only for values of $i$ that have realistic chance of being observed.  The expected number of observed synthetic bits is $qN$, while the deviation of $S$ random variable is $\sigma=\sqrt{pqN}$.  Consider the interval $[qN - 3\sigma,  qN +3\sigma]$. Since the probabilistic ratio grows as $i$ decreased, the maximum ratio will be attained when $i= qN - 3\sigma$.  Hence, the local differential privacy reaches maximum at $i= qN - 3\sigma$, and we want to express analytically the relationship between the probabilistic privacy ratio $\lambda$, number of records $N$, and RRT parameters $p$ and $q$:

\begin{align}
i = qN - 3\sigma = qN - 3\sqrt{pqN} \\
R_i = \frac{P(s=i | D_m)}{P(s=i | D)} \le \lambda \\
\frac{1}{R_i} = \frac{P(s=i | D)}{P(s=i | D_m)} \ge \frac{1}{ \lambda} \\
\frac{N-i}{N}\frac{q}{p} + \frac{i}{N} \frac{p}{q} \ge  \frac{1}{ \lambda}  \\
\frac{N-qN + 3\sqrt{pqN}}{N} \cdot \frac{q}{p} + \frac{qN - 3\sqrt{pqN}}{N} \cdot \frac{p}{q} \ge  \frac{1}{ \lambda}
\end{align}
From here:
\begin{align}
p + q  -  3\sqrt{\frac{pq}{N}} \left ( \frac{p}{q}  - \frac{q}{p} \right ) \ge  \frac{1}{ \lambda} \\
1 -  3\sqrt{\frac{pq}{N}} \frac{p^2 - q^2}{pq}  \ge  \frac{1}{ \lambda}\\
 3\sqrt{\frac{pq}{N}} \frac{p^2 - q^2}{pq}  \le  1 - \frac{1}{ \lambda}\\
 3\sqrt{\frac{1}{N}}  \cdot \frac{p - q}{\sqrt{pq}} \le  1 - \frac{1}{ \lambda} \\
\frac{pqN}{(p-q)^2} \ge \frac{9}{(1 - \frac{1}{\lambda} )^2}
\end{align}

This is an interesting result. Note that left side of inequality is the variance of estimate $\bar{T}$.  The local differential privacy grantee simply places a lower bound on the variance of RRT estimates:

\begin{align}
VAR(\bar{T}) = \frac{pqN}{(p-q)^2} \ge \frac{9}{(1 - \frac{1}{\lambda} )^2}
\end{align}

For a randomization algorithm applied independently to $N$ bits to be $\epsilon$-differentially private in local sense, means that estimate deviation is lower-bounded by:

\begin{align}
\sigma(\bar{T})  \ge \frac{3}{1 - \frac{1}{\lambda} } =  \frac{3}{1 - \frac{1}{e^{\epsilon}} }
\end{align}

From here, we can express RRT noise parameter $q$ through $N$ and $\lambda$:

\begin{align}
 \frac{pqN}{(p-q)^2} \ge \frac{9}{(1 - \frac{1}{\lambda})^2} \\
 \frac{(1-q)q}{(1-2q)^2} \ge \frac{9}{(1 - \frac{1}{\lambda})^2N} \\
q \ge \frac{1}{2} \left (1 -  \frac{1}{\sqrt{ 1 + 4 \frac{9}{(1 - \frac{1}{\lambda})^2N} } } \right ) 
\end{align}

Suppose  $\lambda=2$ and there are 1000 single bits records in $D$.  The required noise is:
\[
q = 0.032527
\]
Compare that to the level of noise that absolute differential privacy bound would require for $\epsilon=ln(2)$. 
\begin{align}
\frac{p}{q} \le 2 \\
q \ge \frac{1}{3} =  0.333
\end{align}

The notion of local privacy allowed us to reduce RRT noise 10 times and enabled drastic improvement in estimation accuracy.  In the classical case, the estimation deviation is $\sigma =  44.7$, while for the local privacy the deviation is $\sigma =  5.6$, meaning that precision of RRT estimates had grown 8 fold.  It's worth reflecting on what's  exactly going on and why such a drastic performance increase is achievable. 

Consider confidence intervals for both an original collection $D$  and modified collection $D_m$.  $D_m$ contains 1000 empty bits and $D$ contains 999 empty bits.  Corresponding means and deviation for sum of observed synthetic bits in each case is given below:
\begin{align}
E(S) = q \cdot 1000 \\
\sigma(S) = \sqrt{pq\cdot 1000} \\
E(S_m) = q \cdot 1000 + p  \\
\sigma(S_m) = \sqrt{pq\cdot 999 + pq}
\end{align}

Consider the confidence intervals for both $S$ and $S_m$ for RRT under classical and local differential privacy constrains.  If $q=0.333$ the confidence interval for $S$ and $S_m$  are:
\begin{align}
S -> [198.9 , 467.1] \\
S_m -> [198.6 , 467.8]
\end{align}

Under local differential privacy, the noise level $q= 0. 0.033$, and the confidence intervals become:
\begin{align}
S -> [16.05 , 49.94] \\
S_m -> [ 17.02 , 50.91]
\end{align}

The intervals are nearly identical in either case.  Which illustrates the point - we do not need the full power of the absolute differential privacy bound: the local privacy bound will guarantee privacy ratio for 99.98\% of possible synthetic outcomes.  Effectively, we exploit the noise of large collection to reduce the RRT noise required to randomize each individual record.  Rephrasing this important idiom - hiding a record among other records needs less noise than obfuscating a single record.


\section{K-randomization for a single bit case}

We know consider an important technique for further increasing the estimation precision while providing same local privacy guarantees.  Recall from previous example, that if collection $D$ consists of $N=1000$ records, the corresponding RRT noise at $\lambda = 2$ is $q=0.0325$.  The deviation of the estimate in this case is $\sigma=6$.  Hence our estimation error will be roughly 18 in either direction.   We can increase the estimate precision by  repeating randomization $k$ times, hence the name  \textbf{k-ranomization}.  

It will be shown that repeating randomization $k$ times achieves increase in precision proportional to $\sqrt{k}$,  it also causes slight increase in RRT noise necessary to maintain same differential privacy guarantee.  However, the RRT noise increase is usually insignificant compared to the precision gain, which gives a nice dimension to the usual privacy vs. precision tradeoff.  K-randomization enables precision increase at the same privacy level for the expense of increasing synthetic record volume $k$ times.  Instead of trading privacy for precision, k-randomization allows to trade infrastructure cost for precision while keeping privacy the same.  This is especially apparent for long multivariate records,  but we will lay mathematical grounds starting from a single bit case.

\subsection{Estimating number of single bits under k-randomization}

Suppose there are $T$ set bits in the original collection of $N$ single bit records. Each record is randomized $k$-times.  The number of observed synthetic bits $S$ is a random variable expressed as:

\[ S = p \cdot kT + q \cdot (kN-kT) \]

The estimate for $T$, computed from observed value of $S$ is:
\begin{align}
\bar{T} = \frac{S-qkN}{k(p-q)}
\end{align}

The aggregator simply divides the estimate computed from $kN$ records by $k$.  The expectation, variance and deviation of $\bar{T}$ random variable are given by:
\begin{align}
E(\bar{T}) = T\\
VAR(\bar{T}) = \frac{qpkN}{k^2 \cdot (p-q)^2} = \frac{qpN}{k\cdot (p-q)^2}\\
\sigma(\bar{T}) = \sqrt{\frac{qpN}{k \cdot (p-q)^2}}
\end{align}

Note that deviation of the estimate is reduced by $\sqrt{k}$ compared to a single randomization case.

\subsection{Choice of D}

WE NEED PROOF FOR MAXIMALITY UNDER K

\subsection{Local differential privacy under k-randomization}

Consider probabilities of seeing $s$ set bits in the synthetic output for $D_m$ and $D$ respectively:

Since $D_m$ consists of $N$ empty bits, the probability of see $s$ synthetic bits after randomization is binomial
\begin{align}
P(S=s | D_m ) = \binom{kN}{s}q^sp^{kN-s}
\end{align}

While the original collection $D$ has a single set bit, and the probability of finding $s$ set synthetic bits is:

\begin{align}
P(S=s | D ) = \sum_{i=0}^{k} \binom{k(N-1)}{s - i}q^{s-i}p^{k(N-1) - s + i} \cdot \binom{k}{i}p^iq^{k-i}  \\
P(S=s | D_m ) = \sum_{i=0}^{k}  \binom{k(N-1)}{s - i} \binom{k}{i} q^{s+k - 2i} p^{kN - s - (k-2i)}
\end{align}

Expressing the quotient of privacy ratio at given $s$, we have:
\begin{align}
\frac{1}{R_s} =  \sum_{i=0}^{k}  \frac{ \binom{k(N-1)}{s - i} \cdot \binom{k}{i} }{  \binom{kN}{s} }  \cdot \frac{q^{k-2i}} {p^{k-2i}}
\end{align}

Consider the binomial ratio in the sum:
\begin{align}
\frac{\binom{k(N-1)}{s - i} }{  \binom{kN}{s} } = \frac{(kN-k)!}{(kN)!}  \cdot \frac{s!}{(s-i)!}  \cdot \frac{(kN -s)!}{(kN - s - (k-i))!} = \frac{ \prod_{j=0}^{i-1} (S-j) \cdot \prod_{j=0}^{k-i-1} (kN - S-j) }{  \prod_{j=0}^{k-1} (kN -j) }
\end{align}

For positive $B$,  $A$ and $e$ such that $A < B$ the following holds:  
\begin{align}
\frac{A-e}{B-e} < \frac{A}{B}
\end{align}

Hence the expression in 5.10 is upper bounded by:

\begin{align}
\frac{ \prod_{j=0}^{i-1} (s-j) \cdot \prod_{j=0}^{k-i-1} (kN - s-j) }{  \prod_{j=0}^{k-1} (kN -j) }  < \frac{ \prod_{j=0}^{i-1} s \cdot \prod_{j=0}^{k-i-1} (kN - s)}{  \prod_{j=0}^{k-1} kN } = \frac{s^i \cdot (kN - s)^{k-i} } { (kN)^k }
\end{align}

Dividing each numerator term by $kN$ we arrive to an upper bound of the privacy ratio:

\begin{align}
\frac{1}{R_s} < \sum_{i=0}^{k}  \left ( \frac{s}{kN} \right )^i \left ( 1 - \frac{s}{kN} \right )^{k-i} \cdot \binom{k}{i}  \cdot \frac{q^{k-2i}} {p^{k-2i}}
\end{align}

Again, under local privacy constrains we compute privacy ratio for $s$ located $3 \sigma$ bellow the mean:
\[ s = qkN - 3 \sqrt{pqkN} \]

Replacing $s$ in formula 5.13, we get:
\begin{align}
\sum_{i=0}^{k}  \left ( \frac{qkN - 3 \sqrt{pqkN} }{kN} \right )^i \left ( 1 - \frac{qkN - 3 \sqrt{pqkN} }{kN} \right )^{k-i} \cdot \binom{k}{i}  \cdot \frac{q^{k-2i}} {p^{k-2i}} = \\
\sum_{i=0}^{k}  \left ( q - 3 \sqrt{\frac{pq}{kN}} \right )^i \left ( 1 - q + 3 \sqrt{\frac{pq}{kN}} \right )^{k-i} \cdot \binom{k}{i}  \cdot \frac{q^{k-2i}} {p^{k-2i}} = \\
\sum_{i=0}^{k}  \frac{p^i}{q^i} \left ( q - 3 \sqrt{\frac{pq}{kN}} \right )^i  \cdot \frac{q^{k-i}}{p^{k-i}} \left ( p + 3 \sqrt{\frac{pq}{kN}} \right )^{k-i} \cdot \binom{k}{i} = \\
\sum_{i=0}^{k}   \binom{k}{i} \left ( p - 3p \sqrt{\frac{p}{qkN}} \right )^i  \cdot  \left ( q + 3q \sqrt{\frac{q}{pkN}} \right )^{k-i} = \\
\left ( q + 3q \sqrt{\frac{q}{pkN}} + p - 3p \sqrt{\frac{p}{qkN}}  \right )^k = \\
\left ( q + p - 3p \sqrt{\frac{p}{qkN}}  + 3q \sqrt{\frac{q}{pkN}} \right )^k = \\
\left ( 1 - \frac{3(p-q)}{\sqrt{qpkN}} \right )^k
\end{align}

Should the differential privacy ratio limit be $\lambda$ we have the lower bound below:
\begin{align}
 \left ( 1 + \frac{3(p-q)}{\sqrt{qpkN}} \right )^k  > \frac{1}{R_s} \ge \frac{1}{\lambda}
 \end{align}

From here we have:
\begin{align}
\left ( 1 + \frac{3(p-q)}{\sqrt{qpkN}} \right )^k  \ge \frac{1}{\lambda} \\
\frac{qpkN}{(p-q)^2} \ge  \frac{9}{(1 - \frac{1}{\sqrt[k]{\lambda}} )^2}
\end{align}

From here we express required RRT noise through $\lambda$, $N$ and $k$.

\begin{align}
q \ge \frac{1}{2} \left (1 -  \frac{1}{\sqrt{ 1 + 4 \frac{9}{(  1 - \frac{1}{\sqrt[k]{\lambda}} )^2kN} } } \right ) 
\end{align}

Using exact same example as before:  $N=1000$ records and $\lambda = 2$. Suppose the randomization is repeated 16 times, the corresponding RRT noise $q=0.1668$.
The noise increased to make the privacy stay at the same level, however the precision of the measurement actually decreased, because of $\frac{1}{\sqrt{k}}$ factor.
The corresponding sigma is:

\begin{align}
\sigma(\bar{T}) = \sqrt{\frac{qpN}{k \cdot (p-q)^2}}  = 4.42
\end{align}

So we gain 25\% precision increase by repeating randomization $16$ times.  The k-randomization gain main not be very significant for a single bit reporting, but becomes very useful for high-dimentionality vectors, where privacy level are increased due to longer bits vectors reported.

\section{multivariate vectors}

\subsection{Differential privacy ratio}

\subsubsection{Sufficient Statistics proof}

The original collection $D$ consists of $N-1$ zero vectors vectors and one unit vector of length $L$. 
Denote a zero vector as $0$ and a unit vector as $1$.  
The unit-vector $1$ is modified into a zero-vector $0$, hence the modified collection $D_m$ consists of only $0$ vectors.
There are $2^L$ possible distinct synthetic vectors.
Denote $v_i$ a distinct synthetic vector.
Denote a observed synthetic configuration $S$ as $s_1,s2,\dots,s_{2^L}$, 
whereby $s_i$ represents a count of original vectors that mapped into specific synthetic vector $v_i$ after randomization.
Denote $D'$ as a collection of $(N-1)$ zero vectors.
Then the probability of generating $S$ from collection $D'$ and a single vector $y$ is given by:
\begin{align}
P(S|D'+y) = P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|y) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|y) 
\end{align}

Re-writing the ratio

\begin{align}
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|0) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|0) }{P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|1) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|1)}\\
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{ P(v_1|0)  + \sum_{i=2}^{2^L} \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_{2^L}|D')} p(v_i|0)} {P(v_1|1)  + \sum_{i=2}^{2^L} \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_{2^L}|D')} p(v_i|1)}
\end{align}

Note that distribution of randomized vectors generated by $D'$ is multinomial, since the probability of generating a particular $v_i$ from a zero vector remains constant over all N trials.

\begin{align}
 \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_i,\dots,s_{2^L}|D')}  = \frac{ \frac{(2^L)!}{s_1!\cdot s_2! \dots (s_i-1)!\dots} p(v_1|0)^{s_1} \dots p(v_i|0)^{s_i-1} \dots  }{ \frac{(2^L)!}{(s_1-1)!\cdot s_2! \dots s_i!\dots} p(v_1|0)^{s_1-1} \dots p(v_i|0)^{s_i} \dots} = \\
\frac{(s_1-1)!s_i!}{s_1!(s_i-1)!} \cdot \frac{p(v_1|0)^{s_1} p(v_i|0)^{s_i-1} }{  p(v_1|0)^{s_1-1} p(v_i|0)^{s_i} } = \frac{s_i}{s_1} \cdot \frac{p(v_1|0)}{p(v_i|0)} =  \frac{s_i}{s_1}  \cdot \frac{p^L}{p(v_i|0)}
 \end{align}
 
 
Using that result in the ratio expression we have:

\begin{align}
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{ p^L  + \sum_{i=2}^{2^L} \frac{s_i}{s_1}  \cdot \frac{p^L}{p(v_i|0)}  p(v_i|0) } {q^L  + \sum_{i=2}^{2^L} \frac{s_i}{s_1}  \cdot \frac{p^L}{p(v_i|0)} p(v_i|1)} = \
\frac{ s_1  + \sum_{i=2}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|0)}   } {s_1 \left ( \frac{q}{p} \right )^L +   \sum_{i=2}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|0)}   } = \\
\frac {s_1 +   \sum_{i=2}^{2^L}  s_i} { s_1 \left ( \frac{q}{p} \right )^L  + \sum_{i=2}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)}   }   = \frac {N} { s_1 \left ( \frac{q}{p} \right )^L  + \sum_{i=2}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)}   }= \frac{N}{ \cdot  \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)} }
\end{align}

Note that if $v_i$ and $v_j$ have same number of set bits, the ratio inside the sum is the same:

\begin{align}
\text { if } v_i \text { has same number of set bits as } v_j \text{, and this number is  } l \text{ ,then}  \\
\frac{p(v_i|1)}{p(v_i|0)}  = \frac{p(v_j|1)}{p(v_j|0)} = \frac{p^lq^{L-l}}{p^{L-l}q^l } = \left ( \frac{q}{p} \right )^ {L - 2l}
\end{align}

This allows us to express privacy ratio as function of synthetic output $S$ through counts of synthetic vectors that have same number of set bits $l$:

\begin{align}
R(S) = \frac{P(S|D'+0)}{P(S|D'+1)} =  \frac{N}{ \cdot  \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)} }  = \frac{N}{ \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} } \\
\frac{1}{R(S)} = \frac{1}{N} \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} 
\end{align}

Hence, an observer does not gain any more privacy insight by looking at individual vectors than by looking at aggregated counts in a histogram buckets each collecting synthetic vectors with same bit count. 

\subsubsection{Local differential privacy}

As mentioned above, we can equivalently represent collection $S$ by set-bits-histogram counts. For vectors of length $L$, there are $L+1$ histogram buckets raging from $l=0$ to $l=L$. 
Let's consider the privacy ratio when the synthetic collection is in the expected state $S_e$ and assume bucket $l$ is sufficiently filled, that is $s_0 \ge 1$.  We should represent state S as:

\[ S = [s_0, s_1, \dots, s_L] \]

For the expected synthetic state $S_e$ we choose the state generated from modified collection $D_m$ consisting of $N$ zero vectors.  
The distribution $S$ is a some of $N$ independent random vectors of size $L$ consisting of probabilities of finding $1$ in a bucket $l$:

Note that probability of generating a synthetic vector containing $l$ set bits from either unit or zero original is given by:

\begin{align}
 p(l|1) = \binom{L}{l} p^lq^{L-l} \\
 p(l|0) = \binom{L}{l} q^lp^{L-l}
\end{align}


We now consider $\frac{1}{R(S)}$ to be a random variable $X$ of its own. 
\[
X = \frac{1}{R(S)} = \frac{1}{N} \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} 
\]

Note that bucket counts $s_l$ assume multinomial distribution with bucket probabilities:
\[
 p(l|0) = \binom{L}{l} q^lp^{L-l}
 \]
 
Each count is multiplied by a constant factor $\left ( \frac{q}{p} \right )^ {L - 2l}$, hence X is the sum of $L$ correlated variables $X_l$, where:

\begin{align}
 X_l =    \left ( \frac{q}{p} \right )^ {L - 2l}  Binomial(p(l|0), N)
\end{align} 
 
 
The expected values of X is given below:
\begin{align}
E(X)=  \sum_{l=0}^{L} N  \cdot  p(l|0) \left ( \frac{q}{p} \right )^ {L - 2l}  = \sum_{l=0}^{L} N  \cdot   \binom{L}{l} q^lp^{L-l} \left ( \frac{q}{p} \right )^ {L - 2l} = N \sum_{l=0}^{L}  \binom{L}{l} \cdot q^{L-l}p^{l} = N (p+q)^L = N
\end{align}

The variance of $X$ is expressed through variance-covariance of multinomial distribution:

\begin{align}
VAR(X)=  \sum_{l=0}^{L} VAR(X_l) + 2 \sum_{i \le j} \sum_{ < j \le L} COV(X_i, X_j) \\
VAR(X) =  \sum_{l=0}^{L} N \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)(1-p(l|0)) - 2 \sum_{j \ne j } N p(i|0)  \left ( \frac{q}{p} \right )^ {L - 2i} p(j|0) \left ( \frac{q}{p} \right )^ {L - 2j} = \\
VAR(X) = N \left ( \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0) -   \sum_{l=0}^{L} \left [ p(l|0) \left (  \frac{q}{p} \right )^ {L - 2l} \right ]^2  - 2 \sum_{j \ne j }  p(i|0)  \left ( \frac{q}{p} \right )^ {L - 2i} p(j|0) \left ( \frac{q}{p} \right )^ {L - 2j}  \right )
\end{align}

Note that negative terms is an expansion of the square of the sum, hence:
\begin{align}
VAR(X) = N   \left ( \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)  - \left [   \sum_{l=0}^{L} p(l|0) \left (  \frac{q}{p} \right )^ {L - 2l}    \right ]^2 \right ) \\
VAR(X) = N   \left ( \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)  - 1 \right )
\end{align}

We now simplify the first term of the sum:
\begin{align}
 \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)  = \
 \sum_{l=0}^{L} \binom{L}{l} q^lp^{L-l} \left ( \frac{q}{p} \right )^ {L - 2l} \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} = \\
 \sum_{l=0}^{L} \binom{L}{l} p^lq^{L-l} \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \\
 \sum_{l=0}^{L}  \binom{L}{l}   \frac{q^{2L-3l}}{p^{L-3l}} = \sum_{l=0}^{L}  \binom{L}{l}   \frac{q^{3L-3l} p^{3l}}{(pq)^{L}} = \\
 \frac{1}{(pq)^{L}} \sum_{l=0}^{L}  \binom{L}{l}   (q^3)^{L-l} (p^3)^{l} = \left ( \frac{p^3 + q^3}{pq} \right )^L
\end{align}

Hence the variance of $X$ has the final form of
\begin{align}
VAR(X) = N   \left (  \left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \right )
\end{align}

The local differential privacy requires that $X$ should not be too far away from $X_e$.  Which we express as a requirement that the X should not deviate more than certain number $\sigma$ away from expected value.  Hence the local privacy expression is given by:

\begin{align}
\frac{1}{R(S)} =   \frac{1}{N}   (E(X) - 3\sqrt{VAR(X)}) \ge \frac{1}{\lambda}  \\
1 - \frac{3}{N} \sqrt{N   \left (  \left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \right )}  \ge \frac{1}{\lambda}  \\
\left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \le (1 -  \frac{1}{\lambda})^2 \frac{N}{9} \\
\left ( \frac{p^3 + q^3}{pq} \right )^L  \le 1 + (1 -  \frac{1}{\lambda})^2 \frac{N}{9} \\
 \frac{p^3 + q^3}{pq} \le \sqrt[L]{1 + (1 -  \frac{1}{\lambda})^2 \frac{N}{9}} 
\end{align}

From here we express $q$:
\begin{align}
 \frac{p^3 + q^3}{pq}  = \frac{1 - 3q +3q^2}{q(1-q)} \le \sqrt[L]{1 + (1 -  \frac{1}{\lambda})^2 \frac{N}{9}} \\
 q^2 - q + \frac{1}{3+\sqrt[L]{1 + (1 -  \frac{1}{\lambda})^2 \frac{N}{9}} } \le 0 \\
 q  \ge \frac{1}{2} \left ( 1 - \sqrt{1- \frac{4}{ 3+\sqrt[L]{1 + (1 -  \frac{1}{\lambda})^2 \frac{N}{9}} }} \right )
\end{align}



\end{document}

