%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,draft]{article}
%\documentclass[11pt,draft]{amsart}

% Custom styling.
\usepackage{mozdp}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}
%% Title matter
\title{K-Randomization}
\author{Maxim Zhilyaev \and David Zeber}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Outline of the procedure}
\begin{itemize}
   \item 
\end{itemize}

\section{Theoretical setup}
In the following we work with data in the form of bit vectors. A \textbf{bit
vector} is a vector $v \in \{0,1\}^L$.

First we define the randomization procedure we will be applying.

\begin{defn}
    The randomization procedure $R$ with \textbf{lie probability} $0 < q < 1/2$
    flips a bit with probability $q$, and leaves
it as-is with probability $1-q$. 
In other words, for a bit $b \in \{0,1\}$,
\[ R(b) = R(b; X) = (1-b)\cdot X + b \cdot (1-X) \quad\text{where } 
X\sim Ber(q). \]
When applied to a vector, each bit is randomized independently:
\[ R(v) = R\big(v; (X_1,\dots,X_L)\big) = 
    \big(R(v_1; X_1),\dots,R(v_L; X_L)\big) 
\quad\text{where } X \iid Ber(q). \]
\end{defn}

\begin{rmk}
    The randomization $R$ reports the original bit value with probability 
    $1-q > q$, and lies with probability $q$. 
    This is equivalent to the randomized response procedure where the value is 
    reported as-is with probability $1-f$, and with probability $f$ the reported 
    value is the outcome of the toss of a fair coin. 
    In this case, $q = f/2$.
\end{rmk}
\begin{rmk}
    If $q = 1/2$, then $R(0)\eqdist R(1)$, and the reported value is 
    ``completely'' randomly generated, \ie independently of the original value.
\end{rmk}

Distribution of $R(v)$.

For a bit $b$, the randomization lies iff $R(b) \neq b$: 
\[
    P[R(b) = s] = q^{\ind{b \neq s}}(1-q)^{\ind{b = s}}
\]
Hence, for a bit vector $v$, 
\[ P[R(v) = s] = q^{\sum\ind{b_i \neq s_i}}(1-q)^{\sum\ind{b_i = s_i}} 
    = q^{L-m(v,s)}(1-q)^{m(v,s)}, \]
where $m(v,s) = |\{i: v_i = s_i\}|$. 
Note that this probability is maximized when $m(v,s) = L$ (the reported vector
$s$ is identical to the original vector $v$), and minimized when $m(v,s) = 0$.
In other words, the most likely outcome of randomizing a bit vector is obtaining
an identical vector.

For a collection $T$,
\[ 
    P[s \in R(T)] = 1-P[s \not\in R(T)] = 1-\prod_{v \in T} P[R(v) \neq s]
    = 1-\prod_{v \in T} \big[1 - q^{L-m(v,s)}(1-q)^{m(v,s)} \big].
\]

\section{Differential Privacy}

The typical setting for differential privacy is the following. 
We consider a \textbf{database} as a collection of records. 
The records are elements of some space $D$, and a database $\boldsymbol{x}$
is a vector of $n$ records: $\boldsymbol{x} \in D^n$. 

We wish to release information based on the database by applying a 
\textbf{query} to it. 
This is a function $A$ mapping the database into another space: 
$\map{A}{D^n}{\boldsymbol{S}}$. 
If the function $A$ is random, \ie $A(\boldsymbol{x}) = A(\boldsymbol{x}, X)$
for a random element $X$, then the output $A(\boldsymbol{x})$ is a random 
element of $\boldsymbol{S}$.

In considering the differential privacy of $A$, we compare the result of 
applying $A$ to two very similar databases $\boldsymbol{x},\ \boldsymbol{x'} 
\in D^n$.
We say the databases \textbf{differ in one row} if 
$\sum_{i=1}^n \ind{x_i \neq x'_i} = 1$.
The random query $A$ is said to be $\epsilon$-\textbf{differentially private} if,
for any two databases $\boldsymbol{x},\ \boldsymbol{x'} \in D^n$ differing in
one row,
\[ P[A(\boldsymbol{x}) \in S] \leq \epsilon \cdot P[A(\boldsymbol{x'}) \in S] \]
for all $S \cont \boldsymbol{S}$ (measurable).
An alternative notion of differing in one row that is sometimes used is that
$\boldsymbol{x} \in D^n$, $\boldsymbol{x'} \in D^{n+1}$, and $x_i = x'_i$ for 
$i = 1,\dots,n$. 
In other words, $\boldsymbol{x'}$ includes an additional record that is not
in $\boldsymbol{x}$.

If $\boldsymbol{S}$ is countable, then we can write 
\[ P[A(\boldsymbol{x}) \in S] = \sum_{s \in S} P[A(\boldsymbol{x}) = s]. \]
Hence, 
\[ \frac{P[A(\boldsymbol{x}) \in S]}{P[A(\boldsymbol{x'}) \in S]} = 
    \frac{\sum_{s \in S} P[A(\boldsymbol{x}) = s]}
    {\sum_{s \in S} P[A(\boldsymbol{x'}) = s]} \leq
\max_{s \in S} \frac{P[A(\boldsymbol{x}) = s]}{P[A(\boldsymbol{x'}) = s]} \]
by the Lemma (need reference).

Furthermore, if $A$ randomizes each record in the database independently,
\ie $A(\boldsymbol{x}) = A(\boldsymbol{x}, \boldsymbol{X}) := 
\big(A_0(x_1, X_1),\dots,A_0(x_n,X_n)\big)$ where $X_i$ are independent,
then $\boldsymbol{S} = \boldsymbol{S}_0^n$ and $s = (s_1,\dots,s_n)$ with 
$s_i \in \boldsymbol{S}_0$. 
In this case $P[A(\boldsymbol{x}) = s] = 
P[A_0(x_1) = s_1,\dots,A_0(x_n) = s_n] = \prod P[A_0(x_i) = s_i]$.
If $\boldsymbol{x}$ and $\boldsymbol{x'}$ differ in one row (wlog 
$x_1 \neq x'_1$ and $x_i = x'_i$ for $i = 2,\dots,n$), then
\[ \frac{P[A(\boldsymbol{x}) = s]}{P[A(\boldsymbol{x'}) = s]} = 
\frac{P[A_0(x_1) = s_1]}{P[A_0(x'_1) = s_1]}. \]
Therefore, in this case, the query $A$ will satisfy differential privacy if
\[ P[A_0(x) = s] \leq \epsilon \cdot P[A_0(x') = s] \] for all $x,x' \in D$
and $s \in \boldsymbol{S}_0$.
This is the formulation used in the RAPPOR paper that applies to differences
between individual records rather than collections differing on a single element.


-------------------------------------------------------

Consider a collection $T$ of bit vectors, and write $T_v = T\backslash\{v\}$.
The randomization procedure $R$ is $\epsilon$-differentially private if
\[ \log\left(\frac{P[R(T)\in S]}{P[R(T_v)\in S]}\right) \leq \epsilon \]
for any set of bit vectors $S$.

Anonymity:
\[
    A_p = \min_{v \in T, s \in \{0,1\}^L} \frac{P[s \in R(T_v)]}{P[s = R(v)]}
\]

\section{Single bit case}

\subsection{Estimating number of single bits}

Suppose there are $T$ set bits in the original collection of $N$ single bit records. After randomization is performed the number of observed synthetic bits $S$ is a random variable which we express as:
\[ S = p \cdot T + q \cdot (N-T) \]

From here we can express an estimate for $T$, computed from observed value of $S$:
\begin{align}
\bar{T} = \frac{S-qN}{p-q}
\end{align}

The expectation, variance and deviation of $\bar{T}$ random variable are given by:
\begin{align}
E(\bar{T}) = T\\
VAR(\bar{T}) = \frac{qpN}{(p-q)^2} \\
\sigma(\bar{T}) = \sqrt{\frac{qpN}{(p-q)^2}}
\end{align}


\subsection{Local Differential Privacy}

We now study how differential privacy ratio changes depending  on the configuration of underlying database  $D$.  
Assuming that  $D$ consists of $N$ single bit records, we are interested in deriving the expression of differential probability ratio as a function of observed number of set bits after randomization is performed.


\subsubsection{Choice of D}

We are seeking collection $D$ that maximizes differential privacy ratio for any number of observed bits in the randomized collection $S$.   Since we initially consider $D$ to consists of single bits only, the modified record switches the original bit to an opposite value.   Without loss of generality, assume that the original record was 1 and it was modified to 0.  Hence the original collection $D$ contains at least one set bit, and the modified collection $D_m$ contains one less set bits.   Both collections generate synthetic collection $S$.   Call the number of set bits in the synthetic collection a random variable $s$.  Then, the differential privacy ratio when $s$ is equal a particular number $i$ of set bits is given by:

\[
R_i = \frac{P(s=i | D_m)}{P(s=i | D)}
\]

\begin{thm}
$R_i$  is maximized when $D$ contains N set bits
\end{thm}

\begin{proof}

Suppose there are $m$ set bits in the original collection $D_m$.  Consider generating function for the number of set bits $s$ in $S$.

\[
G_m(x) = (q + px)^m (p + qx)^{N-m} = \sum_{i=0}^{N} a_i^mx^i
\]

Note that coefficients $a_i^m$ in the expansion of the generating function $G_m$ represent probabilities of $P(s=i | D)$.  We prove that for any $i$, the differential privacy ratio grows with $m$:

\[
\frac{a_i^m}{a_i^{m+1}} > \frac{a_i^{m-1}}{a_i^{m}}
\] 

which holds when

\begin{align}
(a_i^m)^2 > a_i^{m-1}a_i^{m+1}  \\
(a_i^m)^2  - a_i^{m-1}a_i^{m+1} > 0
\end{align}

Consider generating functions for $m+1$, $m$ and $m-1$ respectively:

\begin{align}
G_{m+1}(x) = (p + qx)^{m+1} (p + qx)^{N-m-1} \\
G_m(x) = (p + qx)^m (p + qx)^{N-m} \\
G_{m-1}(x) = (p + qx)^{m-1} (p + qx)^{N-m+1}
\end{align}

Define $Q(x)$ as:

\[ 
Q(x) = (p + qx)^{m-1} (p + qx)^{N-m-1} = \sum_{i=0}^{N-2} b_i x^i 
\]

Then generating functions above are expressed as:

\begin{align}
G_{m+1}(x) = Q(x) (q+px)^2 =  \sum_{i=0}^{N} [b_iq^2 + 2qp b_{i-1} + b_{i-2} p^2] x^i \\
G_m(x) = Q(x) (q+px)(p + qx) =  \sum_{i=0}^{N} [b_iqp + (q^2 + p^2) b_{i-1} + b_{i-2} qp] x^i \\
G_{m-1}(x) = Q(x) (p+qx)^2 = \sum_{i=0}^{N} [b_ip^2 + 2qp b_{i-1} + b_{i-2} q^2] x^i
\end{align}

From here we can express coefficients of each generating function through coefficients of $Q(x)$

\begin{align}
a_i^{m+1} = b_iq^2 + 2qp b_{i-1} + b_{i-2} p^2\\
a_i^m = b_iqp + (q^2 + p^2) b_{i-1} + b_{i-2} qp \\
a_i^{m-1}= b_ip^2 + 2qp b_{i-1} + b_{i-2} q^2
\end{align}

Now, replace the coefficients $a_i$ in the $4.6$ with their expressions through $b_i$.

\[
(a_i^m)^2  - a_i^{m-1}a_i^{m+1}  = 
(b_iqp + (q^2 + p^2) b_{i-1} + b_{i-2} qp)^2 - (b_iq^2 + 2qp b_{i-1} + b_{i-2} p^2) \cdot (b_ip^2 + 2qp b_{i-1} + b_{i-2} q^2) 
\]

After trivial algebraic transformations the above expression simplifies to:

\[
(a_i^m)^2  - a_i^{m-1}a_i^{m+1}  = 
(p^2 - q^2)^2  \cdot (b_i^2 - b_{i+1}b_{i-1})  \ge 0
\]

Note that the first term of the product is always greater than 0, and we will show that the second term is greater or equal to zero as well.

% total hack
 \textbf{Lemma 1}  If a polynomial has the from bellow
 
 \[
 Q(x) =  \sum_{i=0}^{n} a_i x^i = a_n \prod_{i}^{n} (r_i + x), \; where \; \; r_i \ge 0
\]

Then  \[ (a_i^2 - a_{i+1}a_{i-1})  \ge 0 \]

\begin{proof}
Assume polynomial is monic (e.g. $a_n=1$), and prove lemma by induction.

For n=2:

\begin{align}
(r_1 + x) (r_2 + x) = r_1*r_2 + (r_1 + r_2) x + x^2 \\
a_1^2 - a_0a_2 =  (r_1 + r_2)^2 - r_1*r_2 = r_1^2 + r_2^2 + r_1r_2 > 0 \; , since \; r_1 > 0 \; and \; r_2 > 0
\end{align}

Assume that for n, the statement holds for all $i$, then for $n+1$ we can express the polynomial as:

\begin{align}
 Q^{n+1}(x) =  \sum_{i=0}^{n+1} a_i x^i  = \prod_{i}^{n+1} (r_i + x) = Q^n(x) \cdot (r_{n+1} + x) =  \left ( \sum_{i=0}^{n} b_i x^i \right ) \cdot  (r_{n+1}+ x) \\
\sum_{i=0}^{n+1} a_i x^i   =  \sum_{i=0}^{n+1} [b_ir_{n+1} + b_{i-1}]x^i \\
a_i = b_ir_{n+1} + b_{i-1}
\end{align}

The index of $r_{n+1}$ is irrelevant for the proof, hence we drop it.  We now express  $(a_i^2 - a_{i+1}a_{i-1})$ through coefficients of $Q^n(x)$ and preform algebraic simplifications:

\begin{align}
a_i^2 - a_{i+1}a_{i-1} =  [b_ir + b_{i-1}]^2 -  [b_{i+1}r + b_i] \cdot [b_{i-1}r + b_{i-2}]  \\
a_i^2 - a_{i+1}a_{i-1} = r^2 (b_i^2 - b_{i+1}b_{i-1}) + r (b_i b_{i-1} - b_{i+1}b_{i-1}) + (b_{i-1}^2 - b_ib_{i-2}) \\
b_i^2 - b_{i+1}b_{i-1} \ge 0 \; by \; induction \; hypothesis \\
b_{i-1}^2 - b_ib_{i-2} \ge 0  \; by \; induction \; hypothesis \\
\\
b_i b_{i-1} - b_{i+1}b_{i-1} \ge 0 \; because \; all \; b_i \; are \; positive \; and \\
b_i^2 \ge b_{i+1} b_{i-1} \; and \; b_{i-1}^2 > b_ib_{i-2} \\
b_i^2 \cdot b_{i-1}^2 \ge b_{i+1} b_{i-1} b_ib_{i-2} \\
b_i b_{i-1} \ge b_{i+1}b_{i-1}
\end{align}

This completes the proof of  \textbf{Lemma 1}  for monic polynomials.  Same result is true for non-monic polynomials because if   $ (a_i^2 - a_{i+1}a_{i-1})  \ge 0 $, then multiplying each coefficient by constant factor does not change the inequality.  

We now ready to finish the proof of \textbf{Theorem 4.1}.  Consider the generating function $G_m(x)$ again:
\[
G_m(x) = (q + px)^m (p + qx)^{N-m} = p^m q^{N-m}  \left (\frac{q}{p} + x \right )^m \left (\frac{p}{q} + x \right )^{N-m}
\]
Note that since $p$ and $q$ are probabilities, the expressions in parenthesis are of the from necessary for \textbf{Lemma 1} to hold. Which proves that:
\[
\frac{a_i^m}{a_i^{m+1}} > \frac{a_i^{m-1}}{a_i^{m}}
\] 

Which in turn proves that the differential privacy ratio maximizes when $m=N$
\end{proof}
\end{proof}

\subsection{Maximum and Local differential privacy}

Since we established a notion of a differential privacy ratio $R_i$ to be a function of the observed number of set bits in the synthetic output, it's instructive to see how this ratio changes with $i$.   Since $D$ consists of set bits, we have for any $i$

\begin{align}
P(s=i | D ) = \binom{N}{i}p^iq^{N-i} \\
P(s=i | D_m ) = \binom{N-1}{i}p^{i+1}q^{N-i} +   \binom{N-1}{i-1}p^{i-1}q^{N-i+1}  \\
R_i = \frac{P(s=i | D_m)}{P(s=i | D)} = \frac{N-i}{N}\frac{p}{q} + \frac{i}{N} \frac{q}{p}
\end{align}

When all $i=0$ - all synthetic bits are 0, the ratio reaches its maximum:
\[
R_0 = \frac{p}{q}
\]

When $i=N$ - the synthetic output consists of set bits entirely, the privacy ratio reaches minimum:

\[
R_N = \frac{q}{p}
\]
The ratio reduces as $i$ increases, and becomes 1 when number of synthetic bits is equal to expected number of set synthetic bits after randomization:
\[
R_{pN} =  \frac{N-pN}{N}\frac{p}{q} + \frac{pN}{N} \frac{q}{p} =  (1-p)\frac{p}{q} + p \frac{q}{p} = p + q = 1
\]

This observation raises a question of reducing absolute theoretical bound of classical differential privacy by considering realistic values of $i$, rather then all possible outcomes of randomization.  Indeed, the probability of all $N$ bits of $D$ generating $N$ zeros is very low.  For example, assuming $p=0.7$,  $q=0.3$ and $N=100$,  the probability of seeing no synthetic ones is $q^{100} = 5e^{-53}$, which is improbable for any realistic scenario.  Instead, we should consider values of $i$ that are realistic.  In statistical sense, we should only consider values of $i$ that fall within certain number of $\sigma$ away from the expected mean.  

This brings about a notion of a  \textbf{local differential privacy}, whereby the probabilistic ratio is considered only for values of $i$ that have realistic chance of being observed.  Consider the expression for $R_i$ again.
\[
R_i = \frac{P(s=i | D_m)}{P(s=i | D)} = \frac{N-i}{N}\frac{p}{q} + \frac{i}{N} \frac{q}{p}
\]

The expected number of observed synthetic bits is $pN$, while the deviation of $S$ random variable is $\sigma=\sqrt{pqN}$.  Consider the interval $[pN - 3\sigma,  pN +3\sigma]$. Since the probabilistic ratio grows as $i$ decreased, the maximum ratio will be attained when $i= pN - 3\sigma$.  Hence, the local differential privacy reaches maximum at $i= pN - 3\sigma$, and we want to express analytically the relationship between the probabilistic privacy ratio $\lambda$, number of records $N$, and RRT parameters $p$ and $q$:

\begin{align}
i = pN - 3\sigma = pN - 3\sqrt{pqN} \\
R_i = \frac{P(s=i | D_m)}{P(s=i | D)} = \frac{N-i}{N}\frac{p}{q} + \frac{i}{N} \frac{q}{p} \le \lambda \\
Max(R_i) = \frac{N-pN + 3\sqrt{pqN}}{N} \cdot \frac{p}{q} + \frac{pN + 3\sqrt{pqN}}{N} \cdot \frac{q}{p} \le \lambda
\end{align}
From here:
\begin{align}
\frac{N-pN + 3\sqrt{pqN}}{N} \cdot \frac{p}{q} + \frac{pN - 3\sqrt{pqN}}{N} \cdot \frac{q}{p} \le \lambda \\
p + q  +  3\sqrt{\frac{pq}{N}} \left ( \frac{p}{q}  - \frac{q}{p} \right )  \le \lambda \\
1 +  3\sqrt{\frac{pq}{N}} \frac{p^2 - q^2}{pq}  \le \lambda \\
1 + 3\sqrt{\frac{1}{N}}  \cdot \frac{p - q}{\sqrt{pq}}  \le \lambda \\
\frac{pqN}{(p-q)^2} \ge \frac{9}{(\lambda - 1)^2}
\end{align}

This is an interesting result. Note that left side of inequality is the variance of estimate $\bar{T}$.  The local differential privacy grantee simply places a lower bound on the variance of RRT estimates:

\begin{align}
VAR(\bar{T}) = \frac{pqN}{(p-q)^2} \ge \frac{9}{(\lambda - 1)^2}
\end{align}

For a randomization algorithm applied independently to $N$ bits to be $\epsilon$-differentially private in local sense, means that estimate deviation is lower-bounded by:

\begin{align}
\sigma(\bar{T})  \ge \frac{3}{\lambda - 1} =  \frac{3}{e^{\epsilon} - 1}
\end{align}

From here, we can express RRT noise parameter $q$ through $N$ and $\lambda$:

\begin{align}
 \frac{pqN}{(p-q)^2} \ge \frac{9}{(\lambda - 1)^2} \\
 \frac{(1-q)q}{(1-2q)^2} \ge \frac{9}{(\lambda - 1)^2N} \\
q \ge \frac{1}{2} \left (1 -  \frac{1}{\sqrt{ 1 + 4 \frac{9}{(\lambda - 1)^2N} } } \right ) 
\end{align}

Suppose  $\lambda=2$ and there are 1000 single bits records in $D$.  The required noise is:
\[
q = 0.009
\]
Compare that to the level of noise that absolute differential privacy bound would require for $\epsilon=ln(2)$. 
\begin{align}
\frac{p}{q} \le 2 \\
q \ge \frac{1}{3} =  0.333
\end{align}

The notion of local privacy allowed us to reduce RRT noise 37 times and enabled drastic improvement in estimation accuracy.  In the classical case, the estimation deviation is $\sigma =  44.7$, while for the local privacy the deviation is $\sigma =  3$, meaning that precision of RRT estimates had grown 10 fold.  It's worth reflecting on what's  exactly going on and why such a drastic performance increase is achievable. 

Consider confidence intervals for both an original collection $D$  and modified collection $D_m$.  $D$ contains 1000 set bits and $D_m$ contains 999 set bits.  Corresponding means and deviation for sum of observed synthetic bits in each case is given below:
\begin{align}
E(S) = p \cdot 1000 \\
\sigma(S) = \sqrt{pq\cdot 1000} \\
E(S_m) = p \cdot 999 + q \\
\sigma(S_m) = \sqrt{pq\cdot 999 + pq}
\end{align}

Consider the confidence intervals for both $S$ and $S_m$ for RRT under classical and local differential privacy constrains.  If $q=0.333$ the confidence interval for $S$ and $S_m$  are:
\begin{align}
S -> [621.98 , 711.42] \\
S_m -> [621.65 , 711.09]
\end{align}

Under local differential privacy, the noise level $q= 0.009$, and the confidence intervals become:
\begin{align}
S -> [982.04 , 999.96] \\
S_m -> [981.06 , 998.98]
\end{align}

The intervals are nearly identical in either case.  Which illustrates the point - we do not need the full power of the absolute differential privacy bound: the local privacy bound will guarantee privacy ratio for 99.98\% of possible synthetic outcomes.  Effectively, we exploit the noise of large collection to reduce the RRT noise required to randomize each individual record.  Rephrasing this important idiom - hiding a record among other records needs less noise than obfuscating a single record.


\section{K-randomization for a single bit case}

We know consider an important technique for further increasing the estimation precision while providing same local privacy guarantees.  Recall from previous example, that if collection $D$ consists of $N=1000$ records, the corresponding RRT noise at $\lambda = 2$ is $q=0.009$.  We saw that deviation in this case is $\sigma=3$.  Hence our estimation error will be roughly 9 in either direction.   We can increase the estimate precision by  repeating randomization $k$ times, hence the name  \textbf{k-ranomization}.  

It will be shown that repeating randomization $k$ times achieves increase in precision proportional to $\sqrt{k}$,  it also causes slight increase in RRT noise necessary to maintain same differential privacy guarantee.  However, the RRT noise increase is usually insignificant compared to the precision gain, which gives a nice dimension to the usual privacy vs. precision tradeoff.  K-randomization enables precision increase at the same privacy level for the expense of increasing synthetic record volume $k$ times.  Instead of trading privacy for precision, k-randomization allows to trade infrastructure cost for precision while keeping privacy the same.  This is especially apparent for long multivariate records,  but we will lay mathematical grounds starting from a single bit case.

\subsection{Estimating number of single bits under k-randomization}

Suppose there are $T$ set bits in the original collection of $N$ single bit records. Each record is randomized $k$-times.  The number of observed synthetic bits $S$ is a random variable expressed as:

\[ S = p \cdot kT + q \cdot (kN-kT) \]

The estimate for $T$, computed from observed value of $S$ is:
\begin{align}
\bar{T} = \frac{S-qkN}{k(p-q)}
\end{align}

The aggregator simply divides the estimate computed from $kN$ records by $k$.  The expectation, variance and deviation of $\bar{T}$ random variable are given by:
\begin{align}
E(\bar{T}) = T\\
VAR(\bar{T}) = \frac{qpkN}{k^2 \cdot (p-q)^2} = \frac{qpN}{k\cdot (p-q)^2}\\
\sigma(\bar{T}) = \sqrt{\frac{qpN}{k \cdot (p-q)^2}}
\end{align}

Note that deviation of the estimate is reduced by $\sqrt{k}$ compared to a single randomization case.

\subsection{Choice of D}

We now prove that $D$ consisting of only set bits maximizes local differential privacy ratio for any number of observed bits in the randomized collection $S$.  Recall that for a time randomization, the generating function for $S$ given that $D$ contains $m$ set bits is:

\[
G_m(x) = (q + px)^{km} (p + qx)^{k(N-m)} =  [(q + px)^{m} (p + qx)^{N-m}]^k = \left [ \sum_{i=0}^{N} a_i^mx^i \right ]^k =  \sum_{j=0}^{kN} b_i^mx^i
\]

The generating function for $S$ given that $D_m$ contains $m-1$ set bits is:

\[
G_{m-1}(x) = [(q + px)^{(m-1)} (p + qx)^{(N-m+1)}]^k =  \left [  \sum_{i=0}^{kN} a_i^{m-1}x^i \right ]^k = =  \sum_{j=0}^{kN} b_i^{m-1}x^i
\]

Note that each coefficients $b^m_j$ and $b_j^{m-1}$ are products of $a^m_i$ and $a^{m-1}_i$ with exact same indexes of $i$.  Hence, by Theorem 4.1:

\begin{align}
\frac{b_j^m}{b_j^{m+1}}  = \frac{\prod{a_i^m}}{\prod{a_i^{m+1}}}>   \frac{\prod{a_i^{m-1}}}{\prod{a_i^{m}}} = \frac{b_j^{m-1}}{b_j^{m}}
\end{align}

\subsection{Local differential privacy under k-randomization}

Consider probabilities of seeing $s$ set bits in the synthetic output for $D$ and $D_m$ respectively:

\begin{align}
P(S=s | D ) = \binom{kN}{s}p^sq^{kN-s} \\
P(S=s | D_m ) = \sum_{i=0}^{k} \binom{k(N-1)}{s - i}p^{s-i}q^{k(N-1) - s + i} \cdot \binom{k}{i}p^{k-i}q^{i}  \\
P(S=s | D_m ) = \sum_{i=0}^{k}  \binom{k(N-1)}{s - i} \binom{k}{i} p^{s+k - 2i} q^{kN - s - (k-2i)}
\end{align}

Expressing the privacy ratio at given $s$, we have:
\begin{align}
R_s =  \sum_{i=0}^{k}  \frac{ \binom{k(N-1)}{s - i} \cdot \binom{k}{i} }{  \binom{kN}{s} }  \cdot \frac{p^{k-2i}} {q^{k-2i}}
\end{align}

Consider the binomial ratio in the sum:
\begin{align}
\frac{\binom{k(N-1)}{s - i} }{  \binom{kN}{s} } = \frac{(kN-k)!}{(kN)!}  \cdot \frac{s!}{(s-i)!}  \cdot \frac{(kN -s)!}{(kN - s - (k-i))!} = \frac{ \prod_{j=0}^{i-1} (S-j) \cdot \prod_{j=0}^{k-i-1} (kN - S-j) }{  \prod_{j=0}^{k-1} (kN -j) }
\end{align}

For positive $B$,  $A$ and $e$ such that $A < B$ the following holds:  
\begin{align}
\frac{A-e}{B-e} < \frac{A}{B}
\end{align}

Hence the expression in 5.10 is upper bounded by:

\begin{align}
\frac{ \prod_{j=0}^{i-1} (s-j) \cdot \prod_{j=0}^{k-i-1} (kN - s-j) }{  \prod_{j=0}^{k-1} (kN -j) }  < \frac{ \prod_{j=0}^{i-1} s \cdot \prod_{j=0}^{k-i-1} (kN - s)}{  \prod_{j=0}^{k-1} kN } = \frac{s^i \cdot (kN - s)^{k-i} } { (kN)^k }
\end{align}

Dividing each numerator term by $kN$ we arrive to an upper bound of the privacy ratio:

\begin{align}
R_s < \sum_{i=0}^{k}  \left ( \frac{s}{kN} \right )^i \left ( 1 - \frac{s}{kN} \right )^{k-i} \cdot \binom{k}{i}  \cdot \frac{p^{k-2i}} {q^{k-2i}}
\end{align}

Again, under local privacy constrains we compute privacy ratio for $s$ located $3 \sigma$ bellow the mean:
\[ s = pkN - 3 \sqrt{pqkN} \]

Replacing $s$ in formula 5.13, we get:
\begin{align}
\sum_{i=0}^{k}  \left ( \frac{pkN - 3 \sqrt{pqkN} }{kN} \right )^i \left ( 1 - \frac{pkN - 3 \sqrt{pqkN} }{kN} \right )^{k-i} \cdot \binom{k}{i}  \cdot \frac{p^{k-2i}} {q^{k-2i}} = \\
\sum_{i=0}^{k}  \left ( p - 3 \sqrt{\frac{pq}{kN}} \right )^i \left ( 1 - p + 3 \sqrt{\frac{pq}{kN}} \right )^{k-i} \cdot \binom{k}{i}  \cdot \frac{p^{k-2i}} {q^{k-2i}} = \\
\sum_{i=0}^{k}  \frac{q^i}{p^i} \left ( p - 3 \sqrt{\frac{pq}{kN}} \right )^i  \cdot \frac{p^{k-i}}{q^{k-i}} \left ( q + 3 \sqrt{\frac{pq}{kN}} \right )^{k-i} \cdot \binom{k}{i} = \\
\sum_{i=0}^{k}   \binom{k}{i} \left ( q - 3q \sqrt{\frac{q}{pkN}} \right )^i  \cdot  \left ( p + 3p \sqrt{\frac{p}{qkN}} \right )^{k-i} = \\
\left ( q - 3q \sqrt{\frac{q}{pkN}} + p + 3p \sqrt{\frac{p}{qkN}}  \right )^k = \\
\left ( q + p + 3p \sqrt{\frac{p}{qkN}}  - 3q \sqrt{\frac{q}{pkN}} \right )^k = \\
\left ( 1 + \frac{3(p-q)}{\sqrt{qpkN}} \right )^k
\end{align}

Should the differential privacy ratio limit be $\lambda$ we have the lower bound below:
\begin{align}
R_s < \left ( 1 + \frac{3(p-q)}{\sqrt{qpkN}} \right )^k  \le \lambda
\end{align}

From here we have:
\begin{align}
\left ( 1 + \frac{3(p-q)}{\sqrt{qpkN}} \right )^k  \le \lambda \\
\frac{qpkN}{(p-q)^2} \ge  \frac{9}{(\sqrt[k]{\lambda} - 1)^2}
\end{align}

From here we express required RRT noise through $\lambda$, $N$ and $k$.

\begin{align}
q \ge \frac{1}{2} \left (1 -  \frac{1}{\sqrt{ 1 + 4 \frac{9}{(\sqrt[k]{\lambda} - 1)^2kN} } } \right ) 
\end{align}

\section{multivariate vectors}

\subsection{Differential privacy ratio}

\subsubsection{Sufficient Statistics proof}

The original collection $D$ consists of $N$ unit vectors of length $L$.  
One of the unit-vectors $1$ is modified into a zero-vector $0$.
There are $2^L$ possible distinct synthetic vectors.
Denote $v_i$ a distinct synthetic vector.
Denote a observed synthetic configuration $S$ as $s_1,s2,\dots,s_{2^L}$, 
whereby $s_i$ represents a count of original vectors that mapped into specific synthetic vector $v_i$ after randomization.
Denote $D'$ as a collection of $(N-1)$ unit vectors.
Then the probability of generating $S$ from collection $D'$ and a single vector $y$ is given by:
\begin{align}
P(S|D'+y) = P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|y) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|y) 
\end{align}

Re-writing the ratio

\begin{align}
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|0) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|0) }{P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|1) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|1)}\\
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{ P(v_1|0)  + \sum_{i=2}^{2^L} \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_{2^L}|D')} p(v_i|0)} {P(v_1|1)  + \sum_{i=2}^{2^L} \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_{2^L}|D')} p(v_i|1)}
\end{align}

Note that distribution of randomized vectors generated by $D'$ is multinomial, since the probability of generating a particular $v_i$ from a unit vector remains constant over all N trials.

\begin{align}
 \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_i,\dots,s_{2^L}|D')}  = \frac{ \frac{(2^L)!}{s_1!\cdot s_2! \dots (s_i-1)!\dots} p(v_1|1)^{s_1} \dots p(v_i|1)^{s_i-1} \dots  }{ \frac{(2^L)!}{(s_1-1)!\cdot s_2! \dots s_i!\dots} p(v_1|1)^{s_1-1} \dots p(v_i|1)^{s_i} \dots} = \\
\frac{(s_1-1)!s_i!}{s_1!(s_i-1)!} \cdot \frac{p(v_1|1)^{s_1} p(v_i|1)^{s_i-1} }{  p(v_1|1)^{s_1-1} p(v_i|1)^{s_i} } = \frac{s_i}{s_1} \cdot \frac{p(v_1|1)}{p(v_i|1)} =  \frac{s_i}{s_1}  \cdot \frac{q^L}{p(v_i|1)}
 \end{align}
 
 
Using that result in the ratio expression we have:

\begin{align}
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{ p^L  + \sum_{i=2}^{2^L} \frac{s_i}{s_1}  \cdot \frac{q^L}{p(v_i|1)}  p(v_i|0) } {q^L  + \sum_{i=2}^{2^L} \frac{s_i}{s_1}  \cdot \frac{q^L}{p(v_i|1)} p(v_i|1)} = \
\frac{ s_1 \left ( \frac{p}{q} \right )^L  + \sum_{i=2}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|1)}   } {s_1 +   \sum_{i=2}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|1)}   } = \\
\frac{ s_1 \left ( \frac{p}{q} \right )^L  + \sum_{i=2}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|1)}   } {s_1 +   \sum_{i=2}^{2^L}  s_i}    = \frac{ s_1 \left ( \frac{p}{q} \right )^L  + \sum_{i=2}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|1)}   } {N} = \frac{1}{N}  \cdot  \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|1)} 
\end{align}

Note that if $v_i$ and $v_j$ have same number of set bits, the ratio inside the sum is the same:

\begin{align}
\text { if } v_i \text { has same number of set bits as } v_j \text{, and this number is  } l \text{ ,then}  \\
\frac{p(v_i|0)}{p(v_i|1)}  = \frac{p(v_j|0)}{p(v_j|1)} = \frac{p^{L-l}q^l }{p^lq^{L-l}} = \left ( \frac{p}{q} \right )^ {L - 2l}
\end{align}

This allows us to express privacy ratio through counts of synthetic vectors that have same number of set bits $l$:

\begin{align}
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{1}{N}  \cdot  \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|1)}  =  \frac{1}{N}   \cdot  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{p}{q} \right )^ {L - 2l}
\end{align}

Hence, an observer does not gain any more privacy insight by looking at individual vectors than by looking at aggregated counts in a histogram buckets each collecting synthetic vectors with same bit count. 

\subsubsection{Local differential privacy}

As mentioned above, we can equivalently represent collection $S$ by set-bits-histogram counts. For vectors of length $L$, there are $L+1$ histogram buckets raging from $l=0$ to $l=L$. 
Let's consider the privacy ratio when the synthetic collection is in the expected state $S_e$ and assume bucket $l$ is sufficiently filled, that is $s_0 \ge 1$.  We should represent state S as:

\[ S = [s_0, s_1, \dots, s_L] \]

For the expected synthetic state $S_e$ we choose the state generated from modified collection since this increases privacy ratio. Recall that modified collection $D' + 0$ consists of $(N-1)$ unit vectors and one zero valued vector.  Hence the distribution $S$ is a some of $N$ independent random vectors of size $L$ consisting of probabilities of finding $1$ in a bucket $l$:

\begin{align}
S = (N-1) \
 \begin{bmatrix}  \
  p(l=0|1) \\
  p(l=1|1) \\
  \dots \\
  p(l=L|1)
\end{bmatrix} + \
 \begin{bmatrix}  \
  p(l=0|0) \\
  p(l=1|0) \\
  \dots \\
  p(l=|0)
\end{bmatrix} = \
 N \begin{bmatrix}  \
  p(l=0|1) \\
  p(l=1|1) \\
  \dots \\
  p(l=L|1)
\end{bmatrix} + \
 \begin{bmatrix}  \
  p(l=0|0) \\
  p(l=1|0) \\
  \dots \\
  p(l=L|0)
\end{bmatrix} - \
 \begin{bmatrix}  \
  p(l=0|1) \\
  p(l=1|1) \\
  \dots \\
  p(l=L|1)
\end{bmatrix}
\end{align}

Note that probability of generating a synthetic vector containing $l$ set bits from either unit or zero original is given by:

\begin{align}
 p(l|1) = \binom{L}{l} p^lq^{L-l} \\
 p(l|0) = \binom{L}{l} q^lp^{L-l}
\end{align}

For a single unit vector, the expected value of the privacy ratio is given by:

\begin{align}
\sum_{l=0}^{L}   p(l|1)  \cdot  \left ( \frac{p}{q} \right )^ {L - 2l} = \sum_{l=0}^{L}  \cdot   \binom{L}{l} p^lq^{L-l} \left ( \frac{p}{q} \right )^ {L - 2l} =\sum_{l=0}^{L}  \binom{L}{l} \cdot p^{L-l}q^{l} = (p+q)^L = 1
\end{align}

For a single zero-valued vector, the expected value of the privacy ratio is given by:

\begin{align}
\sum_{l=0}^{L} p(l|0)  \cdot  \left ( \frac{p}{q} \right )^ {L - 2l} = \sum_{l=0}^{L}  \cdot   \binom{L}{l} p^{L-l}q^l \left ( \frac{p}{q} \right )^ {L - 2l} = \
\sum_{l=0}^{L}  \binom{L}{l}   \frac{p^{2L-3l}}{q^{L-3l}} = \\
\sum_{l=0}^{L}  \binom{L}{l}   \frac{p^{3L-3l} q^{3l}}{(pq)^{L}} = \frac{1}{(pq)^{L}} \sum_{l=0}^{L}  \binom{L}{l}   (p^3)^{L-l} (q^3)^{l} = \left ( \frac{p^3 + q^3}{pq} \right )^L
\end{align}

Using these expressions we have for the expected state $S_e$

\begin{align}
\frac{P(S_e|D'+0)}{P(S_e|D'+1)} = \frac{N-1}{N} + \frac{1}{N}  \left ( \frac{p^3 + q^3}{pq} \right )^L
\end{align}

The local differential privacy requires that state $S$ should not be too far away from $S_e$.  Which we express as a requirement that the count in every bucket can't be more than $3\sigma$ away from expected value.  Since, the number of synthetic vectors in each bucket has to sum to N, then the total number of deviations in each bucket must sum to 0. Which immediately gives us the upper bound for the privacy ratio:

\begin{align}
\frac{P(S_e|D'+0)}{P(S_e|D'+1)} <   \frac{1}{N}   \cdot  \sum_{l=0}^{L} (E(s_l) + 3\sqrt{VAR(s_l)}) \cdot  \left ( \frac{p}{q} \right )^ {L - 2l}
\end{align}

Note that variance in each $l$-bucket is identical for either unit or zero original vector and is:

\begin{align}
VAR(s_l) =  \frac{1}{N}   \cdot  \sum_{l=0}^{L} (E(s_l) + 3\sqrt{VAR(s_l)}) \cdot  \left ( \frac{p}{q} \right )^ {L - 2l}
\end{align}

\newpage 


\section{===== IGNORE BELOW THIS POINT  ============}

Suppose there are two vectors of length $L$ in D.  One is modified to all-zeros vector.  
Assume synthetic vectors are n$l_1$ and $l_2$, and original collection is vector $z$ and $x$

\begin{align}
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{1}{N}   \cdot  \sum_{l=0}^{L} N \binom{L}{l} p^lq^{L-l} \cdot  \left ( \frac{p}{q} \right )^ {L - 2l} =  \sum_{l=0}^{L} \binom{L}{l} p^{L-l}q^{l} = 1
\end{align}


\begin{align}
\frac{p(l_1|0)p(l_2|x) + p(l_1|x)p(l_2|0)}{p(l_1|z)p(l_2|x) + p(l_1|x)p(l_2|z)} = \
\frac{ \frac{p(l_1|0)}{p(l_1|z)} + \frac{p(l_1|x)p(l_2|0)}{p(l_1|z)p(l_2|x)}}{1 + \frac{p(l_1|x)p(l_2|z)}{p(l_1|z)p(l_2|x)}} \\
\frac{p(l_1|0)p(l_2|x) + p(l_1|x)p(l_2|0)}{p(l_1|1)p(l_2|x) + p(l_1|x)p(l_2|1)} = \
\frac{ \frac{p(l_1|0)}{p(l_1|1)} + \frac{p(l_1|x)p(l_2|0)}{p(l_1|1)p(l_2|x)}}{1 + \frac{p(l_1|x)p(l_2|1)}{p(l_1|1)p(l_2|x)}} \\
p(l_1|0) = p^{L-l_1}q^{l_1}\\
p(l_1|1) = p^{l_1}q^{L-l_1}\\
\frac{ \left ( \frac{p}{q} \right )^{L-2\cdot l_1}  +  \frac{p(l_2|0)}{p(l_2|1)} \cdot \frac{p(l_1|x)p(l_2|1)}{p(l_1|1)p(l_2|x)}  } { 1 + \frac{p(l_1|x)p(l_2|1)}{p(l_1|1)p(l_2|x)}} \\
\frac{ \left ( \frac{p}{q} \right )^{L-2\cdot l_1}  +   \left ( \frac{p}{q} \right )^{L-2\cdot l_2} \cdot p^{(l_2-l_1)}q^{L-(l_2-l_1)} \cdot\frac{p(l_1|x)}{p(l_2|x)}  } { 1 +  p^{(l_2-l_1)}q^{L-(l_2-l_1)} \cdot\frac{p(l_1|x)}{p(l_2|x)} } \\
\left ( \frac{p}{q} \right )^{L-2\cdot l_1} \cdot \frac{ 1 +   \left ( \frac{p}{q} \right )^{2(l_1-l_2)} \cdot p^{(l_2-l_1)}q^{L-(l_2-l_1)} \cdot\frac{p(l_1|x)}{p(l_2|x)}  } { 1 +  p^{(l_2-l_1)}q^{L-(l_2-l_1)} \cdot\frac{p(l_1|x)}{p(l_2|x)} } \\
\left ( \frac{p}{q} \right )^{L-2\cdot l_1} \cdot \frac{ 1 +   q^L \left ( \frac{p}{q} \right )^{(l_1-l_2)} \cdot\frac{p(l_1|x)}{p(l_2|x)}  } { 1 +  q^L  \left ( \frac{q}{p} \right )^{(l_1-l_2)}  \cdot\frac{p(l_1|x)}{p(l_2|x)} } \\
\end{align}

\begin{align}
p(S|D) \\
p(S|D_m) = \sum_{i=1}^{N-1}  p(s_i |0) p(S-s_i | D_{N-1}) \\
\frac{p(S|D_m)}{p(S|D)} = \frac{ \sum_{i=1}^{N}  p(s_i |0) p(S-s_i | D_{N-1})}{ \sum_{i=1}^{N}  p(s_i |1) p(S-s_i | D_{N-1})}
\end{align}

\begin{align}
p(S|D) = N! \prod_{i=1}^N p^{s_i}q^{L-s_i}\\
p(S|D_m) =  \sum_{i=1}^{N}  p(s_i |0) \cdot (N-1)!  \prod_{j=1, j \neq i}^N p^{s_j}q^{L-s_j} \\
\frac{p(S|D_m)}{p(S|D)} = \sum_{i=1}^{N}  p(s_i |0) \cdot \frac{  (N-1)!  \prod_{j=1, j \neq i}^N p^{s_j}q^{L-s_j} } { N! \prod_{i=1}^N p^{s_i}q^{L-s_i}}  \\
\frac{p(S|D_m)}{p(S|D)} = \frac{1}{N}  \sum_{i=1}^{N}  \frac{p(s_i |0)}{ p(s_i |1)} \\
\frac{p(S|D_m)}{p(S|D)} = K/N  \sum_{i=1}^{K}  \frac{p(0 |0)}{ p(0 |1)} +  M/N  \sum_{i=1}^{M}  \frac{p(1 |0)}{ p(1 |1)} 
\end{align}




\begin{align}
p(S|D_m) =  \sum_{i=1}^{N}  p(s_i |0) \cdot (N-1)!  \prod_{j=1, j \neq i}^N p^{s_j}q^{L-s_j} \\
p(S|D) =  \sum_{i=1}^{N}  p(s_i |x) \cdot (N-1)!  \prod_{j=1, j \neq i}^N p^{s_j}q^{L-s_j} \\
\frac{p(S|D_m)}{p(S|D)} = \frac { \sum_{i=1}^{N}  p(s_i |0) \cdot (N-1)!  \prod_{j=1, j \neq i}^N p^{s_j}q^{L-s_j}  } { \sum_{i=1}^{N}  p(s_i |x) \cdot (N-1)!  \prod_{j=1, j \neq i}^N p^{s_j}q^{L-s_j}  } \\
\frac{p(S|D_m)}{p(S|D)} =  \frac { p(s_1|0)/p(s_1|1) + } { 1} 
\end{align}



\section{WILD WEST}

Suppose $S$ consists of  $0$ and identical $s$ vectors 
If  $D$ is N unit vectors:

\begin{align}
p(S|D) =  Nq^Lp(s|1)^{N-1}\\
p(S|D_m) = p(0|0) p(s|1)^{N-1} + (N-1) p(0|1)p(s|0)p(s|1)^{N-2} \\
p(S|D_m) = p^Lp(s|1)^{N-1} + (N-1) q^Lp(s|0)p(s|1)^{N-2}\\
\frac{p(S|D_m)}{p(S|D)} = \frac { p^Lp(s|1)^{N-1} + (N-1) q^Lp(s|0)p(s|1)^{N-2} } {  Nq^Lp(s|1)^{N-1}}\\
\frac{p(S|D_m)}{p(S|D)} = \frac { p^Lp(s|1)^{N-1} + (N-1) q^Lp(s|0)p(s|1)^{N-2} } {  Nq^Lp(s|1)^{N-1}}\\
\frac{p(S|D_m)}{p(S|D)} = \frac {\left ( \frac{p}{q} \right )^L + (N-1) \frac{p(s|0)}{p(s|1)} } { N}\\
\frac{p(S|D_m)}{p(S|D)} = \frac {\left ( \frac{p}{q} \right )^L + (N-1)\left ( \frac{p}{q} \right )^{L-2s} } { N} \\
\frac{p(S|D_m)}{p(S|D)} = \frac{1}{N}\left ( \frac{p}{q} \right )^L + \frac{N-1}{N}\left ( \frac{p}{q} \right )^{L-2s}\\
\end{align}

\section{MSTACK QUESTION}
Bounding ratio of probabilities of Poisson-Binomial Distribution.

There are $N$ Bernoulli trials, where $m$  trails have probability of success $p$ and $N-m$ trails have probability of success $q=1-p$.  Assume $p>q$.
The number of successes in these $N$ Bernoulli random variables is a random variable $S$ expressed as a sum of two binomial random variables:

\begin{align}
S = Bin(p,m) + Bin(q,N-m)
\end{align}

The distribution of $S$ is known to be a Poisson-Binomial Distribution.
I am studying the behavior of the ratio between P(S=k) and P(S=k-1) with respect to $m$ and $k$.
For a given $m$ and number of successes $k$, denote such ratio as $R(k,m)$:

\begin{align}
R(k,m)  = \frac{P(S=k \; | \;m)}{P(S=k-1 \; |\;m)}
\end{align}

I am particularly interested in the behavior of this ratio when for small quantiles, and ran numerical simulation for $R(k,m)$ when $k << mean$. 
It appears that for values of $k$ that are equally distant from the mean, the following holds:

Let $\mu_m=m \cdot (p-q) + N \cdot q$ be the mean of the corresponding distribution and $\alpha$ be the distance away from the mean.
Then for $k=\mu_m - \alpha$:

\begin{align}
R(\mu_0 - \alpha,0)  > R(\mu_1 - \alpha,1) > \dots > R(\mu_{m-1} - \alpha,m-1) > R(\mu_{m} - \alpha,m) > \dots > R(\mu_{N} - \alpha,N) 
\end{align}

The ratio seems to bounded by two binomial distribution for $m=0$ and $m=N$ respectively.
It is easy to see why  $R(\mu_0 - \alpha,0) >  R(\mu_{N} - \alpha,N)$.

\begin{align}
R(k,0) = \frac{\binom{N}{k} q^kp^{N-k}}{\binom{N}{k-1} q^{k-1}p^{N-k+1}} = \frac{N-k+1}{k} \cdot \frac{q}{p} \\
R(k,N) = \frac{\binom{N}{k} q^kp^{N-k}}{\binom{N}{k-1} q^{k-1}p^{N-k+1}} = \frac{N-k+1}{k} \cdot \frac{p}{q} 
\end{align}

Setting  $k=\mu - \alpha$ for each case, we have
\begin{align}
R(k,0) =  \frac{N-qN + \alpha+1}{qN-\alpha} \cdot \frac{q}{p} \approx \frac{qp + \frac{\alpha q}{N}}{qp - \frac{\alpha p}{N}}\\
R(k,N) = \frac{N-pN + \alpha+1}{pN-\alpha} \cdot \frac{p}{q} \approx \frac{qp + \frac{\alpha p}{N}}{qp - \frac{\alpha q}{N}}\\
\frac{qp + \frac{\alpha q}{N}}{qp - \frac{\alpha p}{N}} >  \frac{qp + \frac{\alpha p}{N}}{qp - \frac{\alpha q}{N}} \\
(qp + \frac{\alpha q}{N})(qp - \frac{\alpha q}{N}) > (qp + \frac{\alpha p}{N})(qp - \frac{\alpha p}{N}) \\
(qp)^2 - \left ( \frac{\alpha q}{N} \right )^2 > (qp)^2 - \left ( \frac{\alpha p}{N} \right )^2 
\end{align}

Since $p>q$, the above inequality holds.

But I am unable to express $R(k,m)$ analytically, no prove that it is bounded between by ratios corresponding to the limiting binomial distribution.
Any help with proving this and/or pointers to related papers will be much appreciated 


\end{document}

