%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,draft]{article}
%\documentclass[11pt,draft]{amsart}

%% Custom styling.
\usepackage{mozdp}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}

%% Some convenience aliases.
\newcommand{\Dsp}{\mathcal{D}}
\newcommand{\Ssp}{\mathcal{S}}
\newcommand{\Bsp}{\mathcal{B}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\sv}{\mathbf{s}}
\newcommand{\mv}{\mathbf{m}}
\newcommand{\ev}{\mathbf{e}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\pv}{\mathbf{p}}
\newcommand{\Pm}{\mathbf{P}}
\newcommand{\xvt}{\tilde{\xv}}
\newcommand{\yvt}{\tilde{\yv}}
\newcommand{\sm}{\sv^-}
\newcommand{\iv}{\mathbf{i}}
\newcommand{\one}{\boldsymbol{1}}
\newcommand{\zero}{\boldsymbol{0}}
%% Title matter
\title{K-Randomization}
\author{Maxim Zhilyaev \and David Zeber}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Differential Privacy}
\label{sec:dp}

The typical setting for differential privacy is as follows. 
We consider a \textbf{database} as a collection of records. 
Each record is an element of some space $\Dsp$, and a database $\xv$ is a vector of $n$ records: $\xv = (x_1,\dots,x_n) \in \Dsp^n$.

We wish to release information retrieved from the database by means of a \textbf{query}, a function $A$ mapping the database into another space: $\map{A}{\Dsp^n}{\Ssp}$. The result of applying a query to a database is termed a \textbf{transcript}.
The query usually applies some aggregation to the database records, and so the output space $\Ssp$ is generally of lower dimensionality than the original database.
If the query is \textbf{randomized}, \ie $A(\xv) = A(\xv; \xi)$ for a random element $\xi$, then the transcript will be a random element of $\Ssp$.

The notion of differential privacy for a database query is that the resulting transcript does not change substantially when a record in the database is modified,
\ie transcripts are not sensitive to particular individual records in the database.
Hence, releasing query transcripts publicly will not jeopardize privacy, since information regarding individual records cannot be gained by analyzing query transcripts.

%%% Differential privacy definition

Differential privacy for a randomized query $A$ is formulated by comparing the transcripts generated by applying $A$ to two very similar databases $\xv,\xv' \in \Dsp^n$.
We say the databases \textbf{differ in one row} if 
$\sum_{i=1}^n I(x_i \neq x'_i) = 1$.
\begin{defn}
A randomized query $A$ is $\epsilon$-\textbf{differentially private} if, 
for any two databases $\xv,\xv' \in \Dsp^n$ differing in one row,
\begin{equation}\label{eq:dpdef}
\P[A(\xv) \in S] \leq \exp(\epsilon) \cdot \P[A(\xv') \in S]
\end{equation}
for all $S \cont \Ssp$ (measurable).
\end{defn}
In other words, the transcripts from the two databases databases differing in one row are close in distribution.
An alternative notion of differing in one row that is sometimes used is that $\xv'$ includes an additional record that is not in $\xv$:
$\xv\in \Dsp^n$, $\xv' \in \Dsp^{n+1}$, and $x_i = x'_i$ for $i = 1,\dots,n$.

%%% Countable output space

If $\Ssp$ is finite, which is common in cases where the transcript involves integer counts, then the distribution of the transcript $A(\xv)$ can be represented using its pmf $\P[A(\xv) = s]$ for $s \in \Ssp$.
In this case, the differential privacy condition can also be expressed in terms of the pmf.
\begin{prop} \label{prop:dpcnt}
If $\Ssp$ is finite, then $A$ is $\epsilon$-\textbf{differentially private} if and only if
\begin{equation} \label{eq:dpcnt}
\P[A(\xv) = s] \leq \exp(\epsilon) \cdot \P[A(\xv') = s]
\end{equation}
for all $s\in\Ssp$, where $\xv,\xv'$ differ in one row.
\end{prop}
\begin{pf}
\pfleftdir
Given $S \cont \Ssp$, we can write
$\P[A(\xv) \in S] = \sum_{s \in S} \P[A(\xv) = s]$.
If $\P[A(\xv') \in S] = 0$, then $\P[A(\xv') = s] = 0$ for each $s \in S$.
From \eqref{eq:dpcnt} we have that $\P[A(\xv) = s] = 0$ as well, and so $P[A(\xv) \in S] = 0$, verifying \eqref{eq:dpdef}.
Otherwise, if $\P[A(\xv') \in S] > 0$,
\[ \frac{\P[A(\xv) \in S]}{\P[A(\xv') \in S]}
= \frac{\sum_{s \in S} \P[A(\xv) = s]}{\sum_{s \in S} \P[A(\xv') = s]}
\leq \max_{s \in S} \frac{\P[A(\xv) = s]}{\P[A(\xv') = s]}
\leq \exp(\epsilon), \]
using Lemma \ref{lem:rsbound}.\\
\pfrightdir Take $S = \{s\}$ in \eqref{eq:dpdef}.
\end{pf}


\section{Randomization of bit vectors}

Our goal is to establish differential privacy properties for user data reported
in the form of vectors of bits.
To protect user privacy, each user record is randomized prior to leaving the
client and anonymized on reaching the server.
We now describe the randomization procedure, and place ourselves in the setting
of Section \ref{sec:dp} by representing it as a query applied to a database.

\subsection{Bit randomization}

For our purposes, a \textbf{bit} is an integer $b \in \{0,1\}$, and a
\textbf{bit vector} is a vector $x \in \Bsp_L := \{0,1\}^L$.
Bits and bit vectors are randomized in the following way.

\begin{defn}
The \textbf{bit randomization} procedure $R$ with \textbf{lie probability} $0 < q < 1/2$ flips a bit $b$ with probability $q$, and leaves it as-is with probability $p := 1-q$:
\[ R(b) =
\begin{cases}
b & \text{with prob }p \\
1-b & \text{with prob }q
\end{cases}.
\]
This can be expressed concisely as
\[ R(b) = R(b; \xi) = b \cdot \xi + (1-b) \cdot (1-\xi)
\quad\text{where } \xi\sim Ber(p). \]
We extend the procedure to \textbf{bit vector randomization} by applying the randomization independently to each bit in the vector.
Given a bit vector $x = (b_1,\dots,b_L)$, define
\begin{equation}\label{eq:bitvecrand}
R(x) = R(x;\xi) = \big(R(b_1;\xi_1), \dots, R(b_L;\xi_L)\big)
\quad\text{where } \xi = (\xi_1,\dots,\xi_L) \iid Ber(p).
\end{equation}
\end{defn}

\begin{rmk}
Note that $R$ reports the original bit value with probability $p = 1-q > q$, and lies with probability $q$.
This is equivalent to the randomized response procedure where the value is reported as-is with probability $1-f$, and with probability $f$ the reported value is the outcome of the toss of a fair coin.
In our case, $q = f/2$.
\end{rmk}
\begin{rmk}
If $q = 1/2$, then $R(0)\eqdist R(1)$, and the reported value is ``completely'' randomly generated, \ie independently of the original value.
\end{rmk}

%%% Distribution of randomized bit vectors.

The distribution of the randomized bit vectors can be expressed in terms of
the Hamming distance between the original and randomized vectors,
\[ \delta(x,x') = \sum_{\ell = 1}^L I(x_\ell \neq x'_\ell)
= \sum_{\ell = 1}^L |x_\ell - x'_\ell|. \]
For a single bit, the randomization has lied when the outcome is different from the original value: 
\[ \P[R(b) = b'] = p^{I(b = b')}\cdot q^{I(b \neq b')}
= p^{1 - \delta(b,b')}\cdot q^{\delta(b,b')}. \]
For a bit vector $x$, this becomes
\[ \P[R(x) = y] = p^{\sum I(x_\ell = y_\ell)}\cdot 
q^{\sum I(x_\ell \neq y_\ell)}
= p^{L-\delta(x,y)}\cdot q^{\delta(x,y)}. \]
Note that this probability is maximized when $\delta(x,y) = 0$ (the randomized vector $y$ is identical to the original vector $x$), and minimized when $\delta(x,y) = L$.
In the latter case, we say that $y$ is the \textbf{opposite} of $x$.
In other words, the most likely outcome of randomizing a bit vector is obtaining an identical vector.

\subsection{Randomization on $\Bsp_L$}
\label{sec:transprobs}

In general, given $x\in\Bsp_L$, the mapping $R(x)$ is itself a random element of
$\Bsp_L$, and its distributions over $x$ can be represented by a transition
matrix which admits a convenient form.
This in turn depends on enumerating the space $\Bsp_L$ as described below.

In the following we rely heavily on the fact that $|\Bsp_L| = 2^L$, as well
as the symmetry and recursive structure between ``halves'' of the cube
whose vertices constitute $\Bsp_L$.
To this end, we introduce the following notation. The explicit dependence
on $L$ is dropped when its value is clear.
Let
\[ D(L) = 2^L \qquad\text{and}\qquad d(L) = \frac{D(L)}{2} = 2^{L-1}. \]
Split the index range $1,\dots, D(L)$ into
\[ J_1(L) = \big\{1,\dots,d(L)\big\} \qquad\text{and}\qquad
J_2(L) = \big\{d(L) + 1,\dots, D(L) \big\}. \]
Thus, $J_2 = J_1 + d$ and $J_1(L) = J_1(L-1) \cup J_2(L-1)$.
Finally, define the index mappings
\[ \chi_L(i) = \begin{cases} i & i \in J_1(L) \\ i - d(L) & i \in J_2(L)
\end{cases} \qquad\text{and}\qquad
\kappa_L(i) = \begin{cases} i + d(L) & i \in J_1(L) \\ i & i \in J_2(L)
\end{cases}, \]
projecting an index in $1,\dots,D$ onto $J_1$ and $J_2$, respectively.

\begin{defn}
The \textbf{recursive enumeration} of $\Bsp_L$ is a labelling of its elements
as $\big(v_1^L,v_2^L,\dots,v_{D(L)}^L\big)$, ordered as follows:
\begin{itemize}
\item when $L=1$, $(v_1^1, v_2^1) := (1, 0)$;
\item for $L > 1$,
\[ v_j^L := \begin{cases}(1, v_j^{L-1}) & j \in J_1(L) \\
    (0, v_{\chi_L(j)}^{L-1}) & j \in J_2(L) \end{cases}. \]
\end{itemize}
\end{defn}

The recursive enumeration orders the vectors bitwise, placing 1 before 0.
For example, when $L=2$,
$\big(v_1^2, v_2^2, v_3^2, v_4^2\big) = \big((11), (10), (01), (00)\big)$.
Alternatively, if the bit vectors are viewed as binary representations of
integers, the recursive enumeration places them in decreasing order.

Assume from now on that $v_j^L$ refers to the recursive enumeration of
$\Bsp_L$, and write the randomization probabilities as
\[ p_L(i,j) = \P[R(v_i^L) = v_j^L]. \]
Since bits within a vector are randomized independently, we have
\begin{equation} \label{eq:transprobs}
p_L(i,j) = \begin{cases}
p\cdot p_{L-1}(i, j) &
    i \in J_1,\; j \in J_1 \\
q\cdot p_{L-1}(i,\chi(j)) &
    i \in J_1,\; j \in J_2 \\
q\cdot p_{L-1}(\chi(i),j) &
    i \in J_2,\; j \in J_1 \\
p\cdot p_{L-1}(\chi(i), \chi(j)) &
    i \in J_2,\; j \in J_2
 \end{cases}.
\end{equation}
The distribution of $R(x)$ is summarized by the transition matrix
\[ \Pm_L = \Big(p_L(i,j)\,;\; 1 \leq i,j \leq D(L)\Big). \]
Note that this is a doubly stochastic, symmetric matrix with all $p^L$ on
the diagonal and $q^L$ on the antidiagonal.
Furthermore, we have
\[ \Pm_1 = \begin{pmatrix}p & q \\ q & p \end{pmatrix} \]
and, as a consequence of \eqref{eq:transprobs},
\[ \Pm_L = \begin{pmatrix} p\Pm_{L-1} & q\Pm_{L-1} \\
q\Pm_{L-1} & p\Pm_{L-1} \end{pmatrix}. \]
In other words,
\[ \Pm_L = \Pm_1 \otimes \Pm_{L-1}, \]
where $\otimes$ denotes the Kronecker product.


\section{Privacy-preserving reporting for bit records}

We now adapt the framework of Section \ref{sec:dp} to the task of reporting
user records encoded as bit-vectors.

Set $\Dsp = \Bsp_L = \{v_1,\dots, v_D\}$, ordered according to the recursive
enumeration described in Section \ref{sec:transprobs}.
We use the term \textbf{collection} (of records) interchangeably with
``database''.
We consider a randomized query $A$ that randomizes each record in the
collection independently, and aggregates the results by reporting occurrence
counts for every possible randomization outcome.
We adopt this aggregation step as a model for \textbf{anonymization}.
After anonymization, we cannot associate a specific randomized record with
a specific record out of the original collection, and so the ordering of
records in the synthetic collection conveys no information about the ordering
of the original records.

TODO: can represent collections using matrices, and write $\Phi$ as 
$X^T\mathbf{1}$ for a collection $X$ (column sums).

We consider an aggregation function $\Phi$ mapping a collection of $n$ elements
of $\Dsp$ to a vector $s = (s_1,\dots,s_D)$, where $s_j$ counts how many copies
of $v_j$ the collection contains.
Note that the range of $\Phi$ is
\[ \Ssp_n^L := \big\{ s \in \{0,\dots,n\}^{D(L)}:
%s_j \geq 0,\,
s_1 +\cdots + s_{D(L)} = n\big\}, \]
a subset of the standard $(D - 1)$-simplex (whose vertices are the standard
basis vectors in $\R^D$) consisting of the points with integer coordinates.

\begin{defn}
The aggregation function $\map{\Phi}{\Dsp^n}{\Ssp_n}$ counts occurrences of
vectors $v_1,\dots, v_D \in \Dsp$ in the collection $\yv \in \Dsp^n$:
\[ \Phi(\yv) = \left(\sum_{i = 1}^n I(y_i = v_1),\dots,
\sum_{i = 1}^n I(y_i = v_D) \right)
\]
\end{defn}

We also extend the bit vector randomization \eqref{eq:bitvecrand} to
collections $\xv\in\Dsp^n$ by applying it independently to each vector in the
collection:
\[ R(\xv) = R(\xv;\boldsymbol{\xi})
= \big(R(x_1;\xi_1),\dots,R(x_n; \xi_n)\big)
\quad\text{where } \xi_i = (\xi_{i1},\dots,\xi_{iL}) \text{ and }
\xi_{i\ell} \iid Ber(p). \]
We call $R(\xv)$ the \textbf{synthetic} collection obtained from the
\textbf{original} collection $\xv$.
Note that $R(\xv)$ is itself a random element of $\Dsp^n$, with distribution
given by
\[ \P[R(\xv) = \yv] = \prod_{i=1}^n \P[R(x_i) = y_i] =
\prod_{i=1}^n p^{L-\delta(x_i,y_i)}q^{\delta(x_i,y_i)} =
p^{nL} \bigg(\frac{q}{p}\bigg)^{\sum_{i=1}^n \delta(x_i, y_i)}.
\]
Thus, $\P[R(\xv) = \yv] > 0$ for any $\yv\in\Dsp^n$, \ie any synthetic
collection of size $n$ has a non-zero probability of being generated from any
given original collection of the same size.

The randomized query we use for reporting collections of bit vectors may now
be defined as follows.

\begin{defn}
The randomized query $\map{A}{\Dsp^n}{\Ssp_n}$ maps collections of bit vectors
to occurrence counts according to
\[ A = \Phi \circ R. \]
\end{defn}

\subsection{The distribution of $A$}

For $s\in\Ssp_n$, the distribution of $A(\xv)$ is given by
\begin{equation}\label{eq:Adist}
\P[A(\xv) = s] = \P[\Phi(R(\xv)) = s]
= \sum_{\yv:\Phi(\yv) = s} \P[R(\xv) = \yv]
%= \sum_{\yv:\Phi(\yv) = s} \prod_{i=1}^n \P[R(x_i) = y_i]
.
\end{equation}
Note that the set of synthetic collections included in the summation is
$[s] := \{\yv:\Phi(\yv) = s\}$, an equivalence class of $\Phi$.
We consider as its canonical representative the collection
\[ \vv(s) := \big(\underbrace{v_1,\dots,v_1}_{s_1},\,
   \underbrace{v_2,\dots,v_2}_{s_2},\,\dots,\,
   \underbrace{v_D,\dots,v_D}_{s_D}\big), \]
which contains $s_j$ consecutive copies of $v_j$, ordered as usual.
Observe that $[s]$ coincides with the set of all permutations of $\vv(s)$.

We now show that $\P[A(\xv) = s]$ is invariant under reordering the original
collection $\xv$.

\begin{prop}
For fixed $s\in\Ssp_n$, $\P[A(\xv) = s] = \P[A(\xv') = s]$ whenever
$\Phi(\xv) = \Phi(\xv')$.
\end{prop}
\begin{pf}
If $\Phi(\xv) = \Phi(\xv')$, there is a permutation $\sigma$ of
$\{1,2,\dots,n\}$ such that
$\xv'_i = \xv_{\sigma(i)}$, $1 \leq i \leq n$.
Hence,
\begin{align*}
\P[A(\xv') = s] &= \sum_{[s]} \P[R(\xv') = \yv] =
\sum_{[s]} \prod_{i = 1}^n \P[R(x_{\sigma(i)}) = y_i] =
\sum_{[s]} \prod_{i = 1}^n \P[R(x_{i}) = y_{\inv{\sigma}(i)}] = \\
&\sum_{\sigma\cdot\yv : \yv \in [s]} \P[R(\xv) = \yv].
\end{align*}
Furthermore, $\{\sigma\cdot\yv : \yv \in [s]\} = [s]$.
Indeed, if $\yv\in [s]$, then any permutation of $\yv$ remains in $[s]$,
since the tallies of unique components of $\yv$ remain unchanged.
On the other hand, if $\mathbf{w} \in [s]$, consider
$\yv' = \inv{\sigma}\cdot\mathbf{w} \in [s]$.
We have $(\sigma\cdot\yv')_i = y'_{\inv{\sigma}(i)} =
w_{\sigma(\inv{\sigma}(i))} = w_i$.
Hence $\mathbf{w} \in \{\sigma\cdot\yv : \yv \in [s]\}$.
We conclude that
\[ \P[A(\xv') = s] = \sum_{\sigma\cdot\yv : \yv \in [s]} \P[R(\xv) = \yv] =
\sum_{[s]} \P[R(\xv) = \yv] = \P[A(\xv) = s]. \]
\end{pf}

Consequently, we view $A$ as a mapping $\Ssp_n \rightarrow \Ssp_n$, and denote
$A(m) := A(\vv(m))$ for $m\in\Ssp_n$ summarizing the original collection.

%%% Distribution of A(m) as a sum of multinomials.

We now discuss a more convenient characterization of the distribution of $A(m)$
given in \eqref{eq:Adist}.
First, observe that if $m = n\cdot \ev_\ell$, where $\ev_\ell$ is the $\ell$-th
standard basis vector in $\Ssp_n$, then all of the vectors in the original
collection are identical and equal to $v_\ell$.
In this case, $A(m)$ has a Multinomial distribution:
\begin{align*}
\P[A(m) = s] &= \sum_{[s]} \prod_{i=1}^n \P[R(v_\ell) = y_i] =
\sum_{[s]} \prod_{j = 1}^D\prod_{i: y_i = v_j} p(\ell, j) =
\sum_{[s]} p(\ell, 1)^{s_1}\cdots p(\ell, D)^{s_D} \\
&= \frac{n!}{s_1!\cdots s_D!} p(\ell, 1)^{s_1}\cdots p(\ell, D)^{s_D},
\end{align*}
since $[s]$ consists of all permutations of $\vv(s)$.

In general, suppose $m = (m_1,\dots,m_D)\in\Ssp_n$.
Since $\Phi(\yv) = \Phi(y_1) + \cdots + \Phi(y_n)$, we have
\[ A(m) = \Phi\big(R\big(\vv(m)\big)\big) =
\sum_{j=1}^D \Phi\big(R\big((\underbrace{v_j,\dots,v_j}_{s_j})\big)\big) =
\sum_{j=1}^D A\big((v_j,\dots,v_j)\big), \]
where the terms of the sum are independent since $R$ randomizes each vector
component independently.
Therefore, $A(m)$ is distributed as a sum of Multinomial random vectors:
\begin{equation} \label{eq:Adistmn}
A(m) \sim MN(m_1, \pv_L(1)) + \cdots + MN(m_D, \pv_L(D)),
\end{equation}
where $\pv_L(j)$ denotes the $j$-th row in the transition matrix
$\Pm_L$, \ie $\pv_L(j) = \big(p_L(j,1),\dots,p_L(j,D)\big)$.

Furthermore, we can leverage the binary recursive structure of $\Dsp$ to get a
more specific representation.
Let
\[ M_i \sim MN(m_i,\; \pv_L(i)), \quad i = 1,\dots,D. \]
We first show that, as a consequence of \eqref{eq:transprobs}, we can express
the distribution of $M_i$ in terms of $\pv_{L-1}(i)$.
This is accomplished by representing an outcome of $M_i$ as follows.
Split the multinomial categories into two groups of $d = D/2$, indexed by $J_1$
and $J_2$ respectively.
Out of the $m_i$ Bernoulli trials generating the multinomial outcome, determine
the number assigned to each group according to a Binomial random variable.
Given this value, arrange the assigned results to specific categories
independently in each group according to a Multinomial over those $d$
categories.

For convenience, define the partial summation operator $\xi$ on $\R^D$ as
\[ \xi x = \sum_{j \in J_1} x_j, \]
and denote $x_{J_k} = (x_j,\; j \in J_k)$.

\begin{lem}
Given $i \in \{1,\dots,D\}$, let
\[ (W, S_1, S_2) \in \{0,\dots,m_i\} \times \{0,\dots,m_i\}^d \times
\{0,\dots,m_i\}^d \]
be a random element distributed according to
\begin{align}
W &\sim \begin{cases} Bin(m_i,\; p) & i \in J_1 \\
Bin(m_i,\; q) & i \in J_2 \end{cases} \nonumber \\
%Bin\big(m,\; p \cdot I_{J_1}(i) + q \cdot I_{J_2}(i)\big)\nonumber\\
S_1 \given W = w &\sim MN\big(w,\; \pv_{L-1}(\chi(i))\big) \label{eq:mndecomp}\\
S_2 \given W = w &\sim MN\big(m_i - w,\; \pv_{L-1}(\chi(i))\big), \nonumber
\end{align}
where $S_1$ and $S_2$ are conditionally independent given $W$.
Then, for $s \in \Ssp_{m_i}^L$,
\[ \P[M_i = s] = \P\big[(W, S_1, S_2) = (\xi s,\; s_{J_1},\; s_{J_2})
\big]. \]
\end{lem}
If $L=1$, this simplifies to $M_i \eqdist (W, m_i - W)$.
\begin{pf}
Taking $a = \xi \pv_L(i)$ and $w = \xi s$, we have
\begin{align*}
\P[M_i = s] &= \frac{m_i!}{s_1!\cdots s_D!} p_L(i,1)^{s_1} \cdots
p_L(i,D)^{s_D} \\
&= \frac{m_i!}{w! (m_i - w)!} a^w (1 - a)^{(m_i - w)} \cdot
\frac{w!}{s_1!\cdots s_d!} \frac{p_L(i,1)^{s_1}}{a^{s_1}}\cdots
\frac{p_L(i, d)^{s_d}}{a^{s_d}} \\
&\hspace{12em}\cdot
\frac{(m_i - w)!}{s_{d+1}!\cdots s_D!}
\frac{p_L(i,d + 1)^{s_{d + 1}}}{(1 - a)^{s_{d+1}}}\cdots
\frac{p_L(i,D)^{s_D}}{(1 - a)^{s_D}}.
\end{align*}
By \eqref{eq:transprobs}, $a = p'\cdot\sum_{j \in J_1(L)} p_{L-1}(i, j) = p'$,
where $p' = p$ if $i \in J_1$ and $q$ otherwise.
Furthermore, for $j\in J_1$,
\[ \frac{p_L(i,j)}{a} = \frac{p_L(i,\kappa(j))}{1-a} = p_{L-1}(\chi(i), j). \]
Therefore,
\begin{equation*}
\P[M_i = s] = \P[W = w]\cdot \P[MN(w,\; \pv_{L-1}(\chi(i))) = s_{J_1}]\cdot
\P[MN(m_i - w,\; \pv_{L-1}(\chi(i))) = s_{J_2}].
\end{equation*}
\end{pf}

Next, we extend this representation to sums of the (independent) $M_i$.
Let $Y_i = M_i + M_{\kappa(i)}$.

\begin{lem}
Given $i \in \{1,\dots,d\}$, let
\[ (T, U, V) \in \{0,\dots,m_i + m_{\kappa(i)}\} \times
\{0,\dots,m_i + m_{\kappa(i)}\}^d \times \{0,\dots,m_i + m_{\kappa(i)}\}^d \]
be a random element distributed according to
\begin{align}
T &\sim Bin\big(m_i,\; p\big) + Bin\big(m_{\kappa(i)},\; q\big)
\quad \text{(independent)}\nonumber\\
U \given T = t &\sim MN\big(t,\; \pv_{L-1}(i)\big) \nonumber\\
V \given T = t &\sim MN\big(m_i + m_{\kappa(i)} - t,\; \pv_{L-1}(i)\big),
\nonumber
\end{align}
where $U$ and $V$ are conditionally independent given $T$.
Then, for $s \in \Ssp_{m_i + m_{\kappa(i)}}^L$,
\[ \P[Y_i = s] = \P\big[(T, U, V) = (\xi s,\; s_{J_1},\; s_{J_2})
\big]. \]
\end{lem}
\begin{pf}
Let $(W_k, S_{k1}, S_{k2})$, $k = i, \kappa(i)$, be independent random elements
distributed according to \eqref{eq:mndecomp}.
Define
\[ \Ssp_r(s) = \big\{s' \in \Ssp_r^L : s'_j \leq s_j,\, j = 1,\dots,D\}, \]
and observe that $\Ssp_r(s)$ can be decomposed as
\begin{align*}
\Ssp_r(s) &= \bigcup_{t = 0}^r
\big\{ s' \in \Ssp_r^L: s'_j \leq s_j,\, j = 1,\dots,D;\; \xi s' = t \big\} \\
&= \bigcup_{t = 0}^r
\big\{ u \in \Ssp_t^{L-1}: u_j \leq s_j, \; j \in J_1(L) \} \times
\big\{ v \in \Ssp_{r - t}^{L-1} : v_j \leq s_{\kappa(j)},\;
j \in J_1(L) \} \\
&= \bigcup_{t = 0}^r \Ssp_{t}\big(s_{J_1}\big) \times
\Ssp_{r - t}\big(s_{J_2}\big)
\end{align*}
(note that some of the sets in the union may be empty, for example if
$t > \xi s$).
Now,
\[ \P[Y_i = s] = \sum_{s'} \P[M_i = s'] \P[M_{\kappa(i)} = s - s'], \]
where the summation runs over
$\big\{ s' : s' \in \Ssp_{m_i}^L,\; s-s' \in \Ssp_{m_{\kappa(i)}}^L \big\}
= \Ssp_{m_i}(s)$.
Hence,
\begin{align*}
\P[Y_i = s] &= \sum_{t = 0}^{m_i} \sum_{u \in \Ssp_t(s_{J_1})}
\sum_{v \in \Ssp_{m_i - t}(s_{J_2})}
\P\big[(W_i, S_{i1}, S_{i2}) = \big(t, u, v\big)\big] \\
&\hspace{12em} \cdot
\P\big[(W_{\kappa(i)}, S_{\kappa(i)1}, S_{\kappa(i)2}) =
\big(\xi s - t,\; s_{J_1} - u,\; s_{J_2} - v\big)\big] \\
&= \sum_{t = 0}^{m_i} \P[W_i = t] \P[W_{\kappa(i)} = \xi s - t] \\
&\hspace{6em}\cdot
\bigg\{ \sum_{u \in \Ssp_t(s_{J_1})} \P[S_{i1} = u \given W_i = t]
\P[S_{\kappa(i)1} = s_{J_1} - u \given W_{\kappa(i)} = \xi s - t] \bigg\} \\
&\hspace{6em}\cdot \bigg\{ \sum_{v \in \Ssp_{m_i - t}(s_{J_2})}
\P[S_{i2} = v \given W_i = t]
\P[S_{\kappa(i)2} = s_{J_2} - v \given W_{\kappa(i)} = \xi s - t] \bigg\} \\
&= \sum_{t = 0}^{m_i} \P[W_i = t] \P[W_{\kappa(i)} = \xi s - t]
\cdot \P[S_{i1} + S_{\kappa(i)1} = s_{J_1} \given
W_i = t, W_{\kappa(i)} = \xi s - t] \\
&\hspace{17em}\cdot \P[S_{i2} + S_{\kappa(i)2} = s_{J_2} \given
W_i = t, W_{\kappa(i)} = \xi s - t].
\end{align*}
Recall that, given $W_k$, $S_{k1}$ and $S_{k2}$ are independent
Multinomial random vectors, all with the same category probabilities
$\pv_{L-1}(i)$.
Thus, given $W_i + W_{\kappa(i)} = \xi s$,
\[ S_{i1} + S_{\kappa(i)1} \sim MN\big(\xi s,\, \pv_{L-1}(i)\big)
\qquad\text{and}\qquad
S_{i2} + S_{\kappa(i)2} \sim MN\big(m_i + m_{\kappa(i)} - \xi s,\,
\pv_{L-1}(i)\big). \]
Therefore,
\begin{align*}
\P[Y_i = s] &= \P[W_i + W_{\kappa(i)} = \xi s] \cdot
\P[S_{i1} + S_{\kappa(i)1} = s_{J_1}] \cdot
\P[S_{i2} + S_{\kappa(i)2} = s_{J_2}] \\
&= \P[T = \xi s] \cdot \P[U = s_{J_1} \given T = \xi s] \cdot
\P[V = s_{J_2} \given T = \xi s].
\end{align*}
\end{pf}

\hrulefill




%we pursue a similar result for the full distribution of $A(m)$.
Let $M_i \sim MN(m_i,\; \pv_L(i))$ and $s\in \Ssp_n$, and write
.
For $a \in \{0,\dots,n\}^k$, define
\begin{equation*}
\Ssp(s; a) := \big\{ (s'_1,\dots,s'_k) :
s'_j \in \Ssp_{a_j}, 1 \leq j \leq k;\ s'_1 + \cdots + s'_k \leq s \big\},
\end{equation*}
a collection of multinomial outcomes from various sets of trials which
constitute a subset of the outcome $s$.
Observe that
%for $s'\in \Ssp(s; a)$,
%\[ s'_1 \in \Ssp(s, a_1),\qquad s'_2 \in \Ssp(s - s'_1, a_2), \quad\dots,\quad
%s'_k \in \Ssp(s - (s'_1 + \cdots + s'_{k-1}), a_k). \]
\[ \Ssp(s; a) = \big\{(s'_1,\dots, s'_k) : (s'_1,\dots,s'_{k-1}) \in 
\Ssp(s; (a_1,\dots,a_{k-1})),\ s'_k \in \Ssp(s-(s'_1 + \cdots + s'_{k-1}); a_k)
\big\}. \]
Let $d = 2^{L-1}$, and take
$a = \big(m_{i} + m_{\kappa(i)}, i = 1,\dots,d\big)$.
Then, conditioning on the $Y_i$, we have
\begin{align*}
\P&[M_1 + \cdots + M_{2^L} = s] =
\sum_{s' \in \Ssp(s; a)} \P[Y_1 = s'_1]\cdots \P[Y_d = s'_d] \\
&= \sum_{(s'_1,\dots,s'_{d-2}) \in \Ssp(s; (a_1,\dots,a_{d-2}))}
\P[Y_1 = s'_1]\cdots \P[Y_{d-2} = s'_{d-2}] \\
&\hspace{8em}\cdot
\sum_{s'_{d-1} \in \Ssp(\tilde{s}; a_{d-1})}
\P[Y_{d-1} = s'_{d-1}] \P[Y_d = \tilde{s} - s'_{d-1}],
\end{align*}
where $\tilde{s} = s - (s'_1 + \cdots + s'_{d-2})$.

\hrulefill

Note that $(Y_1,\dots,Y_{2^{L-1}})$ takes values in
Let $\tilde{s}_0 = (s'_1,\dots,s'_{2^{L-1}-2})$,
$\tilde{s} = s'_{2^{L-1} - 1}$, and $\tilde{t} = \sum (\tilde{s}_0)_j$.
The previous sum becomes

\hrulefill


\subsection{Next steps}

\begin{itemize}
\item
View the randomized query $A$ as a mapping from $\Ssp_n$ to $\Ssp_n$.
Hamming distance on $\Dsp^n$ corresponds to $L_1$ metric on $\Ssp_n$.

\item
Privacy ratio takes as input two ``neighbouring'' original collections (defined
in terms of a metric on $\Ssp_n$) and a synthetic one. Behaviour of privacy
ratio can be studied in terms of ``probability ratio'', a mapping over a single
original collection and two neighbouring synthetic ones.

\item
Define the \textbf{privacy range} as a subset of the most likely synthetic
outcomes for a given original collection.

\item
Goal is to uniformly bound the privacy ratio over all privacy ranges
corresponding to all possible original collections.

\item
Work with privacy ratio for $x = \one$ and $x' = \zero$. Argue by symmetry that
the bound on the privacy ratio only depends on the choice of $x$ and $x'$,
regardless of remaining elements of the collection.
Argue that without loss of generality we can choose $x'$ to be the opposite of
$x$.

\item
Show that the privacy ratio is increasing as the synthetic collection moves
towards the one consisting of all $\zero$.
Direction of maximal increase is along the line connecting $\mu_{\mv} =
\EP[\sv\given\mv]$ (must be on the line, or just in the direction?)

\item
Think of the privacy range in terms of a neighbourhood of $\mu_{\mv}$.
\end{itemize}

\hrulefill

\subsection{Old stuff}


(TODO: maybe this is not useful at this point)
Writing $m(\ell) := \sum_{i=1}^n I(\delta(x_i, y_i) = \ell)$ for
$\ell = 0,\dots,L$, the probability can be expressed as
\[ \P[R(\xv) = \yv] = \prod_{\ell = 0}^L \big(p^{L-\ell}q^\ell\big)^{m(\ell)}.
\]
Hence, the distribution is determined by the magnitudes of the pairwise
distances, and does not depend on the ordering within the collections.

\begin{align}
&= \sum_{\yv:\Phi(\yv) = s} \prod_{i=1}^n p^{L - \delta(x_i,y_i)}\cdot
q^{\delta(x_i,y_i)}
= \sum_{\yv:\Phi(\yv) = s} p^{nL - \delta(\xv,\yv)}\cdot
q^{\delta(\xv,\yv)}  \nonumber \\
&= p^{nL}\sum_{\yv:\Phi(\yv) = s} \left(\frac{q}{p}\right)^{\delta(\xv,\yv)}
\end{align}


where $\delta(\xv,\yv) = \sum_{i=1}^n \delta(x_i,y_i)$ can be considered the distance between collections $\xv$ and $\yv$.
Note that the support of $A$ is the support of a multinomial random variable with $n$ trials: \[\Ssp_n := \bigg\{s \in \Ssp : \sum_j s_j = n \bigg\},\]

Define the partial aggregation function
$\map{\Psi_m}{\Dsp^n}{\Ssp_{m_1} \times \cdots \times \Ssp_{m_{2^L}}}$ as
\[ \Psi_m(\yv)= \Big(\Phi\big((y_1, \dots, y_{m_1})\big),
\Phi\big((y_{m_1 + 1}, \dots, y_{m_1 + m_2})\big),\dots,
\Phi\big((y_{n - m_{2^L} + 1}, \dots, y_{n})\big)\Big), \]
and observe that
\[ \Phi(\yv) = \big(\Psi_m(\yv)\big)_1 + \cdots + \big(\Psi_m(\yv)\big)_{2^L}.
\]
Thus, letting
\[ \Ssp_m(s) := \big\{ (u_1,\dots,u_{2^L}) : u_j \in \Ssp_{m_j},
u_1 + \dots + u_{2^L} = s \big\} \cont
\Ssp_{m_1} \times \cdots \times \Ssp_{m_{2^L}}, \]
we can write
\[ [s] = \bigcup_{\mathbf{u} \in \Ssp_m(s)}
\big\{ \yv : \Psi_m(\yv) = \mathbf{u} \big\} =
\bigcup_{\mathbf{u} \in \Ssp_m(s)} \prod_{j = 1}^{2^L}
\big\{\yv_j \in \Dsp^{m_j}: \Phi(\yv_j) = u_j \big\},
\]
where the product of sets denotes the Cartesian product.
Hence,
\begin{align*}
\P[A(m) = s] &=
\sum_{\mathbf{u} \in \Ssp_m(s)}
\sum_{\substack{\yv_1 \in \Dsp^{m_1}\\ \Phi(\yv_1) = u_1}}\cdots
\sum_{\substack{\yv_{2^L} \in \Dsp^{m_{2^L}}\\ \Phi(\yv_{2^L}) = u_{2^L}}}
\prod_{j = 1}^{2^L} \P\big[R\big((v_j,\dots,v_j)\big) = \yv_j\big] \\
&= \sum_{\mathbf{u} \in \Ssp_m(s)}\prod_{j = 1}^{2^L} 
\sum_{\substack{\yv_j \in \Dsp^{m_j}\\ \Phi(\yv_j) = u_j}}
\P\big[R\big((v_j,\dots,v_j)\big) = \yv_j\big] \\
&= \sum_{\mathbf{u} \in \Ssp_m(s)}\prod_{j = 1}^{2^L} 
\P[A(m^{(j)}) = u_j] \\
&= \P[M_1 + \cdots + M_{2^L} = s],
\end{align*}
where $M_j \sim MN(m_j, \pv_j)$ are independent Multinmoial random variables,
with $\pv_j$ denoting the $j$-th row in the transition matrix
$\Pm^L$, \ie $\pv_j = (p(j,1),\dots,p(j,2^L))$, and $m^{(j)}$ represents an
original collection of size $m_j$ consisting only of $v_j$.


\section{The privacy ratio}

We will study the differential privacy of the query $A$ in terms of the \textbf{privacy ratio}
\[ \pi(\sv;\xv,\xv') := \frac{\P[A(\xv') = \sv]}{\P[A(\xv) = \sv]} \]
for two collections $\xv,\xv'\in\Dsp^n$ differing in one row and $\sv \in\Ssp_n$.
Note that $\pi$ is well-defined, since any outcome in $\Ssp_n$ occurs with non-zero probability starting from any collection $\xv$.

Differential privacy is typically concerned with bounding the privacy ratio over all original and synthetic collections.
In particular, by Proposition \ref{prop:dpcnt}, $A$ is $\epsilon$-differentially private if $\pi$ is bounded everywhere on $\Ssp_n$,
with
\[ \epsilon = \max_{\substack{\sv\in\Ssp_n\\ \xv,\xv' \in \Dsp^n}} \log\pi(\sv;\xv,\xv'). \]
We take this further by studying the behaviour of the privacy ratio and how it varies across the support of $A$ and across collections. This will allow us to understand in which situations it is at or near its bound.

Without loss of generality (in light of \eqref{eq:Adist}), assume that the element differing between $\xv$ and $\xv'$ is the first one, and denote $\xvt = (x_2,\dots,x_n)$.
Then $\xv = (x,\xvt)$ and $\xv' = (x', \xvt)$.
Also write $\one_j = (0,\dots,0,1,0,\dots,0)$, the vector with a $1$ in the $j$-th position and the rest $0$, and let $\sv_{-j} := \sv - \one_j$.
%For convenience, introduce the notation
%$\sm_j = (s_1,\dots,s_{j-1},s_j - 1,s_{j+1},\dots,s_{2^L})$.

Conditioning on the value of the modified record, the privacy ratio becomes
\begin{equation} \label{eq:prcond}
\pi(\sv;\xvt,x,x') = \frac{\displaystyle\sum_{j=1}^{2^L}
\P[A(x') = \one_j] \P[A(\xvt) = \sv_{-i}]}
{\displaystyle\sum_{j=1}^{2^L}
\P[A(x) = \one_j] \P[A(\xvt) = \sv_{-i}]}
= \frac{\displaystyle\sum_{j=1}^{2^L}
\P[R(x') = d_j] \P[A(\xvt) = \sv_{-i}]}
{\displaystyle\sum_{j=1}^{2^L}
\P[R(x) = d_j] \P[A(\xvt) = \sv_{-i}]},
\end{equation}
where we understand $\P[A(\xvt) = \sv_{-i}] = 0$ if $\sv_i = 0$.
For convenience, we introduce the notation $p_j := \P[R(x) = d_j]$,
$p'_j := \P[R(x') = d_j]$, and $P(\sv, \xv) := \P[A(\xv) = \sv]$, and express
the privacy ratio as
\[ \pi(\sv;\xv,x,x') = \frac{\displaystyle\sum_{j=1}^{2^L}
p'_j P(\sv_{-i},\xv)}{\displaystyle\sum_{j=1}^{2^L} p_j P(\sv_{-i},\xv)}, \]
dropping the tilde so that $\xv$ now denotes a collection of size $n-1$.

Denote $p_{ij} := \P[R(d_i) = d_j]$ and $P_n(\sv,\mv) := \P[A_n(\mv) = \sv]$.

\subsection{The space $\Ssp_n$}

Recall that we consider the randomized query $A$ a mapping from the simplex
subset $\Ssp_n$ to itself.
Denote the standard basis vectors in $\R^{2^L}$ by
$\ev_j = (0,\dots, 0, 1, 0,\dots, 0)$.

Viewing a point $\sv = (s_1,\dots,s_{2^L}) \in\Ssp_n$ as a vector of counts,
we consider as its ``neighbours'' those points $\sv'$ for which one of the
counts differs by 1: $\abs{s_i - s'_i} = 1$ for some $j$.
Because of the constraint $\sum_k s_k = \sum_k s'_k = n$, neighbouring points
must differ in exactly two coordinates: $s_i - s'_i = 1 = s'_j - s_j$ for some
$i \neq j$.
In other words, neighbours of $\sv$ are those points belonging to the set
$\{\sv'\in\Ssp_n : \sv' = \sv + \ev_j - \ev_i, i \neq j \}$.
We call the vector $\ev_{ij} := \ev_j - \ev_i$ a \textbf{step from $i$ to $j$};
note that adding a step from $i$ to $j$ to the counts vector $\sv$ has the
effect of shifting one count from bucket $i$ to bucket $j$.
The neighbours of $\sv$ are then those points in $\Ssp_n$ that can be reached
in a single step. Note also that to step from $i$ to $j$ and remain in $\Ssp_n$,
$\sv$ must have a non-zero count in bucket $i$.

In fact, the neighbours of $\sv$ are also the closest points in $\Ssp_n$ in
the Euclidian sense.
The (squared) Euclidian distance between two points $\sv$ and $\sv'$ is given by
\begin{align*}
\norm{\sv - \sv'}_2^2 &= \sum_{k = 1}^{2^L} (s_k - s'_k)^2 =
\sum_{k = 1}^{2^L - 1} (s_k - s'_k)^2 +
\bigg[\sum_{k = 1}^{2^L-1} (s_k - s'_k) \bigg]^2 \\
&= \sum_{k = 1}^{2^L - 1} 2(s_k - s'_k)^2 +
\sum_{1 \leq k < \ell \leq 2^L-1} (s_k - s'_k)(s_\ell - s'_\ell),
\end{align*}
using the fact that $s_{2^L} = n - \sum_{k=1}^{2^L-1} s_k$.
On one hand, the distance between neighbouring points is
$\norm{\sv - \sv'}_2^2= 2$.
On the other, this is the smallest possible distance between $\sv\neq\sv'$.
Without loss of generality, assume $s_1 \neq s'_1$.
Since $s_1, s'_1 \in \{0,\dots,n\}$, the minimum distance occurs when
$s_k = s'_k = 0$ for $k = 2,\dots,2^L-1$, and $\abs{s_k - s'_k} = 1$, \ie $\sv'$
is a neighbour of $\sv$.

It is convenient to measure distance between points in $\Ssp_n$ in terms of
the number of steps required to reach one from the other.
Define the metric $\delta_S$ on $\Ssp_n$ as
\[ \delta_S(\sv, \sv') := \frac{\norm{\sv - \sv'}_1}{2} =
\frac{1}{2} \bigg\{ \sum_{k = 1}^{2^L-1} \abs{s_k - s'_k} +
\bigg\lvert \sum_{k = 1}^{2^L-1} (s_k - s'_k) \bigg\rvert \bigg\}
, \]
where $\norm{\cdot}_1$ denotes the $L_1$ metric.
Then:
\begin{itemize}
\item $\delta_s(\sv,\sv')$ is equal the minimum number of steps required to
reach $\sv'$ from $\sv$.

\item The neighbours of $\sv$ are those points $\sv' \in \Ssp_n$ such that
$\delta_S(\sv, \sv') = 1$.
\end{itemize}

\subsection{The probability ratio}

We observed previously that the privacy ratio can be expressed in terms of the
ratio of probabilities of obtaining neighbouring synthetic collections starting
from a common original collection.
We formalize this notion as follows.

\begin{defn}
Given $i\neq j$, the \textbf{probability ratio} from $i$ to $j$ is given by
\[ \rho_n(\sv, i, j; \mv) :=
\frac{\P[A_n(\mv) = \sv]}{\P[A_n(\mv) = \sv + \ev_{ij}]} =
\frac{P_n(\sv, \mv)}{P_n(\sv + \ev_{ij},\mv)}
\]
for $\mv,\sv\in\Ssp_n$ such that $s_i \geq 1$.
\end{defn}

The probability ratio describes how the likelihood of obtaining synthetic
collections changes as we step between neighbouring synthetic outcomes in
$\Ssp_n$.
We now show that, by conditioning on the randomization outcome of one of the
vectors in the original collection, the probability ratio for a collection of
size $n$ can be expressed in terms of probability ratios for a collection of
size $n-1$ over various synthetic collections.

Denote $\sv_{-i} := \sv - \ev_i$, and fix $1 \leq r \leq 2^L$ such that
$m_r \geq 1$.
Consider $i$ such that $s_i \geq 2$.
Conditioning on the outcome of randomizing one of the original $d_r$ vectors,
we have
\begin{equation*}
\rho_n(\sv, i, j; \mv) = \frac{
\displaystyle\sum_{k = 1}^{2^L} p_{rk} P_{n-1}(\sv_{-k}, \mv_{-r})}{
\displaystyle\sum_{k = 1}^{2^L} p_{rk} P_{n-1}(\sv_{-k} + \ev_{ij}, \mv_{-r})}.
\end{equation*}
If $s_k = 0$ for some $k$, we interpret $P_{n-1}(\sv_{-k}, \mv_{-r}) = 0$, and
we consider the corresponding term omitted from the sum.
Abbreviating $Q(\cdot) := P_{n-1}(\,\cdot\,, \mv_{-r})$ for clarity, and
dividing through by $Q(\sv_{-i} + \ev_{ij})$ (non-zero since $s_i \geq 2$),
yields
\begin{equation} \label{eq:gprobratiodecomp}
\rho_n(\sv, i, j; \mv) = \frac{
p_{ri}\displaystyle \frac{Q(\sv_{-i})}{Q(\sv_{-i} + \ev_{ij})} +
\displaystyle\sum_{k\neq i} p_{rk} \frac{Q(\sv_{-k})}{Q(\sv_{-i} + \ev_{ij})} }{
p_{ri} + p_{rj} \displaystyle\frac{Q(\sv_{-i})}{Q(\sv_{-i} + \ev_{ij})} +
\displaystyle\sum_{k\neq i, j} p_{rk}
\frac{Q(\sv_{-k} + \ev_{ij})}{Q(\sv_{-i} + \ev_{ij})} }.
\end{equation}
Furthermore,
\[ \frac{Q(\sv_{-i})}{Q(\sv_{-i} + \ev_{ij})} =
%\frac{P_{n-1}(\sv_{-1}, \mv_{-1})}{P_{n-1}(\sv_{-1} + \ev_{ij}, \mv_{-1})}
\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r}), \]
%and for $k = 2,\dots,2^L$,
\begin{equation*}
\frac{Q(\sv_{-k})}{Q(\sv_{-i} + \ev_{ij})} =
%\frac{P_{n-1}(\sv_{-k}, \mv_{-1})}{P_{n-1}(\sv_{-1} + \ev_{ij}, \mv_{-1})}
\frac{Q(\sv_{-k})}{Q(\sv_{-k} + \ev_{ik})}
%\frac{P_{n-1}(\sv_{-k}, \mv_{-1})}{P_{n-1}(\sv_{-k} + \ev_{1k}, \mv_{-1})}
\frac{Q(\sv_{-i})}{Q(\sv_{-i} + \ev_{ij})}
%\frac{P_{n-1}(\sv_{-1}, \mv_{-1})}{P_{n-1}(\sv_{-1} + \ev_{ij}, \mv_{-1})} \\
= \rho_{n-1}(\sv_{-k}, i, k; \mv_{-r})
\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r}),
\end{equation*}
and
\begin{equation*}
\frac{Q(\sv_{-k} + \ev_{ij})}{Q(\sv_{-i} + \ev_{ij})} =
\frac{Q(\sv_{-k})}{Q(\sv_{-i} + \ev_{ij})}
\frac{Q(\sv_{-k}  + \ev_{ij})}{Q(\sv_{-k})} =
\frac{\rho_{n-1}(\sv_{-k}, i, k; \mv_{-r})
\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r})}{\rho_{n-1}(\sv_{-k}, i, j; \mv_{-r})}.
%\frac{Q(\sv_{-k} + \ev_{ij})}{Q(\sv_{-k} + \ev_{ij} + \ev_{ik})} =
%&= \frac{P_{n-1}(\sv_{-k} + \ev_{ij}, \mv_{-1})}
%{P_{n-1}(\sv_{-k} + \ev_{1k} + \ev_{ij}, \mv_{-1})}
%\rho_{n-1}(\sv_{-k} + \ev_{ij}, i, k; \mv_{-r}).
\end{equation*}
Applying these identities in \eqref{eq:gprobratiodecomp} gives
\begin{equation} \label{eq:gprobratiodecomprho}
\rho_n(\sv, i, j; \mv) = \frac{
p_{ri}\cdot\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r}) +
\displaystyle\sum_{k \neq i} p_{rk}\cdot
\rho_{n-1}(\sv_{-k}, i, k; \mv_{-r})\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r}) }{
p_{ri} + p_{rj}\cdot\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r}) +
\displaystyle\sum_{k \neq i,j} p_{rk}\cdot
\frac{\rho_{n-1}(\sv_{-k}, i, k; \mv_{-r})
\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r})}{\rho_{n-1}(\sv_{-k}, i, j; \mv_{-r})}
%\rho_{n-1}(\sv_{-k} + \ev_{ij}, i, k; \mv_{-r})
},
\end{equation}
or, after dividing through by $\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r})$,
\begin{equation*}
\rho_n(\sv, i, j; \mv) = \frac{
p_{ri} + p_{rj}\cdot\rho_{n-1}(\sv_{-j}, i, j; \mv_{-r}) +
\displaystyle\sum_{k \neq i,j} p_{rk}\cdot\rho_{n-1}(\sv_{-k}, i, k; \mv_{-r})}{
\displaystyle\frac{p_{ri}}{\rho_{n-1}(\sv_{-i}, i, j; \mv_{-r})} + p_{rj} +
\displaystyle\sum_{k \neq i,j} p_{rk}\cdot
\frac{\rho_{n-1}(\sv_{-k}, i, k; \mv_{-r})}{
\rho_{n-1}(\sv_{-k}, i, j; \mv_{-r})} }.
\end{equation*}
Additionally,
\[ \rho_{n-1}(\sv, i, j; \mv_{-r}) = \frac{Q(\sv)}{Q(\sv + \ev_{ij})} =
\frac{Q(\sv + \ev_{ij} + \ev_{ji})}{Q(\sv + \ev_{ij})} =
\inv{\rho_{n-1}(\sv + \ev_{ij}, j, i; \mv_{-r})} \]
and
\[ \frac{\rho_{n-1}(\sv, i, k; \mv_{-r})}{
\rho_{n-1}(\sv, i, j; \mv_{-r})} = 
\frac{Q(\sv)}{Q(\sv + \ev_{ik})}
\frac{Q(\sv + \ev_{ij})}{Q(\sv)} =
\frac{Q(\sv + \ev_{ij})}{Q(\sv + \ev_{ij} + \ev_{jk})} =
\rho_{n-1}(\sv + \ev_{ij}, j, k; \mv_{-r}),
\]
leading to
\begin{equation*}
\rho_n(\sv, i, j; \mv) = \frac{
p_{ri} + p_{rj}\cdot\rho_{n-1}(\sv_{-j}, i, j; \mv_{-r}) +
\displaystyle\sum_{k \neq i,j} p_{rk}\cdot\rho_{n-1}(\sv_{-k}, i, k; \mv_{-r})}{
p_{rj} + p_{ri}\cdot\rho_{n-1}(\sv_{-i} + \ev_{ij}, j, i; \mv_{-r}) +
\displaystyle\sum_{k \neq i,j} p_{rk}\cdot
\rho_{n-1}(\sv_{-k} + \ev_{ij}, j, k; \mv_{-r}) }.
\end{equation*}

\hrulefill

Identities:
\begin{equation}
\rho_n(\sv, i, j)\rho_n(\sv + \ev_{ij}, j, k) =
\frac{P(\sv)}{P(\sv + \ev_{ij})}
\frac{P(\sv + \ev_{ij})}{P(\sv + \ev_{ij} + \ev_{jk})} =
\frac{P(\sv)}{P(\sv + \ev_{ik})} = \rho_n(\sv, i, k)
\end{equation}
\begin{equation}
\rho_n(\sv, i, j) = \frac{1}{\rho_n(\sv + \ev_{ij}, j, i)}
\end{equation}
\begin{equation}
\frac{\rho_n(\sv, i, k)}{\rho_n(\sv, i, j)} = \rho_n(\sv + \ev_{ij}, j, k) =
\rho_n(\sv + \ev_{ij}, j, \ell)\rho_n(\sv + \ev_{ij} + \ev_{j\ell}, \ell, k) =
\frac{\rho_n(\sv + \ev_{i\ell}, \ell, k)}{\rho_n(\sv + \ev_{i\ell}, \ell, j)}
\end{equation}

Conjectures:
\begin{itemize}
\item Probability ratio from $i$ to $j$ is nondecreasing when shifting in a
direction away from $i$, with the greatest increase on shifting from $i$ to $j$:
\begin{equation}
\rho(\sv, i, j) \leq \rho(\sv + \ev_{ik}, i, j) \leq
\rho(\sv + \ev_{ij}, i, j) \qquad\text{for all } k \neq i,j
\end{equation}

\item Probability ratio from $i$ to $j$ is nondecreasing when shifting in a
direction towards $j$, with the greatest increase on shifting from $i$ to $j$:
\begin{equation}
\rho(\sv, i, j) \leq \rho(\sv + \ev_{kj}, i, j) \leq
\rho(\sv + \ev_{ij}, i, j) \qquad\text{for all } k \neq i,j
\end{equation}

\item Increase in probability ratio from $i$ to $j$ varies when shifting in a
direction orthogonal to $(i, j)$
%\begin{equation}
%\rho(\sv, i, j) \leq \rho(\sv + \ev_{k\ell}, i, j)
%\qquad\text{for all } k,\ell \neq i,j,\; k < \ell
%\end{equation}

\end{itemize}
In fact, the maximality of the shift in the direction $(i, j)$ follows from the
pair of the first inequalities in the statements above:
\[ \rho(\sv + \ev_{ik}, i, j) \leq \rho(\sv + \ev_{ik} + \ev_{kj}, i, j) =
\rho(\sv + \ev_{ij}, i, j). \]

\hrulefill

We now use this relationship to establish an important monotonicity property:
the probability ratio from $i$ to $j$ is nondecreasing when stepping in a
direction that decreases the $i$-th count, \ie away from the corner of $\Ssp_n$
at point $n\cdot\ev_i$.

\begin{prop}
Fix a direction $i\in \{1,\dots,2^L\}$.
Given any original collection $\mv\in\Ssp_n$ and synthetic collection
$\sv\in\Ssp_n$ with $s_i \geq 2$,
\begin{equation} \label{eq:gprobratioincr}
\rho_n(\sv, i, j; \mv) \leq \rho_n(\sv + \ev_{i\ell}, i, j; \mv)
%\qquad\text{for } \ell = 1,\dots,2^L \text{ such that } \ell \neq i.
\end{equation}
for all $1 \leq j,\ell \leq 2^L$ such that $j\neq i$ and $\ell\neq i$.
\end{prop}
\begin{pf}
We proceed by induction on $n$.
Suppose first that $n=2$.
Then either $\mv = 2\ev_r$ or $\mv = \ev_r + \ev_{r'}$, and
$\sv = \ev_k + \ev_{k'}$.
In the first case, $\sv$ is Multinomial with probabilities $\textbf{p}_{r}$:
\[ P_2(\sv, 2\ev_r) = 2\inv{(s_k!s_{k'}!)} p_{rk}^{s_k}p_{rk'}^{s_k'}, \]
and so, for $s\in\Ssp_2$ such that $s_i \geq 1$,
\[ \rho_2(\sv, i, j; 2\ev_r) = \frac{s_j + 1}{s_i}\frac{p_{ri}}{p_{rj}}.
\]
Assumimg $\sv = 2\ev_i$, since we require $s_i \geq 2$, we have
\[ \rho_2(2\ev_i, i, j; 2\ev_r) = \frac{1}{2}\frac{p_{ri}}{p_{rj}} \leq
[I(\ell = j) + 1]\frac{p_{ri}}{p_{rj}} =
\rho_2(\ev_i + \ev_\ell, i, j; 2\ev_r),
\]
verifying \eqref{eq:gprobratioincr} in this case.
If $\mv = \ev_r + \ev_{r'}$,
\[ P_2(\sv, \ev_r + \ev_{r'}) = p_{rk}p_{r'k'} + p_{rk'}p_{r'k}I(k \neq k'), \]
and so
\[ \rho_2(2\ev_i, i, j; \ev_r + \ev_{r'}) =
\frac{p_{ri}p_{r'i}}{p_{ri}p_{r'j} + p_{rj}p_{r'i}} \leq
\frac{p_{ri}p_{r'\ell} + p_{r\ell}p_{r'i}}
{p_{rj}p_{r'\ell} + p_{r\ell}p_{r'j}I(\ell \neq j)} =
\rho_2(\ev_i + \ev_\ell, i, j; \ev_r + \ev_{r'})
\]
iff
\begin{align*}
0 &\leq (p_{ri}p_{r'j} + p_{rj}p_{r'i})(p_{ri}p_{r'\ell} + p_{r\ell}p_{r'i}) -
p_{ri}p_{r'i}[p_{rj}p_{r'\ell} + p_{r\ell}p_{r'j}I(\ell \neq j)] \\
&= p_{ri}^2p_{r'j}p_{r'\ell} + p_{rj}p_{r\ell}p_{r'i}^2
%+ p_{ri}p_{r'\ell}p_{rj}p_{r'i}
+ p_{ri}p_{r'j}p_{r\ell}p_{r'i}
%- p_{ri}p_{r'i}p_{rj}p_{r'\ell}
%- p_{ri}p_{r'i}p_{r\ell}p_{r'j}
I(\ell = j),
\end{align*}
a sum of positive terms.
Suppose now that $n>2$, and assume \eqref{eq:gprobratioincr} holds for $n-1$
over all original and synthetic collections in $\Ssp_{n-1}$ with $s_i \geq 2$.
%whenever $\mv,\sv\in\Ssp_{n-1}$ such that $s_i \geq 2$.
Consider $\mv,\sv\in\Ssp_n$ such that $s_i\geq 2$ and $j,\ell\neq i$.
Choose $r$ such that $m_r \geq 1$, and apply the decompostion
\eqref{eq:gprobratiodecomprho} to $\rho_n(\sv, i, j; \mv)$, writing it in terms
of
\[ \Big(\rho_{n-1}(\,\cdot\,,i, k; \mv_{-r});\; 1 \leq k \leq 2^l,
k\neq i\big). \]
By assumption,
\[ \rho_{n-1}(\sv',i, k; \mv_{-r}) \leq
\rho_{n-1}(\sv' + \ev_{i\ell} ,i, k; \mv_{-r}) \]
for each $k$.
TODO: what about when $s_i = 2$ and we get $\sv_{-i}$?
\end{pf}


\hrulefill

\subsection{Further stuff}

TODO: Is this true?

\begin{prop}
Given an original collection consisting of $(x, \xv)$, the privacy ratio is
maximized when the modification $x'$ is chosen to be the opposite of $x$:
\[ \pi(\sv; \xv, x, x^*) \geq \pi(\sv; \xv, x, x') \]
for any $\sv\in\Ssp_n$, $x'\in \Dsp$, where $\delta(x, x^*) = L$.
\end{prop}


TODO: consequences of this: \\
- difference between numerator and denominator is basically a reweighting of the same terms in the sum \\
- max value of ratio -  $~ (p/q)^L$ but doesn't depend on population size $n$

\subsection{Local differential privacy}

As noted above, differential privacy requires the privacy ratio to be uniformly
bounded over all original and synthetic collections.
This ensures that, regardless of the original collection, the randomization
procedure used to generate the synthetic collection carries the same privacy
guarantee.

However, applying the differential privacy criterion to our scenario of
independently randomized bit vectors has a number of shortcomings.
For one thing, the maximum value of the privacy ratio becomes infeasibly large
when reporting large vectors, since it grows exponentially in $L$, the
dimensionality of the reports.
In order to bound this by what is commonly accepted as a reasonable value
(TODO), the lie probability $q$ needs to be set infeasibly high, reducing the
utility of the collected data.
Also, the maximum value does not depend on the population size $n$, meaning
that no benefit is gained from having a larger population.
Another issue is that the maximum value occurs for a synthetic collection
which becomes increasing unlikely as the population size grows. Hence, the high
cost in terms of utility that we are incurring is mainly spent on protection
for very unlikely outcomes.

We propose to remedy this by relaxing the differential privacy criterion 
\eqref{eq:dpcnt} to hold for all but the most unlikely synthetic outcomes
in $\Ssp$.
We say that $A$ is $\epsilon$-\textbf{locally differentially private} if


\section{The case $L = 1$}

It is instructive to first consider the case where each record in the collection consists of a single bit, as the expressions simplify considerably.

When $L=1$, each original and synthetic record is either 1 or 0, and the transformation $R$ flips each record with probability $q$.
Partition the collection space $\Dsp^n$ according to the number of records that are 1:
\[ \Dsp^n = \bigcup_{m = 0}^n \Dsp_m^n
\quad\quad\text{where}\quad\quad
\Dsp_m^n := \bigg\{ \xv\in\Dsp^n \,:\, \sum_{i=1}^n I(x_i = 1) = m \bigg\}.
\]
For $\xv\in\Dsp_m^n$, we have
\[ A(\xv) = \Phi\circ R(\xv) = \big(A_n(m), n - A_n(m)\big), \]
where
\begin{align*}
A_n(m) &:= \sum_{i=1}^n I(R(x_i) = 1)
= \sum_{i:\, x_i = 1} I(R(1) = 1) + \sum_{i:\, x_i = 0} I(R(0) = 1) \\
&\sim Bin(m, p) + Bin(n-m, q),
\end{align*}
a sum of two independent Binomial random variables with support $\{0,\dots,n\}$.
Furthermore, if $\xv\in\Dsp_m^n$ and $\xv,\xv'$ differ in one row, then $\xv'\in\Dsp_{m-1}^n \cup \Dsp_{m+1}^n$.
Defining
\[ \pi_n(s; m) := \frac{\P[A_n(m) = s]}{\P[A_n(m+1) = s]}
\quad\quad\text{for}\ \ 
s\in\{0,\dots,n\}\ \text{and}\ m\in\{0,\dots,n-1\},
\]
the privacy ratio becomes
\[ \pi\big((s, n-s);\,\xv,\xv'\big) =
\begin{cases}
\pi_n(s;m - 1) & x_1 = 1 \\[0.3em]
\inv{\pi_n(s;m)} & x_1 = 0
\end{cases}\, .
\]
Hence, in the $L=1$ case, it suffices to study the behaviour of $\pi_n(s;m)$.


\subsection{Recursive relationship over $n$ and $m$}

The conditioning argument \eqref{eq:prcond} yields a recursive relationship that lets us express the distribution of $A_n$ in terms of that of $A_{n-1}$.

Recall that $A_n(m)$ is the outcome of applying the bit transformation $R$ to $n$ original bits, $m$ of which are 1 and $n-m$ are 0.
For $m \geq 1$, we can condition on the outcome of one of the original 1s:
\[ A_n(m) \sim Ber(p) + Bin(m-1, p) + Bin(n-m, q) \sim Ber(p) + A_{n-1}(m-1), \]
and so
\begin{equation}\label{eq:rec1}
\P[A_n(m) = s] = p\P[A_{n-1}(m-1) = s-1] + q\P[A_{n-1}(m-1) = s].
\end{equation}
If $s = 0$, the first term on the RHS is interpreted as 0, and if $s = n$, the last term is.
Similarly, for $m \leq n-1$, conditioning on an original 0,
\[ A_n(m) \sim Ber(q) + Bin(m, p) + Bin(n-m-1, q) \sim Ber(q) + A_{n-1}(m), \]
from which
\begin{equation}\label{eq:rec0}
\P[A_n(m) = s] = q\P[A_{n-1}(m) = s-1] + p\P[A_{n-1}(m) = s].
\end{equation}

The recursive formulas \eqref{eq:rec1} and \eqref{eq:rec0} give some insight into how the distribution of $A_n(m)$ changes as $n$ and $m$ vary:
\begin{itemize}
\item  as $n$ increases by 1, the probabilities shift slightly, with $\P[A_n(m) = 0] \leq \P[A_{n-1}(m) = 0]$ and
$\P[A_n(m) = s]$ falling between $\P[A_{n-1}(m) = s-1]$ and $\P[A_{n-1}(m) = s]$ for each $s\geq 1$ (\ie the hump of the pmf shifts to the right);
\item the distribution of $A_n(m+1)$ is not so different to that of $A_n(m)$, since $\P[A_n(m) = s]$ and $\P[A_n(m+1) = s]$ both lie between consecutive pmf values of $A_{n-1}(m)$. In particular, this allows us to express the privacy ratio $\pi(s;m)$ in terms of $A_{n-1}(m)$.
\end{itemize}

Writing $P_{n,m}(s) := \P[A_n(m) = s]$, the formulas \eqref{eq:rec1} and \eqref{eq:rec0} can be expressed as
\[ P_{n,m}(s) = pP_{n-1,m-1}(s-1) + qP_{n-1,m-1}(s)
%\quad\text{for}\ \ s = 0,\dots,n,\ \ m = 1,\dots,n
\quad\text{for}\ \ 0 \leq s \leq n,\ \ 1\leq m \leq n 
\]
and
\[ P_{n,m}(s) = qP_{n-1,m}(s-1) + pP_{n-1,m}(s)
%\quad\text{for}\ \ s = 0,\dots,n,\ \ m = 0,\dots,n-1. 
\quad\text{for}\ \ 0 \leq s \leq n,\ \ 0\leq m \leq n-1.
\]

\subsection{The probability ratio}

The probabilities in the privacy ratio represent the likelihood of observing
the same synthetic collection outcome given two different original collections.
In the expression $\pi_n(s;m) = P_{n,m}(s) / P_{n,m+1}(s)$, the probabilities
correspond to the distributions of $A_n(m)$ and $A_n(m+1)$, respectively.
However, using the decomposition \eqref{eq:rec1} and \eqref{eq:rec0}, we can
rewrite $\pi_n$ in terms of probabilites from the same distribution, which is
more convenient to work with.

Applying \eqref{eq:rec0} to the numerator and \eqref{eq:rec1} to the denominator,
we obtain
\[ \pi_n(s;m) =
\frac{qP_{n-1,m}(s-1) + pP_{n-1,m}(s)}{pP_{n-1,m}(s-1) + qP_{n-1,m}(s)} =
\frac{q + p \frac{P_{n-1,m}(s)}{P_{n-1,m}(s-1)}}
    {p + q \frac{P_{n-1,m}(s)}{P_{n-1,m}(s-1)}}
\]
for $s \geq 1$, and $\pi_n(0;m) \equiv p/q$.
Define the \textbf{probability ratio}
\[ \rho_n(s;m) := \frac{P_{n,m}(s)}{P_{n,m}(s-1)}
\qquad\text{for}\ \ 1 \leq s\leq n \]
a ratio of consecutive probabilities from the distribution of $A_n(m)$, and let
$g(x) = \frac{q + px}{p + qx}$, so that $\pi_n = g \circ \rho_{n-1}$.
The function $g$ is increasing over $x>0$, since
\[ g'(x) = \frac{p-q}{(p+qx)^2} > 0. \]
Therefore, properties of monotonicity and extrema established for $\rho_n$ (for
all $n$) carry over to $\pi_n$ as well.

The probability ratio can be expressed in a concise way using the following 
recursive property of the distribution of $A_n(m)$.

\begin{lem}
For $n \geq 1$,
\begin{equation} \label{eq:recP}
(s+1) P_{n,m}(s+1) = \bigg\{ (m-s)\frac{p}{q} + (n-m-s)\frac{q}{p} \bigg\} P_{n,m}(s) + (n-s+1) P_{n,m}(s-1)
\end{equation}
for $0 \leq m \leq n$ and $0 \leq s \leq n-1$ (with $P_{n,m}(-1) := 0$).
\end{lem}
\begin{pf}
We proceed by induction on $n$.
Suppose first $n=1$, $s = 0$.
If $m=1$, then $A_1(1) \sim Ber(p)$, and \eqref{eq:recP} holds since 
$(mp/q + (1-m)q/p) \cdot P_{1,1}(0) = p = P_{1,1}(1)$.
The argument is similar when $m=0$.
Next assume \eqref{eq:recP} holds for $A_{n-1}(m)$, and suppose $m \leq n-1$ and $1 \leq s \leq n-2$.
Observe that
\begin{align*}
\bigg\{ &(m-s)\frac{p}{q} + (n-m-s)\frac{q}{p} \bigg\} P_{n,m}(s) + (n-s+1) P_{n,m}(s-1) \\
&= \bigg\{ (m-s)\frac{p}{q} + (n-1-m-s)\frac{q}{p} \bigg\} \big[qP_{n-1,m}(s-1) + pP_{n-1,m}(s)\big] \\
 &\qquad\qquad + (n-1-s+1) \big[qP_{n-1,m}(s-2) + pP_{n-1,m}(s-1)\big] +
 \frac{q}{p}P_{n,m}(s) + P_{n,m}(s-1) \\
 &= p\bigg[ \bigg\{ (m-s)\frac{p}{q} + (n-1-m-s)\frac{q}{p} \bigg\} P_{n-1,m}(s) + (n-1-s+1)P_{n-1,m}(s-1) \bigg] \\
 &\qquad + q\bigg[ \bigg\{ (m-(s-1))\frac{p}{q} + (n-1-m-(s-1))\frac{q}{p} \bigg\} P_{n-1,m}(s-1) \\
 &\hspace{20em} + (n-1-(s-1)+1)P_{n-1,m}(s-2) \bigg] \\
 &\qquad - \bigg(p + \frac{q^2}{p}\bigg) P_{n-1,m}(s-1) - qP_{n-1,m}(s-2) + \frac{q^2}{p}P_{n-1,m}(s-1) + qP_{n-1,m}(s) \\
 &\qquad + qP_{n-1,m}(s-2) + pP_{n-1,m}(s-1) \\
 &= p(s+1)P_{n-1,m}(s+1) + qsP_{n-1,m}(s) + qP_{n-1,m}(s) \\
 & = (s+1) \big[ qP_{n-1,m}(s) + pP_{n-1,m}(s+1) \big] = (s+1)P_{n,m}(s+1),
\end{align*}
applying the induction hypothesis for $s$ and for $s-1$ together with \eqref{eq:rec0}. If $s=0$, the argument is similar:
\begin{align*}
\bigg\{ m\frac{p}{q} + (n-m)\frac{q}{p} \bigg\} P_{n,m}(0)
&= p\bigg\{ m\frac{p}{q} + (n-1-m)\frac{q}{p} \bigg\} P_{n-1,m}(0) + qP_{n-1,m}(0) \\
 &= pP_{n-1,m}(1) + qP_{n-1,m}(0) = P_{n,m}(1).
\end{align*}
\end{pf}

Given $m$, the probability ratio can be expressed using \eqref{eq:recP}:
\begin{align*}
\rho(s+1;m) &= \frac{m-s}{s+1}\frac{p}{q} + \frac{n-m-s}{s+1}\frac{q}{p} + \frac{n-s+1}{s+1} \frac{1}{\rho(s;m)} \\
\rho(1;m) &= m\frac{p}{q} + (n-m)\frac{q}{p}
\end{align*}
Write
\[ \eta(s) := \frac{n-s+1}{s+1} \qquad\text{and}\qquad
\gamma_m(s) := \frac{1}{s+1} \left[(m-s)\frac{p}{q} + (n-m-s)\frac{q}{p}\right], \]
to get
\begin{equation}\label{eq:probrrec}
\rho(s+1;m) = \eta(s)\inv{\rho(s;m)} + \gamma_m(s)\,; \quad
\rho(1;m) = \gamma_m(0). 
\end{equation}

Note also that $\gamma_m(s)$ can be expressed in terms of
$\EP A_n(m) = \mu_m = nq + m(p-q)$:
\[ (s+1)\gamma_m(s) = \frac{\mu_m - s}{pq} - n + 2s. \]
%\begin{align*}
% &= \frac{(m-s)p^2 + (n-m-s)q^2}{pq}
% = \frac{m(p^2-q^2) + nq^2 - s(p^2 + q^2)}{pq} \\
% &= \frac{nq + m(p-q) - nq + nq^2 - s(1-2pq)}{pq}
% = \frac{\mu_m - s -(n - 2s)pq}{pq} \\
% &= 
%\end{align*} 

The probability ratio has the following properties (TODO):
\begin{itemize}
\item decreasing in $s$ for fixed $m$
\item increasing in $m$ for fixed $s$.
\end{itemize}

\subsection{Bounding the probability ratio}

For $A$ to satisfy local differential privacy, the privacy ratio $\pi_n(s;m)$
must be bounded for all $s$ except for a set of small probability with respect
to the distribution $\P[A_n(m) = \cdot\,]$.
Furthermore, this bound must hold regardless of the original collection
described through $m$.

Fix $\delta > 0$.
Given $m$, we show that the probability ratio for $s\in[\mu_m-\delta, n]$ 
is bounded by a value $\rho(s^*; 0)$, where $s^*$ is expressed in terms of
$\mu_0-\delta$.
Together with the fact that
$P_{n,m}(\mu_m - \delta) \leq P_{n,0}(\mu_0 - \delta)$ (TODO - is this
necessary?),
this implies that the bound for local differential privacy, required to hold
for all $m$, can be computed in terms of $A_n(0)$ alone.
Note that, since $\rho$ is decreasing in $s$ for fixed $m$, it is sufficient
to consider the probability ratio at the smallest integer value belonging to
the interval $[\mu_m-\delta, n]$.

TODO: how to handle the left endpoint. What is the min value of $\delta$?

For $\delta > 0$ let $s_m(\delta) := \lceil \mu_m - \delta \rceil \vee 0$, and define $R_m(\delta) := \rho(s_m(\delta); m)$.
Note that $R_m(\delta) \leq R_m(\delta')$ for $\delta \leq \delta'$, and
$s_m(\delta + 1) = (s_m(\delta) - 1) \vee 0$.

\begin{prop}
\[ R_m(\delta) \leq R_0(\delta + 2)\qquad\text{for}\ \ m = 0,\dots,n \]
provided $\delta > \sigma_0 + 1$, where
$\sigma_0^2 = \Var A_n(0) = npq$.
\end{prop}

\begin{pf}
Fix $\delta > \sigma_0 + 1$.
(TODO)
If $s_0(\delta) < 2$

Assume $s_0(\delta) \geq 2$, and suppose $R_m(\delta) > R_0(\delta + 2)$ for
some $m$.
Then, we have
\[ R_0(\delta) \leq R_0(\delta + 1) \leq R_0(\delta + 2) < R_m(\delta) \leq
R_m(\delta + 1),\]
implying that
\[ \inv{R_0(\delta + 2)} =
\frac{R_0(\delta+1) - \gamma_0(s_0(\delta+2))}{\eta(s_0(\delta+2))} > 
\frac{R_m(\delta) - \gamma_m(s_m(\delta+1))}{\eta(s_m(\delta+1))} =
\inv{R_m(\delta + 1)} \]
via \eqref{eq:probrrec}.
Write $s_m := s_m(\delta+1)$, $s_0 := s_0(\delta + 2)$.
Since $R_m(\delta) > R_0(\delta+1)$ by assumption, we obtain:
\begin{equation}\label{eq:deltabdineq}
\big\{\eta(s_m) - \eta(s_0)\big\} R_0(\delta + 1)
 + \big\{\eta(s_0)\gamma_m(s_m) - \eta(s_m)\gamma_0(s_0)\big\} > 0.
\end{equation}
Furthermore,
\[ \eta(s_m) - \eta(s_0)
= \frac{n - s_m + 1}{s_m + 1} - \frac{n - s_0 + 1}{s_0 + 1}
= -\frac{(n + 2)(s_m - s_0)}{(s_0 + 1)(s_m + 1)}, \]
and
\begin{align*}
\eta(s_0)&\gamma_m(s_m) - \eta(s_m)\gamma_0(s_0) \\
&= \frac{n - s_0 + 1}{s_0 + 1}\cdot \frac{(\mu_m - s_m)/pq - n + 2s_m}{s_m + 1} - 
\frac{n - s_m + 1}{s_m + 1}\cdot \frac{(\mu_0-s_0)/ pq - n + 2s_0}{s_0 + 1} \\
 &= \frac{(n+2)(s_m-s_0) + (\mu_0 s_m - \mu_m s_0)/pq +
 (n+1)[\mu_m - \mu_0 - (s_m - s_0)]/pq}{(s_0 + 1)(s_m + 1)},
\end{align*}
so \eqref{eq:deltabdineq} implies
\begin{align}
-(n+2)(s_m - s_0)&(R_0(\delta + 1) - 1) + \nonumber\\
&\frac{\mu_0 (s_m - s_0) - m(p-q) s_0}{pq} +
\frac{(n+1)[m(p-q) - (s_m - s_0)]}{pq} > 0. \label{eq:deltabdineq2}
\end{align}
Now, let
$\delta_0 :=
\delta - \{\lceil \mu_0 - \delta \rceil - (\mu_0 - \delta)\} =
\mu_0 - \lceil \mu_0 - \delta \rceil$, \ie
$\delta_0 = \inf\{ \lambda : s_0(\lambda) = s_0(\delta)\}$.
Then $s_0(\delta) = s_0(\delta_0) = \mu_0 - \delta_0$, an integer, and
$s_m(\delta_0) - s_m(\delta) \in\{0, 1\}$, since
$0 \leq \delta-\delta_0 < 1$.
Consequently,
since
\[ s_m(\delta_0) - s_0(\delta_0) = \lceil \mu_0  + m(p-q) - \delta_0 \rceil -
(\mu_0 - \delta_0) = \lceil m(p-q) \rceil, \]
\begin{align*}
s_m - s_0 &= (s_m(\delta) - 1) - (s_0(\delta) - 2) =
s_m(\delta) - s_m(\delta_0) + \lceil m(p-q) \rceil + 1 \\
 &\in \big\{\lceil m(p-q) \rceil, \lceil m(p-q) \rceil + 1 \big\},
\end{align*}
and
\begin{align*}
\mu_0(s_m - s_0) - m(p-q) s_0 & = \mu_0(s_m-s_0) - m(p-q)(s_0(\delta_0) - 2) \\
 &= \mu_0[(s_m-s_0) - m(p-q)] + m(p-q)(\delta_0 + 2).
\end{align*}
Applying these identities in \eqref{eq:deltabdineq2} gives
\[ 
-(n+2)(s_m - s_0)(R_0(\delta + 1) - 1) +
\frac{m(p-q)(\delta_0 + 2)}{pq}
+ \frac{n+1-\mu_0}{pq}\big(m(p-q) - (s_m-s_0)\big) > 0.
\]
Since $s_m-s_0 \geq m(p-q)$,
\begin{equation}\label{eq:deltabdineq3}
(n+2)(R_0(\delta+1) - 1) <
\frac{\delta_0 + 2}{pq}\frac{m(p-q)}{s_m - s_0} < \frac{\delta_0 + 2}{pq}.
\end{equation}
Next, recall that
$R_0(\delta+1) = P_{n,0}(s_0(\delta+1)) / P_{n,0}(s_0(\delta+1) - 1)$.
Since $P_{n,0}(\cdot) = \P[Bin(n,q) = \cdot\,]$, 
\[ R_0(\delta+1) - 1 =
\frac{n - s_0(\delta+1) + 1}{s_0(\delta+1)}\cdot \frac{q}{p} - 1 =
\frac{\mu_0 - (\mu_0 - \delta_0 - 1) + q}{p(\mu_0 - \delta_0 - 1)} =
\frac{\delta_0 + q + 1}{p(\mu_0 - \delta_0 - 1)}.
\]
Hence, substituting this expression in \eqref{eq:deltabdineq3} yields
\begin{align*}
(\delta_0 + 2)(\mu_0 - \delta_0 - 1) &> (n+2)q(\delta_0 + q + 1) >
\mu_0(\delta_0 + q + 1) \\
\iff
-\delta_0^2 - 3\delta_0 + 2\mu_0 - 2 &>  (1+q)\mu_0 \\
\iff
-\delta_0^2 - 3\delta_0 + npq &> 0,
\end{align*}
which requires that $\delta_0$ lie between the roots of the quadratic equation.
In particular,
\[ \delta_0 \leq -\frac{3}{2} + \frac{1}{2}\sqrt{9 + 4npq} \leq
-\frac{3}{2} + \frac{3}{2} + \sqrt{npq} = \sqrt{npq}. \]
Finally, since $0 \leq \delta-\delta_0 < 1$, we conclude that
\[ \delta = \delta_0 + \delta - \delta_0 < \sigma_0 + 1, \]
contradicting our initial choice of $\delta$.
\end{pf}



----------------------------


\subsection{Recursive relationship for CDFs}

Summing both sides of \eqref{eq:rec1} and \eqref{eq:rec0} shows that the recursive relationship extends to cdfs as well.
Writing $F_{n,m}(x) := \P[A_n(m) \leq x]$, we have
\begin{equation*}%\label{eq:Frec1}
F_{n,m}(x) = p\cdot F_{n-1,m-1}(x-1) + q\cdot F_{n-1,m-1}(x)
\qquad\qquad m = 1,\dots,n
\end{equation*}
and 
\begin{equation*}%\label{eq:Frec0}
F_{n,m}(x) = q\cdot F_{n-1,m}(x-1) + p\cdot F_{n-1,m}(x)
\qquad\qquad m = 0,\dots,n-1.
\end{equation*}

Furthermore, we can express the difference on incrementing $m$ as follows:
\begin{align*}
 F_{n,m}(x) &- F_{n,m-1}(x) \\
&= p F_{n-1,m-1}(x-1) + q F_{n-1,m-1}(x) - q F_{n-1,m-1}(x-1)
 - p F_{n-1,m-1}(x) \\
&= (p-q) \big[ F_{n-1,m-1}(x-1) - F_{n-1,m-1}(x) \big] \\
&= -(p-q) P_{n-1, m-1}(\lfloor x \rfloor)
\end{align*}
and
\begin{align*}
 F_{n,m}(x+1) &- F_{n,m-1}(x) \\
&= p F_{n-1,m-1}(x) + q F_{n-1,m-1}(x+1) - q F_{n-1,m-1}(x-1)
 - p F_{n-1,m-1}(x) \\
&= q\big[ F_{n-1,m-1}(x+1) - F_{n-1,m-1}(x-1) \big] \\
&= q\big\{ P_{n-1, m-1}(\lfloor x \rfloor) +
P_{n-1, m-1}(\lfloor x+1 \rfloor) \big\}
\end{align*}

Therefore, $F_{n,m}(x) \leq F_{n,m-1}(x) \leq F_{n,m}(x+1)$ and $F_{n,m-1}(x-1) \leq F_{n,m}(x) \leq F_{n,m-1}(x)$.

For probabilities, the relationship becomes
\[
P_{n, m}(s) - P_{n, m-1}(s) = -(p-q) \big\{ P_{n-1, m-1}(s) - P_{n-1, m-1}(s-1) \big\}
\]
and
\[
P_{n, m}(s+1) - P_{n, m-1}(s) = q \big\{ P_{n-1, m-1}(s+1) - P_{n-1, m-1}(s-1) \big\}.
\]


\subsection{Privacy Ratio}

Using the recursive relationships \eqref{eq:rec1} and \eqref{eq:rec0}, we have
\[ \pi(s; m) = \frac{\P[A_n(m) = s]}{\P[A_n(m+1) = s]}
 = \frac{q P_{n-1, m}(s-1) + p P_{n-1, m}(s)}
 {p P_{n-1, m}(s-1) + q P_{n-1, m}(s)}.
\]
Lemma \ref{lem:rscomp2} implies that monotonicity and extrema of  the privacy ratio $\pi$ are determined by those of the \emph{probability ratio} $P_{n-1, m}(s)/P_{n-1,m}(s-1)$:
\[ \text{if}\quad \frac{P_{n-1,m}(s)}{P_{n-1,m}(s-1)} \geq \frac{P_{n-1,m'}(s')}{P_{n-1,m'}(s'-1)},
\quad\text{then}\quad \pi_n(s;m) \geq \pi_n(s';m'). \]
Hence, studying the behaviour of the privacy ratio reduces to studying 
ratios of consecutive pmf values.
Using this property, we can derive the following facts:
\begin{itemize}
\item $\pi$ is decreasing in $s$: $\pi(s;m) \geq \pi(s+1; m)$
\item $\pi$ is increasing in $m$: $\pi(s;m) \leq \pi(s; m+1)$
\item $\pi$ is net decreasing when both $s$ and $m$ increase:
$\pi(s;m) \geq \pi(s+1; m+1)$.
\end{itemize}

A different approach, using the difference identities above, gives
\begin{align*}
\pi(s; m) &= \frac{P_{n,m}(s)}{P_{n,m+1}(s)}
 = \frac{P_{n,m-1}(s) + (p-q)\{P_{n-1,m-1}(s-1) - P_{n-1,m-1}(s)\}}
 {P_{n,m}(s) + (p-q)\{P_{n-1,m}(s-1) - P_{n-1,m}(s)\}} \\
 &= \frac{\frac{P_{n,m-1}(s)}{P_{n,m}(s)} + (p-q)\frac{P_{n-1,m-1}(s-1) - P_{n-1,m-1}(s)}{P_{n,m}(s)}}
 {1 + (p-q)\frac{P_{n-1,m}(s-1) - P_{n-1,m}(s)}{P_{n,m}(s)}}.
\end{align*}
Therefore,
\begin{align*}
\frac{\pi(s; m)}{\pi(s; m-1)} &= \frac{P_{n,m}(s)}{P_{n,m-1}(s)} \pi(s;m) \\
 &= \frac{1 + (p-q)\frac{P_{n-1,m-1}(s-1) - P_{n-1,m-1}(s)}{P_{n,m-1}(s)}}
 {1 + (p-q)\frac{P_{n-1,m}(s-1) - P_{n-1,m}(s)}{P_{n,m}(s)}}
\end{align*}


\subsection{Quantiles}

Denote by $\tau_n(m)$ the $\alpha$-th quantile of $F_{n,m}$:
\[ \tau_n(m) = \inf\{y : F_{n,m}(y) \geq \alpha \}. \]
Note that $\tau_n(m)$ is an integer satisfying $F_{n,m}(\tau_n(m)) \geq \alpha$ and $F_{n,m}(\tau_n(m) - 1) < \alpha$.

\begin{claim}
For $m = 1,\dots,n$,
\[ \tau_n(m) \in \{ \tau_n(m-1), \tau_n(m-1) + 1 \}. \]
\end{claim}
\begin{pf}
Since $\alpha \leq F_{n,m}(\tau_n(m)) \leq F_{n,m-1}(\tau_n(m))$, we have $\tau_n(m) \geq \tau_n(m-1)$.
On the other hand, $F_{n,m}(\tau_n(m-1)+1) \geq F_{n,m-1}(\tau_n(m-1)) \geq \alpha$, from which $\tau_n(m) \leq \tau_n(m-1) + 1$.
\end{pf}

Furthermore, $\tau_n(m) = \tau_n(m-1)$ if and only if
$F_{n,m}(\tau_n(m-1)) \geq \alpha$, \ie
\[ F_{n,m-1}(\tau_n(m-1)) - \alpha \geq (p-q)\P[A_{n-1}(m-1) = \tau_n(m-1)]. \]



\subsection{Asymptotic approach}

    



\subsection{Approximate location relationship over $m$}

The family $\{A_n(m),\, m = 0,\dots,n\}$ are in fact approximately location-shifted versions of each other.
Indeed, note that
\[ \EP A_n(m) = mp + (n-m)q = nq + m(p-q) \]
and
\[ \Var A_n(m) = mpq + (n-m)qp = npq. \]
In other words, as $m$ varies between $0$ and $n$, the mean varies linearly in $m$ and the variance remains constant.
The central portion of the distribution remains approximately the same shape, although transitioning from right-skewed when $m=0$ to left-skewed when $m=n$.
Thus, 
\[ F_{n,m}(x) \approx F_{n,0}(x - m(p-q)). \]

We quantify this approximation as follows.


\section{Maximal collections}


The first question we address is for which pair of original and modified collections $\xv$ and $\xv'$ does $\pi$ obtain its maximum.

\subsection{The case $L = 1$}

It is instructive to first consider the case where each record in the collection consists of a single bit, as the expressions simplify considerably.
In this case, the outcome of $A$ essentially reduces to the number of 1s obtained in the synthetic collection $R(\xv)$, since $\sv\in\Ssp_n$ can be written as $(s_1,s_2)$ with $s_1\in\{0,\dots,n\}$ and $s_2 = n-s_1$. 
Using this fact, we interpret $A(\xv)$ as $\sum_{i=1}^n I(R(x_i) = 1)$,
and express the privacy ratio as
\[ \pi(s,\xv,x_1') = \frac{\P[R(x'_1) = 1] \P[A(\xvt) = s-1] +
\P[R(x'_1) = 0] \P[A(\xvt) = s]}
{\P[R(x_1) = 1] \P[A(\xvt) = s-1] + \P[R(x_1) = 0] \P[A(\xvt) = s]}
\]
for $s \in \{0,\dots,n\}$. Furthermore, the requirement that $x_1 \neq x'_1$ in the single-bit case implies that $x'_1 = 1-x_1$, so
\[ \pi(s,\xv,x_1') = \frac{(1-\P[R(x_1) = 1]) \P[A(\xvt) = s-1] +
(1-\P[R(x_1) = 0]) \P[A(\xvt) = s]}
{\P[R(x_1) = 1] \P[A(\xvt) = s-1] + \P[R(x_1) = 0] \P[A(\xvt) = s]}.
\]
%Hence, $\pi(s; x_1 = 0) = 1/\pi(s; x_1 = 1)$, and so 
%\[ \max_{\xv,\xv';\,\sv} \pi = \max\left(\max_{\xvt;\,s} \pi(s;x_1 = 1),\, \max_{\xvt;\,s} \pi(s;x_1 = 0)\right)
%=  \max\left(\max_{\xvt;\,s} \pi(s;x_1 = 1),\, \min_{\xvt;\,s} \pi(s;x_1 = 1)\right)
%\]
We fix $s \in \{1,\dots,n\}$ and investigate which choice of collection $\xv = (x_1,\xvt)$ maximizes $\pi(s,\xv,x_1')$.
Assume first $x_1 = 1$. (TODO: what about when $x_1 = 0$?) Then
\[ \pi(s,\xvt,0) = \frac{q \P[A(\xvt) = s-1] + p \P[A(\xvt) = s]}
{p \P[A(\xvt) = s-1] + q \P[A(\xvt) = s]}.
\]
By Lemma \ref{lem:rscomp2}, this ratio is maximized at $\xvt^*$ satisfying 
\[ \frac{\P[A(\xvt^*) = s]}{\P[A(\xvt^*) = s-1]} \geq
\frac{\P[A(\yvt) = s]}{\P[A(\yvt) = s-1]} \]
for any $\yvt \in \Dsp^{n-1}$.
We claim that this is the case when $\xvt^*$ consists of all 1s:
%%% Claim to be proven by induction.
\begin{claim}
Write $\one = (1,...,1)$ as an element of $\Dsp^n$.
Then, given $s \in \{1,\dots,n\}$,
\begin{equation} \label{eq:maxpA}
\frac{\P[A(\one) = s]}{\P[A(\one) = s-1]} \geq
\frac{\P[A(\yv) = s]}{\P[A(\yv) = s-1]}
\end{equation}
for any $\yv \in \Dsp^n$.
\end{claim}
\begin{pf}
We proceed by induction on $n$. If $n = 1$, then $A(x) = R(x)$, we need only confirm \eqref{eq:maxpA} for $s = 1$.
Taking $\yv = 0$ (the only possibility aside from $\one$), we have
\begin{equation} \label{eq:singleratio}
\frac{\P[R(0) = 1]}{\P[R(0) = 0]} = \frac{q}{p} \leq
\frac{p}{q} = \frac{\P[R(1) = 1]}{\P[R(1) = 0]}
\end{equation}
verifying \eqref{eq:maxpA} in this case.
Next, suppose that \eqref{eq:maxpA} holds for $s\in\{1,\dots,n-1\}$ and $\one,\yvt \in \Dsp^{n-1}$, and consider $\yv = (\yvt, y_n) \in \Dsp^n$.
For convenience, write $p(y_n) = \P[R(y_n) = 1]$. 
Observe that
\[ \P[A(\yv) = s] = \P[A(\yvt) = s]\cdot(1-p(y_n)) +
\P[A(\yvt) = s-1]\cdot p(y_n), \]
conditioning on the value of $y_n$.
Thus,
%for $s \in \{2,\dots,n-1\}$,
\begin{equation} \label{eq:inductionratio}
\frac{\P[A(\yv) = s]}{\P[A(\yv) = s-1]}
= \frac{\P[A(\yvt) = s]\cdot(1-p(y_n)) +
\P[A(\yvt) = s-1]\cdot p(y_n)}
{\P[A(\yvt) = s-1]\cdot(1-p(y_n)) +
\P[A(\yvt) = s-2]\cdot p(y_n)}.
\end{equation}
For $s \in \{2,\dots,n-1\}$, our induction hypothesis implies that
\[
\frac{\P[A(\yvt) = s]}{\P[A(\yvt) = s-1]} \leq
\frac{\P[A(\one) = s]}{\P[A(\one) = s-1]}\quad\text{and}\quad
\frac{\P[A(\yvt) = s-1]}{\P[A(\yvt) = s-2]} \leq
\frac{\P[A(\one) = s-1]}{\P[A(\one) = s-2]},
\]
and furthermore,
\[ \frac{\P[A(\yvt) = s]}{\P[A(\yvt) = s-2]} =
\frac{\P[A(\yvt) = s]}{\P[A(\yvt) = s-1]}\cdot
\frac{\P[A(\yvt) = s-1]}{\P[A(\yvt) = s-2]} \leq
\frac{\P[A(\one) = s]}{\P[A(\one) = s-2]}.
\]
Therefore, we can apply Lemma \ref{lem:rscomp} to \eqref{eq:inductionratio} to obtain
\begin{equation} \label{eq:lemmaappl}
\frac{\P[A(\yv) = s]}{\P[A(\yv) = s-1]} \leq
\frac{\P[A(\one) = s]\cdot(1-p(y_n)) +
\P[A(\one) = s-1]\cdot p(y_n)}
{\P[A(\one) = s-1]\cdot(1-p(y_n)) +
\P[A(\one) = s-2]\cdot p(y_n)}
%= \frac{\P[A((\one,y_n)) = s]}{\P[A((\one,y_n)) = s-1]}
\end{equation}
for $s \in \{2,\dots,n-1\}$.
If $s = n$, we consider $\P[A(\yvt) = s] = 0$ since
$\P[A(\yvt) \in \{0,\dots,n-1\}] = 1$, and similarly for $\P[A(\one) = s]$.
In this case, Lemma \ref{lem:rscomp} still applies with $b=b'=0$.
A similar argument establishes \eqref{eq:lemmaappl} when $s = 1$.
Therefore,
\[
\frac{\P[A(\yv) = s]}{\P[A(\yv) = s-1]} \leq
\frac{\P[A((\one,y_n)) = s]}{\P[A((\one,y_n)) = s-1]}
\]
for $s \in \{1,\dots,n\}$.
Since $y_n\in\{0,1\}$, the proof will be complete if we show that
\begin{align}
\frac{\P[A((\one,1)) = s]}{\P[A((\one,1)) = s-1]} &=
\frac{\P[A(\one) = s]\cdot q +
\P[A(\one) = s-1]\cdot p}
{\P[A(\one) = s-1]\cdot q +
\P[A(\one) = s-2]\cdot p} \nonumber\\
&\geq
\frac{\P[A(\one) = s]\cdot p +
\P[A(\one) = s-1]\cdot q}
{\P[A(\one) = s-1]\cdot p +
\P[A(\one) = s-2]\cdot q}
= \frac{\P[A((\one,0)) = s]}{\P[A((\one,0)) = s-1]}.
\label{eq:indfinal}
\end{align}
Using the fact that $A(\one) \sim Bin(n-1, p)$, observe that
\[
\frac{\P[A(\one) = s]}{\P[A(\one) = s-1]}
= \frac{\binom{n-1}{s}p^{s}q^{n-1-s}}{\binom{n-1}{s-1}p^{s-1}q^{n-s}}
= \frac{n-s}{s}\frac{p}{q},
\]
from which
\[
\frac{\lambda_2}{\mu_2} = \frac{\P[A(\one) = s]}{\P[A(\one) = s-1]}
= \frac{n-s}{s}\frac{p}{q} \leq \frac{n-s+1}{s-1}\frac{p}{q}
= \frac{\P[A(\one) = s-1]}{\P[A(\one) = s-2]} = \frac{\lambda_1}{\mu_1},
\]
and hence \eqref{eq:indfinal} follows from Lemma \ref{lem:rscomp2} (unless $s=1$, in which case it follows from a simple direct argument).
\end{pf}
TODO: finish argument in the case when $x_1 = 0$.    

\section{Ratios of sums: properties}

Here we establish some results around bounding and comparing ratios of sums, which will be useful in working with the privacy ratio.

%%% Bounding ratio of sums by max ratio of terms

\begin{lem} \label{lem:rsbound}
Suppose $a_1,\dots,a_m,b_1,\dots,b_m \in \R$ with $b_i > 0$ all $i$.
Then 
\[ \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \leq
\max\left(\frac{a_1}{b_1},\dots,\frac{a_m}{b_m}\right). \]
\end{lem}
\begin{pf}
Write
\[ \frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m}
= \frac{a_1}{b_1}\frac{b_1}{b_1+\cdots+b_m} +
%\frac{a_2}{b_2}\frac{b_2}{b_1+\cdots+b_m} +
\cdots + \frac{a_m}{b_m}\frac{b_m}{b_1+\cdots+b_m} =
\sum_{i=1}^m \frac{a_i}{b_i} \lambda_i
\]
where $\lambda_1 + \cdots + \lambda_m = 1$.
The result follows since each $a_i/b_i$ is bounded by $\max_i a_i/b_i$.
\end{pf}

%%% Comparing ratios of sums by comparing ratios of terms

%\begin{lem} \label{lem:rscomp}
%Suppose $a_i,b_i,a'_i,b'_i > 0$ for $i=1,\dots,m$.
%Then
%\begin{equation} \label{eq:rscomp}
%\frac{a_1 + \cdots + a_m}{b_1 + \cdots + b_m} \geq
%\frac{a'_1 + \cdots + a'_m}{b'_1 + \cdots + b'_m}
%\end{equation}
%if
%\begin{equation} \label{eq:lambdamucond}
%a_i/b_i \geq  a_j/b_j,\quad a'_i/b'_i \geq a'_j/b'_j
%\end{equation}
%and
%\begin{equation} \label{eq:aapcond}
%a_i/a_j \geq a'_i/a'_j,\quad b_i/b_j \geq b'_i/b'_j
%\end{equation}
%whenever $1 \leq i < j \leq m$.
%\end{lem}
%It is easy to see that \eqref{eq:lambdamucond} is satisfied when
%$a_1 \geq a_2 \geq \cdots \geq a_m$ and $b_1 \leq b_2 \leq \cdots \leq b_m$, and analogously for $a'$ and $b'$.
%It is also clear from the proof that \eqref{eq:rscomp} still holds if all the inequalities in \eqref{eq:lambdamucond} and \eqref{eq:aapcond} are reversed.
%\begin{pf}
%Cross-multiplying, we see that \eqref{eq:rscomp} is equivalent to
%\[ \sum_i \sum_j a_i b'_j \geq \sum_i \sum_j a'_i b_j \iff
%\sum_i \sum_{j \neq i} (a_ib'_j - a'_ib_j) \geq 0.
%\]
%Furthermore,
%\begin{align*}
%\sum_i \sum_{j \neq i} (a_ib'_j - a'_ib_j)
%&= \sum_i \sum_{j > i} (a_ib'_j - a'_ib_j) +
%\sum_i \sum_{j < i} (a_ib'_j - a'_ib_j) \\
%% relabel the indices
%&= \sum_i \sum_{j > i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i) +
%\sum_j \sum_{i < j} a_j a'_i (\lambda_j\mu_i - \lambda_i\mu_j) \\
%% reverse the sums
%&= \sum_i \sum_{j > i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i) +
%\sum_i \sum_{j > i} a_j a'_i (\lambda_j\mu_i - \lambda_i\mu_j) \\
%%% factor out -1 from the second term and combine
%&= \sum_i \sum_{j > i} (a_i a'_j - a_j a'_i)(\lambda_i\mu_j - \lambda_j\mu_i),
%\end{align*}
%where the second equality follows from relabeling the summation indices, and the third from reversing the sums.
%It follows that \eqref{eq:rscomp} will hold if
%$(a_i a'_j - a_j a'_i)(\lambda_i\mu_j - \lambda_j\mu_i) \geq 0$ for all  $1 \leq i < j \leq m$, which is implied by \eqref{eq:lambdamucond} and \eqref{eq:aapcond}.
%\end{pf}


%%%%%%%%%%%%%%%%%%%%

\begin{lem} \label{lem:rscomp2}
Suppose $a_i,a'_i,\lambda_i,\mu_i > 0$ for $i=1,\dots,m$.
Then
\begin{equation} \label{eq:rscomp2}
\frac{a_1\lambda_1 + \cdots + a_m\lambda_m}
{a_1\mu_1 + \cdots + a_m\mu_m} \geq
\frac{a'_1\lambda_1 + \cdots + a'_m\lambda_m}
{a'_1\mu_1 + \cdots + a'_m\mu_m}
\end{equation}
if
\begin{equation} \label{eq:lambdamucond2}
\lambda_i/\mu_i \geq  \lambda_j/\mu_j
% \frac{\lambda_i}{\mu_i} \geq \frac{\lambda_j}{\mu_j}
%\quad\quad\text{and}\quad\quad \frac{a_i}{a_j} \geq \frac{a'_i}{a'_j}
\end{equation}
and
\begin{equation} \label{eq:aapcond2}
a_i/a_j \geq a'_i/a'_j
\end{equation}
whenever $1 \leq i < j \leq m$.
\end{lem}
Note that numerator and denominator have the same index \eqref{eq:lambdamucond2}, and different indices in \eqref{eq:aapcond2}.
It is easy to see that \eqref{eq:lambdamucond2} is satisfied when
$\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_m$ and
$\mu_1 \leq \mu_2 \leq \cdots \leq \mu_m$.
It is also clear from the proof that \eqref{eq:rscomp2} still holds if all the inequalities in \eqref{eq:lambdamucond2} and \eqref{eq:aapcond2} are reversed.
An important special case for us is the following:
\begin{cor}
Suppose $0 < q < p < 1$ and $a_i, a'_i > 0$ for $i = 1,2$.
Then
\[ \frac{q \cdot a_1 + p\cdot a_2}{p\cdot a_1 + q\cdot a_2} \geq
\frac{q\cdot a'_1 + p\cdot a'_2}{p\cdot a'_1 + q\cdot a'_2} \]
whenever
\[ \frac{a_2}{a_1} \geq \frac{a'_2}{a'_1}. \]
\end{cor}
\begin{pf}
Cross-multiplying, we see that \eqref{eq:rscomp2} is equivalent to
\[ \sum_i \sum_j a_i a'_j \lambda_i \mu_j \geq
\sum_i \sum_j a_i a'_j \lambda_j \mu_i \iff
\sum_i \sum_{j \neq i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i) \geq 0.
\]
Furthermore,
\begin{align*}
\sum_i \sum_{j \neq i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i)
&= \sum_i \sum_{j > i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i) +
\sum_i \sum_{j < i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i) \\
%% relabel the indices
&= \sum_i \sum_{j > i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i) +
\sum_j \sum_{i < j} a_j a'_i (\lambda_j\mu_i - \lambda_i\mu_j) \\
%% reverse the sums
&= \sum_i \sum_{j > i} a_i a'_j (\lambda_i\mu_j - \lambda_j\mu_i) +
\sum_i \sum_{j > i} a_j a'_i (\lambda_j\mu_i - \lambda_i\mu_j) \\
%% factor out -1 from the second term and combine
&= \sum_i \sum_{j > i} (a_i a'_j - a_j a'_i)(\lambda_i\mu_j - \lambda_j\mu_i),
\end{align*}
where the second equality follows from relabeling the summation indices, and the third from reversing the sums.
It follows that \eqref{eq:rscomp2} will hold if
$(a_i a'_j - a_j a'_i)(\lambda_i\mu_j - \lambda_j\mu_i) \geq 0$ for all  $1 \leq i < j \leq m$, which is implied by \eqref{eq:lambdamucond2} and \eqref{eq:aapcond2}.
\end{pf}

\begin{lem} \label{lem:rscomp}
Suppose $a,a',\lambda,\mu > 0$, and $b,b',c,c'\geq 0$. Then
\begin{equation} \label{eq:lemrscomp}
 \frac{a\lambda + b\mu}{c\lambda + a\mu} \geq
\frac{a'\lambda + b'\mu}{c'\lambda + a'\mu}
\end{equation}
if
\begin{equation} \label{eq:abccond}
ac' \geq a'c,\quad ab' \leq a'b,\quad\text{and}\quad bc' \geq b'c.
\end{equation}
\end{lem}
\begin{pf}
\eqref{eq:lemrscomp} holds iff
\begin{align*}
 ac'\lambda^2 &+ aa'\lambda\mu + bc'\lambda\mu + a'b\mu^2 \geq
a'c\lambda^2 + b'c\lambda\mu + aa'\lambda\mu + ab'\mu^2 \\
&\iff (ac'-a'c)\lambda^2 + (bc'-b'c)\lambda\mu + (a'b-ab')\mu^2 \geq 0,
\end{align*}
which is implied by \eqref{eq:abccond}.
\end{pf}

%The next lemma shows how to maximize a weighted sum through the ordering of the
%weights.
%\begin{lem}
%Suppose $a_1 \geq a_2 \geq \cdots \geq a_m$ and $\lambda_1,\dots,\lambda_m \in
%\R$, and consider the weighted sum of the $a_i$ formed using permutations of
%the weights $\lambda_i$.
%Then
%\begin{equation}\label{eq:lempermsum}
%\sum_{i=1}^m \lambda_{i} a_i \geq \sum_{i=1}^m \lambda_{\sigma(i)} a_i
%\end{equation}
%for all permutations $\sigma$ of $(1,\dots,m)$ if and only if 
%\begin{equation}\label{eq:lempermorder}
%\lambda_1 \geq \cdots \geq \lambda_m.
%\end{equation}
%\end{lem}
%\begin{pf}
%\pfrightdir
%For $m=1$, the result is trivial.
%Assume \eqref{eq:lempermsum} implies \eqref{eq:lempermorder} for
%$\lambda_1,\dots,\lambda_{m-1}$,
%%any values $a'_1 \geq \cdots \geq a'_{m-1}$ and weights
%%$\lambda'_1,\dots,\lambda'_{m-1}$,
%and suppose \eqref{eq:lempermsum} holds for
%$\lambda_1,\dots,\lambda_m$.
%We observe that \eqref{eq:lempermsum} holds for $\lambda_1,\dots,\lambda_{m-1}$
%as well.
%Indeed, let $\sigma$ be a permutation of $(1,\dots,m-1)$.
%Then $\tau = (\sigma(1),\dots,\sigma(m-1),m)$ is a permutation of
%$(1,\dots,m)$, and
%\begin{equation*}
%\sum_{i=1}^{m-1} \lambda_i a_i - \sum_{i=1}^{m-1} \lambda_{\sigma(i)} a_i
%= \sum_{i=1}^m \lambda_i a_i -
%\bigg[\sum_{i=1}^{m-1} \lambda_{\sigma(i)} a_i + \lambda_m a_m \bigg]
%\geq 0
%\end{equation*}
%by applying \eqref{eq:lempermsum} to the permutation $\tau$.
%Therefore, $\lambda_1 \geq \cdots \geq \lambda_{m-1}$ by our induction
%hypothesis.
%Finally, we show that $\lambda_m \leq \lambda_{m-1}$.
%If $\lambda_m > \lambda_{m-1}$, consider the permutation swapping $m$ and $m-1$:
%$\sigma' = (1,\dots,m-2, m, m-1)$.
%We have
%\begin{equation*}
%\sum_{i=1}^m \lambda_i a_i - \sum_{i=1}^m \lambda_{\sigma'(i)} a_i =
%(\lambda_{m-1}- \lambda_m)a_{m-1} + (\lambda_m - \lambda_{m-1})a_m =
%(\lambda_{m-1} - \lambda_m)(a_{m-1} - a_m) < 0,
%\end{equation*}
%contradicting \eqref{eq:lempermsum}.
%We conclude that $\lambda_1 \geq \cdots \geq \lambda_m$.
%
%\pfleftdir
%\end{pf}
%

\section{Old stuff}


%%% Independent randomization/RAPPOR form of DP

%% To prove in general (for sets other than measurable rectangles, represent
%% probability as an iterated integral over sections - only one will be different.

Furthermore, if $A$ randomizes each record in the database independently,
\ie $A(\boldsymbol{x}) = A(\boldsymbol{x}, \boldsymbol{X}) := 
\big(A_0(x_1, X_1),\dots,A_0(x_n,X_n)\big)$
where $X_i$ are independent,
then $\boldsymbol{S} = \boldsymbol{S}_0^n$ and $s = (s_1,\dots,s_n)$ with 
$s_i \in \boldsymbol{S}_0$. 
In this case
$P[A(\boldsymbol{x}) = s] = P[A_0(x_1) = s_1,\dots,A_0(x_n) = s_n]
= \prod P[A_0(x_i) = s_i]$.
If $\boldsymbol{x}$ and $\boldsymbol{x'}$ differ in one row (wlog $x_1 \neq x'_1$ and $x_i = x'_i$ for $i = 2,\dots,n$),
then
\[ \frac{P[A(\boldsymbol{x}) = s]}{P[A(\boldsymbol{x'}) = s]} = 
\frac{P[A_0(x_1) = s_1]}{P[A_0(x'_1) = s_1]}. \]
Therefore, in this case, the query $A$ will satisfy differential privacy if
\[ P[A_0(x) = s] \leq \epsilon \cdot P[A_0(x') = s] \]
for all $x,x' \in D$ and $s \in \boldsymbol{S}_0$.
This is the formulation used in the RAPPOR paper that applies to differences between individual records rather than collections differing on a single element.




\end{document}















