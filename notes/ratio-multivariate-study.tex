%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% Informal notes on k-randomization.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,draft]{article}
%\documentclass[11pt,draft]{amsart}

% Custom styling.
\usepackage{mozdp}
%% Controls enumeration labels
%\usepackage{enumerate}
%% Shrinks margins 
\usepackage{fullpage}
%% Shows equation label keys
%\usepackage[notref]{showkeys}
%% Title matter
\title{K-Randomization}
\author{Maxim Zhilyaev \and David Zeber}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle


\section{Privacy ratio in multivariate case}

\textbf{Definitions}

  $D$ - original collection \\
  $D_m$ - modified collection \\

  $N$ - size of collection in vectors \\
  $L$ - length of a vector \\
  $0$ - 0-vector, a vector consisting of  0 buts \\
  $1$ - unit-vector. a vector consisting of 1 bits \\
  $S$ - observed synthetic collection
  
Note that there are  $2^L$ possible distinct synthetic vectors. We denote $v_i$ a distinct synthetic vector, whereby $i$ is ranging from $1$ to $2^L$.
\begin{align}
v_1 = \underset{L}{\underbrace{0000 \dots 0000}} \\
v_2 = \underset{L}{\underbrace{1000 \dots 0000}} \\
v_3 = \underset{L}{\underbrace{0100 \dots 0000}} \\
\dots \\
v_{2^L} =  \underset{L}{\underbrace{1111 \dots 1111}} 
\end{align}



The synthetic output $S$ can be represented by a count of same $v_i$ vectors found after randomization:
\[ S = [s_1,s_2,\dots,s_{2^L-1},s_{2^L}] \]

Suppose that collection $D$ contains a unit-vector that is modified into a zero-vector to receive a collection $D_m$.  The remaining $N-1$ vectors form a collection called $D'$.
Obviously:
\begin{align}
D =  D' + 1 \\
D_m = D' + 0
\end{align}

The privacy ratio is a function of synthetic collection $S$ and expressed as a ratio of probabilities of $S$ given original and modified collections:
\begin{align}
R(S) = \frac{P(S|D_m)}{P(S|D)} =  \frac{P(S|D'+0)}{P(S|D'+1)}
\end{align}

\section{properties of privacy ratio}

The probability of generating $S$ from collection $D'$ and a single vector $y$ is given by:
\begin{align}
P(S|D'+y) = P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|y) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|y) 
\end{align}

Given that a unit-vector is modified into a zero-vector, we may re-write the privacy ratio $R(S)$ as

\begin{align}
R(S)= \frac{P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|0) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|0) }{P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|1) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|1)}\\
R(S) = \frac{ P(v_1|0)  + \sum_{i=2}^{2^L} \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_{2^L}|D')} p(v_i|0)} {P(v_1|1)  + \sum_{i=2}^{2^L} \frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_{2^L}|D')} p(v_i|1)}
\end{align}

\subsection{privacy ratio of a homogenous vectors collection}
An important class of collections is when all collection vectors are the same. In which case the distribution of randomized vectors generated by $D'$ is multinomial because the probability of generating a particular synthetic $v_i$ from any vector of $D'$ remains constant.  Hence, we consider $D'$ to be a collection of $N-1$ identical vectors $x$ of $L$ bits long.  Each $x$ has $k$ zero bits and $r$ one bits in the exact same bit positions.

Since $P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')$ is multinomial, we can express probabilistic ratio term in $2.3$ as:

\begin{align}
\frac{P(s_1,s_2,\dots,s_i-1,\dots,s_{2^L}|D')}{P(s_1-1,s_2,\dots,s_i,\dots,s_{2^L}|D')}  = \frac{ \frac{(N-1)!}{s_1!\cdot s_2! \dots (s_i-1)!\dots} p(v_1|x)^{s_1} \dots p(v_i|x)^{s_i-1} \dots  }{ \frac{(N-1)!}{(s_1-1)!\cdot s_2! \dots s_i!\dots} p(v_1|x)^{s_1-1} \dots p(v_i|x)^{s_i} \dots} = \\
\frac{(s_1-1)!s_i!}{s_1!(s_i-1)!} \cdot \frac{p(v_1|x)^{s_1} p(v_i|x)^{s_i-1} }{  p(v_1|x)^{s_1-1} p(v_i|x)^{s_i} } = \frac{s_i}{s_1} \cdot \frac{p(v_1|x)}{p(v_i|x)} 
 \end{align}

Using that result in the ratio expression $2.3$ we have:

\begin{align}
\frac{P(S|D'+0)}{P(S|D'+1)} = \frac{ p(v_1|0)  + \sum_{i=2}^{2^L} \frac{s_i}{s_1}  \cdot \frac{p(v_1|x)}{p(v_i|x)}  p(v_i|0) } {p(v_1|1)  + \sum_{i=2}^{2^L} \frac{s_i}{s_1}  \cdot \frac{p(v_1|x)}{p(v_i|x)} p(v_i|1)} = \\
\frac{ s_1 \frac{p(v_1|0)}{p(v_1|x)} + \sum_{i=2}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|x)}   } {s_1 \frac{p(v_1|1)}{p(v_1|x)} +   \sum_{i=2}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|x)}   }  = \\
\frac{ \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|x)}   } { \sum_{i=1}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|x)}   } 
\end{align}

We restate this important result as it will be used extensively below:
\begin{align}
R(S) = \frac{ \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|x)}   } { \sum_{i=1}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|x)}   } 
\end{align}


\section{privacy ratio of a zero-valued collection}

Suppose that $D'$ contains $N-1$ zero vectors, denote such collection as $D'_0$.  Then, replacing $x=0$ in formula $2.9$, we get:

\begin{align}
R(S | D'_0) = \frac{ \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|0)}   } { \sum_{i=1}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|0)}} =   \frac{ \sum_{i=1}^{2^L} s_i} { \sum_{i=1}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|0)}}  = \frac{N}{ \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)} }
\end{align}

Note that if $v_i$ and $v_j$ have same number of set bits (call this number $l$), the corresponding probabilities ratios inside the sum are the same:

\begin{align}
\frac{p(v_i|1)}{p(v_i|0)}  = \frac{p(v_j|1)}{p(v_j|0)} = \frac{p^lq^{L-l}}{p^{L-l}q^l } = \left ( \frac{q}{p} \right )^ {L - 2l}
\end{align}

This allows us to express privacy ratio as function of synthetic output $S$ through counts of synthetic vectors that have same number of set bits $l$:

\begin{align}
R(S) = \frac{P(S|D'+0)}{P(S|D'+1)} =  \frac{N}{ \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)} }  = \frac{N}{ \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} } \\
\frac{1}{R(S)} = \frac{1}{N} \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} 
\end{align}

An observer does not gain any more privacy insight by looking at counts of identical vectors than by looking at aggregated counts in a histogram buckets each collecting synthetic vectors with equal number of set bits.
Hence, we can equivalently represent $S$ as the count of vectors that have same number of set bits $l$, and there are $L+1$ such buckets, where $l=0$ corresponds to the bucket of synthetic zero-vectors and $l=L+1$ corresponds to the bucket of synthetic unit-vectors.

\subsection{properties of the sum}

Consider random variable $X$ such that:
\[
X = \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} 
\]

The privacy ratio depends only on $X$ and the size of the collection.  Hence, we study the properties of $X$ - mainly it's expectation and variance.
Note that probability of generating a synthetic vector containing $l$ set bits from either unit or zero original is given by:

\begin{align}
 p(l|1) = \binom{L}{l} p^lq^{L-l} \\
 p(l|0) = \binom{L}{l} q^lp^{L-l}
\end{align}

Note that bucket counts $s_l$ assume multinomial distribution with bucket probabilities:
\[
 p(l|0) = \binom{L}{l} q^lp^{L-l}
 \]
 
Each count is multiplied by a constant factor $\left ( \frac{q}{p} \right )^ {L - 2l}$, hence X is the sum of $L+1$ correlated variables $X_l$, where:

\begin{align}
 X_l =    \left ( \frac{q}{p} \right )^ {L - 2l}  Binomial(p(l|0), N) \\
 X =  \sum_{l=0}^{L} X_l  =  \sum_{l=0}^{L}   \left ( \frac{q}{p} \right )^ {L - 2l}  Binomial(p(l|0), N)
\end{align} 
 
 
The expected values of X is given below:
\begin{align}
E(X)=  E(\sum_{l=0}^{L} X_l ) = \sum_{l=0}^{L} E(X_l)  \left ( \frac{q}{p} \right )^ {L - 2l} =  \sum_{l=0}^{L} N  \cdot  p(l|0) \left ( \frac{q}{p} \right )^ {L - 2l}  = \\
\sum_{l=0}^{L} N  \cdot   \binom{L}{l} q^lp^{L-l} \left ( \frac{q}{p} \right )^ {L - 2l} = N \sum_{l=0}^{L}  \binom{L}{l} \cdot q^{L-l}p^{l} = N (p+q)^L = N
\end{align}

The variance of $X$ is expressed through variance-covariance of multinomial distribution:

\begin{align}
VAR(X)=  \sum_{l=0}^{L} VAR(X_l) + 2 \sum_{i \le j} \sum_{ < j \le L} COV(X_i, X_j) \\
VAR(X) =  \sum_{l=0}^{L} N \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)(1-p(l|0)) - 2 \sum_{j \ne j } N p(i|0)  \left ( \frac{q}{p} \right )^ {L - 2i} p(j|0) \left ( \frac{q}{p} \right )^ {L - 2j} = \\
VAR(X) = N \left ( \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0) -   \sum_{l=0}^{L} \left [ p(l|0) \left (  \frac{q}{p} \right )^ {L - 2l} \right ]^2  - 2 \sum_{j \ne j }  p(i|0)  \left ( \frac{q}{p} \right )^ {L - 2i} p(j|0) \left ( \frac{q}{p} \right )^ {L - 2j}  \right )
\end{align}

Note that negative terms is an expansion of the square of the sum, hence:
\begin{align}
VAR(X) = N   \left ( \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)  - \left [   \sum_{l=0}^{L} p(l|0) \left (  \frac{q}{p} \right )^ {L - 2l}    \right ]^2 \right ) \\
VAR(X) = N   \left ( \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)  - 1 \right )
\end{align}

We now simplify the first term of the sum:
\begin{align}
 \sum_{l=0}^{L} \left [  \left ( \frac{q}{p} \right )^ {L - 2l} \right ]^2 p(l|0)  = \\
 \sum_{l=0}^{L} \binom{L}{l} q^lp^{L-l} \left ( \frac{q}{p} \right )^ {L - 2l} \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} = \\
 \sum_{l=0}^{L} \binom{L}{l} p^lq^{L-l} \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \\
 \sum_{l=0}^{L}  \binom{L}{l}   \frac{q^{2L-3l}}{p^{L-3l}} = \sum_{l=0}^{L}  \binom{L}{l}   \frac{q^{3L-3l} p^{3l}}{(pq)^{L}} = \\
 \frac{1}{(pq)^{L}} \sum_{l=0}^{L}  \binom{L}{l}   (q^3)^{L-l} (p^3)^{l} = \left ( \frac{p^3 + q^3}{pq} \right )^L
\end{align}

Hence the variance of $X$ has the final form of
\begin{align}
VAR(X) = N   \left (  \left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \right )
\end{align}

\subsection{local privacy ratio}

The local differential privacy requires that $R(S)$ should have a reasonable probability of occurring.  Which we express as a requirement that the $X$ should not deviate more than certain number $\sigma$ deviations away from expected value. 

\textbf{Remark: }  It's not immediately obvious why the last statement is valid.  First, the probabilities of $R(S)$ do not necessarily equal to probabilities of $X$.  Suppose that we know that $P(X< E(X) - 3\sigma)$ is sufficiently small, does it mean that $P(R(S) >  \frac{N}{E(X) - 3\sigma})$ is as small.  I actually think it does, because there should be just as many values of $R(S) >   \frac{N}{E(X) - 3\sigma}$  as there are values of $X< E(X) - 3\sigma$.   Another issue, is why do we think that $P(X< E(X) - 3\sigma)$ is sufficiently small?  I think it holds because $X$ is essentially a sum of wighted binomials and it should have a bell shaped PDF, however we need a better proof of it.

 Assuming $3\sigma$ away form the mean is sufficient and the privacy ratio limit equal $\lambda$, the local differential privacy ratio condition is met when: 

\begin{align}
R(S) =   =   \frac{N}{N - 3\sigma} = \frac{N}{E(X) - 3\sqrt{VAR(X)}} \le \lambda  \\
\frac{1}{R(S)} =   \frac{1}{N}   (E(X) - 3\sqrt{VAR(X)}) \ge \frac{1}{\lambda}  \\
1 - 3 \sqrt{\frac{1}{N} \left (  \left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \right )}  \ge \frac{1}{\lambda}  \\
3 \sqrt{\frac{1}{N} \left (  \left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \right )}  \ge \frac{\lambda - 1}{\lambda} 
\end{align}

\section{privacy ratio of a unit-vector collection}

We now suppose that $D'$ contains $N-1$ unit vectors.  Again, the distribution of randomized vectors generated by $D'$ is multinomial, because the probability of generating a particular $v_i$ from a unit vector remains constant.
The privacy ratio is derived in a similar fashion and is equal to:
 
\begin{align}
R(S) = \frac{P(S|D'+0)}{P(S|D'+1)} =  \frac {  \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|1)}  } {N} = \frac{ \sum_{l=0}^{L} s_l \cdot  \left ( \frac{p}{q} \right )^ {L - 2l}  }{ N}
\end{align}

Considering the sum as a random variable $Y$:
\[
Y = \sum_{l=0}^{L} s_l \cdot  \left ( \frac{p}{q} \right )^ {L - 2l} 
\]

 $Y$ is the sum of correlated random variables $Y_l$ each accepting binomial distribution multiplied by a constant:
 \begin{align}
 Y =  \sum_{l=0}^{L} Y_l  =  \sum_{l=0}^{L}   \left ( \frac{p}{q} \right )^ {L - 2l}  Binomial(p(l|1), N)
\end{align} 

Using same approach as in zero-valued case, one can show that:
 \begin{align}
E(Y) = N \\
VAR(Y) =  N   \left (  \left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \right ) \\
R(S) =   \frac{Y}{ N}
\end{align}

\textbf{Lemma 1}

Denote a collection $D'$ consisting of all zeros-vectors as $D'_0$, and a collection of only unit-vectors as $D'_1$.
Then the privacy ratio for $D'_0$  is greater than that of $D'_1$ when they both deviate from the mean by the same amount.

\textbf{Proof}

Recall that the probability ratio for $D'_0$ is given by:
\begin{align}
R(S_0) =  \frac{N}{X}  \\
\text{ where X is a random variable for the following sum:   }\\ 
X =  \sum_{l=0}^{L}   \left ( \frac{q}{p} \right )^ {L - 2l}  Binomial(p(l|0), N)
\end{align}

Conversely, the probability ratio for $D'_1$ is given by:
\begin{align}
R(S_1) =  \frac{Y}{N}  \\
\text{ where Y is a random variable for the following sum:   } \\
Y =  \sum_{l=0}^{L}   \left ( \frac{p}{q} \right )^ {L - 2l}  Binomial(p(l|1), N)
\end{align}

Note that $R(S_0)$ and $R(S_1)$ are both random variables depending entirely on $X$ and $Y$.  $X$ and $Y$ have same expectation $N$. 
Let $X$ and $Y$ deviate from the mean by the same distance $d$.  The corresponding privacy ratios become:
\begin{align}
R(S_0) =  \frac{N}{E(X) - d} =   \frac{N}{N - d}\\
R(S_1) =  \frac{E(Y) + d}{N}  = \frac{N + d}{N}\\
R(S_0) > R(S_1) \\
 \frac{N}{N - d} >  \frac{N + d}{N} \\
 N^2 > N^2  - d^2
\end{align}

Last formula is always true, which proves the lemma.  Setting $d = 3\sigma$ we immediately prove that privacy ratio for $D'_0$  is greater than that of $D'_1$ at the end of local privacy range.

\textbf{Remark: } This way of reasoning assumes that probabilities of $X$ and $Y$ at 3 deviations of the mean are small and comparable (i.e. $P(X< E(X) - 3\sigma) \le P(Y > E(Y) + 3\sigma)$. 
This obviously needs a better argumentation.

\section{privacy ratio of a homogenous vectors collection}

We now consider $D'$ to be a collection of N-1 identical vectors $x$ of $L$ bits long.  Each $x$ has $k$ zero bits and $r$ one bits in the exact same bit positions.

\textbf{Theorem 1}

Privacy ratio of for a homogenous collection $D'$ is the product of privacy ratios of two collections: $D'_{0,k}$ - a collection of zero-vectors of length $k$,  and $D'_{1,r}$ - a collection of unit-vectors of length $r$.
To rephrase the statement:  If we partition $D'$ into two collections - one containing only 0 bit columns and another containing only 1 bit columns, the privacy ratio of the original collection is the product of privacy ratios of each partition.

\textbf{Proof}
  
Recall that privacy ratio of homogenous collections is given by $2.9$:
\begin{align}
R(S) = \frac{ \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|0)}{p(v_i|x)}   } { \sum_{i=1}^{2^L}  s_i \cdot  \frac{p(v_i|1)}{p(v_i|x)}   } 
\end{align}

Recall that each $x$ contains $k$ zero bits and $r$ sets bits in the exact same positions.   Call $0$-columns the positions where $x$ has 0 bit, and $1$-columns the positions where $x$ has $1$ bit. Suppose a synthetic vector $v_i$ has $j$ sets bits in $0$ columns and $t$ set bits in $1$ columns.   Then the probability ratios for each sum is  expressed as: 
\begin{align}
\frac{p(v_i|0)}{p(v_i|x)} = \frac{ \overset{0-bits}{\overbrace{q^jp^{k-j} }} \cdot   \overset{0-bits}{\overbrace{q^tp^{r-t}}}}{    \underset{0-bits}{\underbrace{q^jp^{k-j}}} \cdot  \underset{1-bits}{\underbrace{p^tq^{r-t}}}  } = \left ( \frac{p}{q} \right )^{r-2t} \\
\\
\frac{p(v_i|1)}{p(v_i|x)} =  \frac{ \overset{1-bits}{\overbrace{p^jq^{k-j} }} \cdot   \overset{1-bits}{\overbrace{p^tq^{r-t}}}}{    \underset{0-bits}{\underbrace{q^jp^{k-j}}} \cdot  \underset{1-bits}{\underbrace{p^tq^{r-t}}}  } = \left ( \frac{q}{p} \right )^{k-2j} 
\end{align}

Using this result in $5.1$, we get 
\begin{align}
R(S) = \frac{ \sum_{i=1}^{2^L} s_i  \cdot   \left ( \frac{p}{q} \right )^{r-2t}   } { \sum_{i=1}^{2^L}  s_i \cdot \left ( \frac{q}{p} \right )^{k-2j}   } 
\end{align}

By adding together $s_i$ corresponding to the same number of set bits in the 1-columns for the numerator, and 0-columns for the denominator, we arrive to:
\begin{align}
R(S|D') = \frac{ \sum_{t=0}^{r} s_t  \cdot   \left ( \frac{p}{q} \right )^{r-2t}   } { \sum_{j=0}^{k}  s_j \cdot \left ( \frac{q}{p} \right )^{k-2j}   }  = \frac{ \sum_{t=0}^{r} s_t  \cdot   \left ( \frac{p}{q} \right )^{r-2t}   } {N}  \cdot \frac{N}{ \sum_{j=0}^{k}  s_j \cdot \left ( \frac{q}{p} \right )^{k-2j}   } \\
\\
R(S|D') = R(S|D'_{1,r}) \cdot R(S|D'_{0,k})
\end{align}

\textbf{Theorem 2}

Privacy ratio of  $D'_0$  at the end of the local privacy range is larger than that of any other homogenous collection. 

\textbf{Proof}

By \textbf{lemma 1} , for the deviation $d$ from the mean, the following holds:
\begin{align}
R(S|D'_{0,r}) >  R(S|D'_{1,r})
\end{align}

Therefore,  at the end of the local privacy range, the ratio $R(S|D')$ will be less than:
\begin{align}
 R(S|D') = R(S|D'_{0,k}) \cdot R(S|D'_{1,r}) <  R(S|D'_{0,k}) \cdot R(S|D'_{0,r}) 
\end{align}

Not consider the product of privacy ratio for $D'_{0,k}$ and $D'_{0,r}$:
\begin{align}
 R(S|D'_{0,k}) \cdot R(S|D'_{0,r}) = \frac{N}{ \sum_{t=0}^{r}  s_t \cdot \left ( \frac{q}{p} \right )^{r-2t} }  \cdot \frac{N}{ \sum_{j=0}^{k}  s_j \cdot \left ( \frac{q}{p} \right )^{k-2j}} = \frac{N^2}{X_k \cdot X_r}
\end{align}

We then need to prove that:

\begin{align}
R(S|D_{0,L}) >  R(S|D'_{0,k}) \cdot R(S|D'_{0,r}) \text{, which holds if} \\
\frac{N}{E(X_L) - 3\sqrt{VAR(X_L})}  >   \frac{N^2}{E(X_k \cdot X_r) - 3\sqrt{VAR(X_r\cdot X_k})} \\
E(X_L) - 3\sqrt{VAR(X_L})  < \frac{   E(X_k \cdot X_r) - 3\sqrt{VAR(X_r\cdot X_k})  }{N}  \\
E(X_L) - 3\sqrt{VAR(X_L})  <  \frac{ E(X_k \cdot X_r)}{N} - \frac{3\sqrt{VAR(X_r\cdot X_k})  }{N}
\end{align}

\textbf{Remark: }  it is again questionable why we choose the end of the local privacy range for the right side of the inequality as $3\sigma$ away from the mean.  $X_r \cdot X_k$ is a product of random variables, does it have same shape, how does probability cut off correspond to deviation cut off in this case?  Nonetheless, proceeding with the proof.

First consider the expectations:
 \begin{align}
E(X_L) = N \\
E(X_k \cdot X_r) = E(X_k) \cdot E(X_r) = N^2 \\
 \frac{ E(X_k \cdot X_r)}{N} = N
\end{align}

Which reduces $5.15$ to:
 \begin{align}
\sqrt{VAR(X_L)}  >  \frac{\sqrt{VAR(X_r\cdot X_k)}  }{N} \\
VAR(X_L) >  \frac{VAR(X_r\cdot X_k)  }{N^2} 
\end{align}

Recall that the variance of $VAR(X_L)$  is given by $3.21$:
 \begin{align}
VAR(X_L) = N   \left (  \left ( \frac{p^3 + q^3}{pq} \right )^L - 1 \right )
 \end{align}
 
 Denote $\phi$ as:
 \[ \phi = \frac{p^3 + q^3}{pq} \]
 
 Then $VAR(X_L)$ takes the form of:
 \[ VAR(X_L) =  N(\phi^L - 1) \]
 
Since $X_r$ and $X_k$ are independent and $k+r=L$, we have the variance of the product  $X_r \cdot X_k$ given by:
 \begin{align}
VAR(X_k) =  N(\phi^k - 1) \\
VAR(X_r) =  N(\phi^r - 1) \\
VAR(X_r\cdot X_k) =  N^2 \cdot VAR(X_k) +  N^2 \cdot VAR(X_r) + VAR(X_k)VAR(X_r) = \\
N^3(\phi^k + \phi^r - 2) + N^2(\phi^k\cdot \phi^r - \phi^k - \phi^r + 1) = N^3(\phi^k + \phi^r - 2) + N^2(\phi^L - (\phi^k + \phi^r - 1))
 \end{align}
 
 Replacing corresponding quantities in  $5.20$ with the above expressions , we have:
 \begin{align}
VAR(X_L) >  \frac{VAR(X_r\cdot X_k)  }{N^2} \\
N(\phi^L - 1)  > \frac{ N^3(\phi^k + \phi^r - 2) + N^2(\phi^L - (\phi^k + \phi^r - 1)) }{ N^2 } \\
\phi^L - 1 > \phi^k + \phi^r - 2 + \frac{ \phi^L - (\phi^k + \phi^r - 1)} { N } \\
 \phi^L - (\phi^k + \phi^r - 1) >  \frac{ \phi^L - (\phi^k + \phi^r - 1) } { N } 
\end{align}

Note that:
\begin{align}
k + r = L \text { , and } \\
\phi = \frac{p^3 + q^3}{pq}  = \frac{(p+q)(p^2 - pq + q^2)}{pq} = \frac{(p-q)^2}{pq} + 1 > 1
\end{align}

From that:
\begin{align}
\phi^L - (\phi^k + \phi^r - 1)  = \phi^{k+r} - \phi^k - ( \phi^r - 1) = \phi^k(\phi^r-1) -   ( \phi^r - 1) = (\phi^k -1)( \phi^r - 1) > 0
 \end{align}

This proves inequality $5.29$ and the \textbf{Theorem 2}. 

\textbf{Lemma 2.}

When $N=2$, and the $D'$ consists only of a single vector, the privacy ratio at the end the local privacy range is alway maximized in $D'_0$.   This follows immediately from \textbf{Theorem 2}, because a single vector collection is necessarily a homogenous collection.

 
\section{Generic collections}

\textbf{Remark: } Attempting to prove $D'_0$ maximality by induction

\textbf{Theorem 3}.

Privacy ratio of  $D'_0$  at the end of the local privacy range is the largest compared to any other collection. 

Step 1.   For $N=2$ \textbf{Theorem 3} holds due to \textbf{Lemma 2}.

Step 2.  Suppose that for $N-2$ local privacy ratio maximizes at $D'_0$.   At $N-1$ step we add a unit-vector to $D'_0$ and compare its privacy ratio to that of $D'$ consisting of $N-1$ zero vectors.

\subsection{$D'$ containing one extra unit-vector} 

$D'$ has $N-2$ zero-vectors and 1 unit vector.  The privacy ratio in this case is given by:
\begin{align}
R(S) = \frac{P(S|D'_0 + 1 + 0)}{P(S|D'_0 + 1 + 1)} =  \frac{P(S|D'_0+ 0 + 1)}{P(S|D'_0+1+1)}
\end{align}

Consider the numerator of the above ratio, which has a familiar form of $P(S|D'_0+ 1)$:
\begin{align}
 P(s_1-1,s_2,\dots,s_{2^L}|D') \cdot P(v_1|1) + \dots + P(s_1,s_2-1,\dots,s_{2^L}|D') \cdot P(v_{2^L}|1) = \\
\frac{p(v_1|0) P(s_1-1,s_2,\dots,s_{2^L}|D') }{ s_1}  \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)} = \frac{p^LP(s_1-1,s_2,\dots,s_{2^L}|D')}{ s_1 }  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \
\end{align}

The denominator requires double conditioning to take advantage of the multinomial distribution of $D'_0$.  $P(S|D'_0+1+1)$ is given by:
\begin{align}
\begin{matrix}
p(v_1|1) \; [ p(v_1|1) P(s_1-2,s_2,s_3\dots) + p(v_2|1) P(s_1-1,s_2-1,s_3,\dots) + p(v_3|1) P(s_1-1,s_2,s_3-1,\dots) \cdots + \\
p(v_2|1) \; [ p(v_1|1) P(s_1-1,s_2-1,s_3\dots) + p(v_2|1) P(s_1,s_2-2,s_3,\dots) + p(v_3|1) P(s_1,s_2-1,s_3-1,\dots) \cdots ] + \\
p(v_3|1) \; [ p(v_1|1) P(s_1-1,s_2,s_3-1\dots) + p(v_2|1) P(s_1,s_2-1,s_3-1,\dots) + p(v_3|1) P(s_1,s_2,s_3-2,\dots) \cdots ] + \\
\dots
\end{matrix}
\end{align}

Opening brackets and dividing by $P(s_1-2,s_2,s_3\dots)$ we  receive the following expression:
\begin{align}
\begin{matrix}
p(v_1|1)^2 + & p(v_1|1)p(v_2|1) \frac{s_2}{s_1-1} \frac{p(v_1|0)}{p(v_2|0)} + & p(v_1|1)p(v_3|1) \frac{s_3}{s_1-1}  \frac{p(v_1|0)}{p(v_3|0)}  + \dots \\
p(v_1|1)p(v_2|1) \frac{s_2}{s_1-1} \frac{p(v_1|0)}{p(v_2|0)} +  & p(v_2|1)^2  \frac{s_2(s_2-1)}{s_1(s_1-1)} \left ( \frac{p(v_1|0)}{ p(v_2|0)} \right )^2  +  & p(v_2|1)p(v_3|1)  \frac{s_2 \cdot s_3}{s_1(s_1-1)} \frac{p(v_1|0)^2}{p(v_2|0)p(v_3|0)} + \dots \\
p(v_1|1)p(v_3|1) \frac{s_3}{s_1-1} \frac{p(v_1|0)}{p(v_3|0)}  + &  p(v_2|1)p(v_3|1)  \frac{s_2 \cdot s_3}{s_1(s_1-1)} \frac{p(v_1|0)^2}{p(v_2|0)p(v_3|0)} + & p(v_3|1)^2  \frac{s_3(s_3-1)}{s_1(s_1-1)} \left ( \frac{p(v_1|0)}{ p(v_3|0)} \right )^2  + \dots \\ 
\dots 
\end{matrix}
\end{align}

Multiplying each term of the sum by $\frac{s1(s1-1)}{p(v1|0)^2}$ produces the following:
\begin{align}
\begin{matrix}
\frac{p(v_1|1)^2}{p(v_1|0)^2}s_1(s_1-1) + &  \frac{p(v_1|1)}{p(v_1|0)}   \frac{p(v_2|1)}{p(v_2|0)} s_1s_2 + &  \frac{p(v_1|1)}{p(v_1|0)}   \frac{p(v_3|1)}{p(v_3|0)} s_1s_3  + \dots \\
\frac{p(v_1|1)}{p(v_1|0)}   \frac{p(v_2|1)}{p(v_2|0)} s_1s_2 +  & \frac{p(v_2|1)^2}{p(v_2|0)^2}s_2(s_2-1) +  &    \frac{p(v_2|1)}{p(v_2|0)}  \frac{p(v_3|1)}{p(v_3|0)} s_2s_3  + \dots \\
 \frac{p(v_1|1)}{p(v_1|0)}   \frac{p(v_3|1)}{p(v_3|0)} s_1s_3  + & \frac{p(v_2|1)}{p(v_2|0)}  \frac{p(v_3|1)}{p(v_3|0)} s_2s_3 + &  \frac{p(v_3|1)^2}{p(v_3|0)^2}s_3(s_3-1) + \dots \\ 
\dots 
\end{matrix}
\end{align}

The sum $6.6$ admits a very simple expression:
\begin{align}
\left ( \sum_{i=1}^{2^L} s_i  \cdot \frac{p(v_i|1)}{p(v_i|0)} \right )^2 -  \sum_{i=1}^{2^L} s_i  \cdot \left ( \frac{p(v_i|1)}{p(v_i|0)} \right )^2 \\
\left ( \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \right )^2 -  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)} 
\end{align}

We now ready to come back to the privacy ratio:
\begin{align}
P(S|D'_0+ 0 + 1) =  \frac{p^LP(s_1-1,s_2,\dots,s_{2^L})}{ s_1 }  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} \\
P(S|D'_0+ 1 + 1) =  \frac{(p^L)^2P(s_1-2,s_2,\dots,s_{2^L})}{ s_1(s_1-1) } \left [  \left ( \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \right )^2 -  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)}  \right ] \\
\frac{P(S|D'_0+ 0 + 1)}{P(S|D'_0+ 1 + 1)} = \frac{p^LP(s_1-1,s_2,\dots,s_{2^L}) \cdot s1(s1-1)}{p^{2L} P(s_1-2,s_2,\dots,s_{2^L}) \cdot s_1} \cdot \frac{ \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} }{  \left ( \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \right )^2 -  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)}}
\end{align}

Using the expression below:
\begin{align}
\frac{P(s_1-1,s_2,\dots,s_i-1,\dots,s_{2^L}|D'_0)}{P(s_1-2,s_2,\dots,s_i,\dots,s_{2^L}|D'_0)}  = \frac{ \frac{(N-1)!}{(s_1!-1)!\cdot s_2!, s_3! \dots} p(v_1|0)^{s_1-1} p(v_2|0)^{s_2} \dots  }{ \frac{(N-2)!}{(s_1-2)! s_2! s_3! \dots } p(v_1|0)^{s_1-2} p(v_2|0)^{s_2} \dots} = 
\frac{N-1}{s_1-1} p^L
 \end{align}
 
 We finally arrive to the privacy ratio of a collection with an extra unit vector:
 \begin{align}
 R(S|D'_0 + 1) = (N-1) \cdot \frac{ \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} }{  \left ( \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \right )^2 -  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)}}
\end{align}

Recall that for purely zero-vectored $D'_0$, the privacy ratio is given by:
 \begin{align}
 R(S|D'_0 + 0) = \frac{ N} {\sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l} }
\end{align}

From here we could express maximality condition as:
 \begin{align}
 \left ( \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \right )^2 > N \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)}
 \end{align}
 
 \textbf{Remark: }  I believe there's an error in this derivation.  Specifically, i do not trust the negative term in $6.13$.  When taking expectation of that expression it can get arbitrary high as in:
 
  \begin{align}
E(\sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)} = \sum_{l=0}^{L} E(s_l)  \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)} = N \sum_{l=0}^{L}  p(l|0) \left ( \frac{q}{p} \right )^ {2(L - 2l)} = \\
N  \sum_{l=0}^{L}  \binom{L}{l} q^lp^{L-l} \left (  \frac{q}{p} \right )^ {L - 2l}  \left (  \frac{q}{p} \right )^ {L - 2l} = N   \sum_{l=0}^{L}  \binom{L}{l} q^{L-l}p^l  \left (  \frac{q}{p} \right )^ {L - 2l} = \\
 N   \sum_{l=0}^{L}  \binom{L}{l}  \frac{q^{2L-3l} } { p^{L-3l} } = N   \sum_{l=0}^{L}  \binom{L}{l}  \frac{q^{2L-2l} } { p^{2L-2l} } \cdot   \left (  \frac{p}{q} \right )^l \cdot p^L = Np^L  \sum_{l=0}^{L}  \binom{L}{l}     \left (    \left (  \frac{q}{p} \right )^2   \right )^{L-l}  \left (  \frac{p}{q} \right )^l = \\
 Np^L \left [     \left (  \frac{q}{p} \right )^2 +  \frac{p}{q}  \right ]^L = N \left ( \frac{q^3 + p^3}{pq} \right )^L
 \end{align}
 
 $  \left ( \frac{q^3 + p^3}{pq} \right )^L $ could be arbitrary large, and the expectation for the denominator of  $6.13$ becomes negative:
 
  \begin{align}
E \left [  \left ( \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {L - 2l}  \right )^2 -  \sum_{l=0}^{L} s_l \cdot  \left ( \frac{q}{p} \right )^ {2(L - 2l)} \right ] = N^2 -  N \left ( \frac{q^3 + p^3}{pq} \right )^L
 \end{align}
 
 This smells fishy :(   
\end{document}

